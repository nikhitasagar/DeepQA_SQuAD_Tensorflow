{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import shuffle\n",
    "import os\n",
    "import gensim\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import operator\n",
    "import math\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import string\n",
    "import fileinput\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import nltk\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import split, explode\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors, DenseVector, VectorUDT\n",
    "\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.models.rnn.translate import seq2seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_PYTHON\"] = 'python2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext('local[4]')\n",
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/media/ai2-rey/data_disk/data_sets/SQuAD/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_dir, prefix):\n",
    "    filenames = [data_dir + prefix + tag for tag in ['.context', '.question', '.answer', '.span']]\n",
    "\n",
    "    data = []\n",
    "    column = []\n",
    "    line_iter = fileinput.input(filenames)\n",
    "    for line in line_iter:\n",
    "        if line_iter.isfirstline():\n",
    "            data.append(column)\n",
    "            column = []\n",
    "        column.append(line.rstrip())\n",
    "    data.append(column)\n",
    "    line_iter.close()\n",
    "    data = data[1:]\n",
    "    return list(zip(data[0], data[1], data[2], data[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = load_data(data_dir, 'train')\n",
    "train_cont, train_ques, train_ans, train_span = list(zip(*train_data))\n",
    "\n",
    "val_data = load_data(data_dir, 'val')\n",
    "val_cont, val_ques, val_ans, val_span = list(zip(*val_data))\n",
    "\n",
    "test_data = load_data(data_dir, 'dev')\n",
    "test_cont, test_ques, test_ans, test_span = list(zip(*test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Martorana church ( decorated around 1143 ) looked originally even more Byzantine although important parts were later demolished . The dome mosaic is similar to that of the Cappella Palatina , with Christ enthroned in the middle and four bowed , elongated angels . The Greek inscriptions , decorative patterns , and evangelists in the squinches are obviously executed by the same Greek masters who worked on the Cappella Palatina . The mosaic depicting Roger II of Sicily , dressed in Byzantine imperial robes and receiving the crown by Christ , was originally in the demolished narthex together with another panel , the Theotokos with Georgios of Antiochia , the founder of the church .\n",
      "The Martorana church was likely decorated by the same greek masters who decorated which church ?\n",
      "the Cappella Palatina\n",
      "['68', '70']\n"
     ]
    }
   ],
   "source": [
    "print(val_cont[120])\n",
    "print(val_ques[120])\n",
    "print(val_ans[120])\n",
    "print(val_span[120].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QA_Dataset:\n",
    "  \n",
    "    def __init__(self, context, question, answer, span,\n",
    "                 max_vocab_size=None, autopad_context=None, \n",
    "                 autopad_ques = None, autopad_answer=None, \n",
    "                 context_pad_len=None, ques_pad_len = None, answer_pad_len=None):\n",
    "\n",
    "        assert autopad_context in {'min', 'max', 'avg'}\n",
    "        assert autopad_answer in {'min', 'max', 'avg'}\n",
    "        assert autopad_ques in {'min', 'max', 'avg'}\n",
    "        \n",
    "        self.context = context\n",
    "        self.question = question\n",
    "        self.answer = answer\n",
    "        self.span = span\n",
    "#         span_ids = [i.split() for i in span]\n",
    "#         t_span = []\n",
    "#         for s in span_ids:\n",
    "#             s = [int(c) for c in s]\n",
    "#             t_span.append(s)\n",
    "#         self.span = t_span\n",
    "        \n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.context_pad_len = context_pad_len\n",
    "        self.ques_pad_len = ques_pad_len\n",
    "        self.answer_pad_len = answer_pad_len\n",
    "        self.__autopad_context = autopad_context\n",
    "        self.__autopad_ques = autopad_ques\n",
    "        self.__autopad_answer = autopad_answer\n",
    "        \n",
    "        self.vocab = set()\n",
    "        self.word_counter = dict()\n",
    "        self.word2id_dict = dict()\n",
    "        self.id2word_dict = dict()\n",
    "        \n",
    "        self.context_ids = None\n",
    "        self.context_sentences_ids = [] \n",
    "        self.question_ids = None\n",
    "#        self.answer_ids = None\n",
    "#         self.answer_masks = []\n",
    "        self.answer_one_hot_labels = []\n",
    "\n",
    "#         self.context_text = None\n",
    "#         self.question_text = None\n",
    "#         self.answer_text = None\n",
    "        \n",
    "#         self.num_tokens = None\n",
    "#         self.max_sent_len_context = 0\n",
    "#         self.max_num_sentences = 0\n",
    "#         self.max_sent_len_question = 0\n",
    "        \n",
    "        self.__parse_data()\n",
    "        self.__create_word2id_dict()\n",
    "        self.__numericize_data()\n",
    "\n",
    "    def __update_word_counter(self, sequence):\n",
    "        \"\"\" Update word_counter with counts for words in a sentence\n",
    "        \n",
    "        Args:\n",
    "            sequence (list<str>) : list of words in a sequence\n",
    "        \n",
    "        \"\"\"\n",
    "        for word in sequence:\n",
    "            self.word_counter[word] = self.word_counter.get(word, 0) + 1\n",
    "            \n",
    "    def __create_vocab(self):\n",
    "        \"\"\" Create set of most frequent unique words found in the training data \"\"\"\n",
    "        \n",
    "        if self.max_vocab_size == None:\n",
    "            self.vocab = set(self.word_counter.keys())\n",
    "        else:\n",
    "            self.vocab = set(sorted(self.word_counter, key=self.word_counter.get, reverse=True)[:self.max_vocab_size])\n",
    "        \n",
    "    def __shuffle_data(self, data):\n",
    "        random.shuffle(data)\n",
    "        return list(zip(*data))\n",
    "    \n",
    "    def __parse_data(self):\n",
    "        \n",
    "        for idx in range(len(self.context)):\n",
    "            self.__update_word_counter(self.context[idx].split())\n",
    "            self.__update_word_counter(self.question[idx].split())\n",
    "            self.__update_word_counter(self.answer[idx].split())\n",
    "        \n",
    "        self.__create_vocab()\n",
    "        \n",
    "        shuffle = list(zip(self.context, self.question, self.answer, self.span))        \n",
    "        self.context, self.question, self.answer, self.span = self.__shuffle_data(shuffle)\n",
    "    \n",
    "    def __create_word2id_dict(self):\n",
    "        \n",
    "        misc_tokens = ['PAD', 'UNK']\n",
    "            \n",
    "        for i, token in enumerate(misc_tokens):\n",
    "            self.word2id_dict[token] = i\n",
    "            \n",
    "        for word in self.vocab:\n",
    "            self.word2id_dict[word] = len(self.word2id_dict)\n",
    "\n",
    "        self.vocab |= set(misc_tokens)\n",
    "        self.id2word_dict = dict(zip(self.word2id_dict.values(), self.word2id_dict.keys()))\n",
    "        self.num_tokens = len(self.word2id_dict)\n",
    "    \n",
    "    def __convert_word2id(self, word):\n",
    "        try:\n",
    "            word_id = self.word2id_dict[word]\n",
    "        except:\n",
    "            word_id = self.word2id_dict['UNK']\n",
    "        return word_id\n",
    "    \n",
    "    def __apply_padding(self, s, pad_len):\n",
    "        sequence = s[:]\n",
    "        if len(sequence) < pad_len:\n",
    "            sequence += [self.word2id_dict['PAD'] for i in range(pad_len - len(sequence))]\n",
    "        elif len(sequence) > pad_len:\n",
    "            sequence = sequence[:pad_len]\n",
    "        else:\n",
    "            pass\n",
    "        return sequence\n",
    "        \n",
    "    def __get_seq_length_stats(self, sequences):\n",
    "        max_len = 0\n",
    "        min_len = 100000\n",
    "        avg_len = 0\n",
    "        for sequence in sequences:\n",
    "            max_len = max(max_len, len(sequence))\n",
    "            min_len = min(min_len, len(sequence))\n",
    "            avg_len += len(sequence)\n",
    "        avg_len = int(float(avg_len) / len(sequences))\n",
    "        return min_len, max_len, avg_len\n",
    "\n",
    "    def __get_max_sequence_lengths(self, context_ids, question_ids, answer_ids):\n",
    "        min_context_len, max_context_len, avg_context_len = self.__get_seq_length_stats(context_ids)\n",
    "        min_ques_len, max_ques_len, avg_ques_len = self.__get_seq_length_stats(question_ids)\n",
    "        min_answer_len, max_answer_len, avg_answer_len = self.__get_seq_length_stats(answer_ids)\n",
    "\n",
    "        if self.context_pad_len == None:\n",
    "            if self.__autopad_context != None:\n",
    "                if self.__autopad_context == 'min':\n",
    "                    self.context_pad_len = min_context_len\n",
    "                elif self.__autopad_context == 'max':\n",
    "                    self.context_pad_len = max_context_len\n",
    "                elif self.__autopad_context == 'avg':\n",
    "                    self.context_pad_len = avg_context_len\n",
    "            else:\n",
    "                self.context_pad_len = avg_context_len\n",
    "                \n",
    "        if self.ques_pad_len == None:\n",
    "            if self.__autopad_ques != None:\n",
    "                if self.__autopad_ques == 'min':\n",
    "                    self.ques_pad_len = min_ques_len\n",
    "                elif self.__autopad_ques == 'max':\n",
    "                    self.ques_pad_len = max_ques_len\n",
    "                elif self.__autopad_ques == 'avg':\n",
    "                    self.ques_pad_len = avg_ques_len\n",
    "            else:\n",
    "                self.ques_pad_len = avg_ques_len\n",
    "                \n",
    "        if self.answer_pad_len == None:\n",
    "            if self.__autopad_answer != None:\n",
    "                if self.__autopad_answer == 'min':\n",
    "                    self.answer_pad_len = min_answer_len\n",
    "                elif self.__autopad_answer == 'max':\n",
    "                    self.answer_pad_len = max_answer_len\n",
    "                elif self.__autopad_answer == 'avg':\n",
    "                    self.answer_pad_len = avg_answer_len\n",
    "            else:\n",
    "                self.answer_pad_len = avg_answer_len \n",
    "                \n",
    "        #print('Context length stats: min = {}, max = {}, avg = {}'.format(min_context_len, max_context_len, avg_context_len))\n",
    "        #print('Context pad length set to: {}'.format(self.context_pad_len))\n",
    "        \n",
    "        #print('Question length stats: min = {}, max = {}, avg = {}'.format(min_ques_len, max_ques_len, avg_ques_len))\n",
    "        #print('Question pad length set to: {}'.format(self.ques_pad_len))        \n",
    "        \n",
    "        #print('Answer length stats: min = {}, max = {}, avg = {}'.format(min_answer_len, max_answer_len, avg_answer_len))\n",
    "        #print('Answer pad length set to: {}'.format(self.answer_pad_len))        \n",
    "    def __convert_to_one_hot(self, span_ids, num_classes):\n",
    "        label = []\n",
    "        for idx in span_ids:\n",
    "            one_hot = [0 for i in range(num_classes)]\n",
    "            if idx >= num_classes - 1:\n",
    "                one_hot[-1] = 1\n",
    "            else:\n",
    "                one_hot[idx] = 1\n",
    "            label.append(one_hot)\n",
    "        return label\n",
    "    \n",
    "    def __convert_to_mask(self, i, a):\n",
    "        mask = (np.in1d(np.array(self.context_ids[i]), a[i]))*1\n",
    "        if len(a[i])>1:\n",
    "            count = 0\n",
    "            while count < len(mask):\n",
    "                if mask[count:count+len(a[i])].all():\n",
    "                    count+=len(a[i])\n",
    "                else:\n",
    "                    mask[count] = False\n",
    "                    count +=1\n",
    "        return mask\n",
    "        \n",
    "    def __tokenize_sentences(self, context_ids, question_ids, answer_ids):\n",
    "        \"\"\" Tokenizes sentences.\n",
    "        :param raw: dict returned from load_babi\n",
    "        :param word_table: WordTable\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        context_tokens = []\n",
    "        question_tokens = []\n",
    "        #answer_tokens = []\n",
    "        \n",
    "        for i in range(len(context_ids)):\n",
    "            c_tkn_ids = self.__apply_padding(context_ids[i], self.context_pad_len)\n",
    "            context_tokens.append(c_tkn_ids)\n",
    "            q_tkn_ids = self.__apply_padding(question_ids[i], self.ques_pad_len)\n",
    "            question_tokens.append(q_tkn_ids)\n",
    "            \n",
    "            span_ids = [int(x) for x in self.span[i].split()]\n",
    "            label = self.__convert_to_one_hot(span_ids, self.context_pad_len)\n",
    "            self.answer_one_hot_labels.append(label)\n",
    "            \n",
    "            if i%100 == 0:\n",
    "                print(i)\n",
    "            #a_tkn_ids = self.__apply_padding(answer_ids[i], self.answer_pad_len)\n",
    "            #answer_tokens.append(a_tkn_ids)\n",
    "        return context_tokens, question_tokens#, answer_tokens\n",
    "        \n",
    "    def __convert_text2ids(self, context, question, answer):\n",
    "        \"\"\" Tokenizes sentences.\n",
    "        :param raw: dict returned from load_babi\n",
    "        :param word_table: WordTable\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        context_ids = []\n",
    "        question_ids = []\n",
    "        answer_ids = []\n",
    "        for i in range(len(context)):\n",
    "            c_ids = [self.__convert_word2id(word) for word in context[i].split()]\n",
    "            context_ids.append(c_ids)\n",
    "            q_ids = [self.__convert_word2id(word) for word in question[i].split()]\n",
    "            question_ids.append(q_ids)\n",
    "            a_ids = [self.__convert_word2id(word) for word in answer[i].split()]\n",
    "            answer_ids.append(a_ids)            \n",
    "        return context_ids, question_ids, answer_ids\n",
    "          \n",
    "    def __convert_text2words(self, context, question, answer):\n",
    "        context_words = []\n",
    "        question_words = []\n",
    "        answer_words = []\n",
    "        for i in range(len(context)):\n",
    "            c_words = context[i].split()\n",
    "            context_words.append(c_words)\n",
    "            q_words = question[i].split()\n",
    "            question_words.append(q_words)\n",
    "            a_words = answer[i].split()\n",
    "            answer_words.append(a_words)\n",
    "        return context_words, question_words, answer_words    \n",
    "\n",
    "    def __get_max_sentence_length(self, stories):\n",
    "        num_sentences = 0\n",
    "        sen_len = 0\n",
    "        for story in stories: \n",
    "            sentences = sent_tokenize(story)\n",
    "            num_sentences = max(num_sentences, len(sentences))\n",
    "            word_count = lambda sentence: len(word_tokenize(sentence))\n",
    "            sen_len = max(sen_len, len((max(sentences, key=word_count)).split()))\n",
    "        return sen_len, num_sentences\n",
    "        \n",
    "    def __context2sentences(self, story):\n",
    "        story_sentences = []\n",
    "        curr_sentence = []\n",
    "        for word_id in story:\n",
    "            if self.id2word_dict[word_id] == '.':\n",
    "                if len(curr_sentence) < self.max_sent_len_context:\n",
    "                    curr_sentence += [0 for i in range(self.max_sent_len_context - len(curr_sentence))]\n",
    "                elif len(curr_sentence) > self.max_sent_len_context:\n",
    "                    curr_sentence = curr_sentence[:self.max_sent_len_context]\n",
    "                story_sentences.append(curr_sentence)\n",
    "                curr_sentence = []\n",
    "            else:\n",
    "                curr_sentence.append(word_id)\n",
    "            \n",
    "        return story_sentences\n",
    "    \n",
    "    def __numericize_data(self):\n",
    "        c, q, a = self.__convert_text2ids(self.context, self.question, self.answer)\n",
    "        \n",
    "        self.__get_max_sequence_lengths(c, q, a)\n",
    "        \n",
    "#         self.context_ids, self.question_ids, self.answer_ids = self.__tokenize_sentences(c, q, a)\n",
    "        self.context_ids, self.question_ids = self.__tokenize_sentences(c, q, a)\n",
    "            \n",
    "#         self.context_text, self.question_text, self.answer_text = self.__convert_text2words(self.context,\n",
    "#                                                                                             self.question,\n",
    "#                                                                                             self.answer)\n",
    "        #self.max_sent_len_context, self.max_num_sentences = self.__get_max_sentence_length(self.context)\n",
    "        #print(\"max_sent_len_context: {} max_num_sentences: {}\".format(self.max_sent_len_context, self.max_num_sentences))\n",
    "        #self.max_sent_len_question, ques_num_sentences = self.__get_max_sentence_length(self.question)\n",
    "        #print(\"max_sent_len_ques: {} ques_num_sentences: {}\".format(self.max_sent_len_question, ques_num_sentences))\n",
    "        #for c_ids in c:\n",
    "            #self.context_sentences_ids.append(self.__context2sentences(c_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "UNKNOWN = \"*UNKNOWN*\"\n",
    "\n",
    "def generate_embedding_files(filename):\n",
    "    embeddings = {}\n",
    "    for line in open(filename):\n",
    "        parts = line.split()\n",
    "        embeddings[parts[0]] = list(map(float, parts[1:]))\n",
    "    embedding_size = len(list(embeddings.values())[0])\n",
    "    embeddings[UNKNOWN] = [0.0 for _ in range(embedding_size)]\n",
    "    \n",
    "    words = embeddings.keys()\n",
    "    embedding_matrix = np.array([embeddings[word] for word in list(embeddings.keys())])\n",
    "    return words, embedding_matrix\n",
    "\n",
    "glove_words, glove_embedding_matrix = generate_embedding_files('/media/ai2-rey/data_disk/data_sets/glove.6B/glove.6B.50d.txt')\n",
    "glove_vocab_lookup = {word: i for i, word  in enumerate(glove_words)}\n",
    "glove_vocab_size, glove_embedding_size= glove_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataIterator:  \n",
    "    def __init__(self, data, batch_size):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.data_iterator = self.make_random_iter()\n",
    "        \n",
    "    def next_batch(self):\n",
    "        try:\n",
    "            idxs = next(self.data_iterator)\n",
    "        except StopIteration:\n",
    "            self.data_iterator = self.make_random_iter()\n",
    "            idxs = next(self.data_iterator)\n",
    "            \n",
    "        batch = [self.data[i] for i in idxs]\n",
    "        batch_idxs = [idx for idx in idxs]\n",
    "        return batch, batch_idxs\n",
    "\n",
    "    def make_random_iter(self):\n",
    "        splits = np.arange(self.batch_size, len(self.data), self.batch_size)\n",
    "        it = np.split(np.random.permutation(range(len(self.data))), splits)[:-1]\n",
    "        return iter(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n",
      "26500\n",
      "26600\n",
      "26700\n",
      "26800\n",
      "26900\n",
      "27000\n",
      "27100\n",
      "27200\n",
      "27300\n",
      "27400\n",
      "27500\n",
      "27600\n",
      "27700\n",
      "27800\n",
      "27900\n",
      "28000\n",
      "28100\n",
      "28200\n",
      "28300\n",
      "28400\n",
      "28500\n",
      "28600\n",
      "28700\n",
      "28800\n",
      "28900\n",
      "29000\n",
      "29100\n",
      "29200\n",
      "29300\n",
      "29400\n",
      "29500\n",
      "29600\n",
      "29700\n",
      "29800\n",
      "29900\n",
      "30000\n",
      "30100\n",
      "30200\n",
      "30300\n",
      "30400\n",
      "30500\n",
      "30600\n",
      "30700\n",
      "30800\n",
      "30900\n",
      "31000\n",
      "31100\n",
      "31200\n",
      "31300\n",
      "31400\n",
      "31500\n",
      "31600\n",
      "31700\n",
      "31800\n",
      "31900\n",
      "32000\n",
      "32100\n",
      "32200\n",
      "32300\n",
      "32400\n",
      "32500\n",
      "32600\n",
      "32700\n",
      "32800\n",
      "32900\n",
      "33000\n",
      "33100\n",
      "33200\n",
      "33300\n",
      "33400\n",
      "33500\n",
      "33600\n",
      "33700\n",
      "33800\n",
      "33900\n",
      "34000\n",
      "34100\n",
      "34200\n",
      "34300\n",
      "34400\n",
      "34500\n",
      "34600\n",
      "34700\n",
      "34800\n",
      "34900\n",
      "35000\n",
      "35100\n",
      "35200\n",
      "35300\n",
      "35400\n",
      "35500\n",
      "35600\n",
      "35700\n",
      "35800\n",
      "35900\n",
      "36000\n",
      "36100\n",
      "36200\n",
      "36300\n",
      "36400\n",
      "36500\n",
      "36600\n",
      "36700\n",
      "36800\n",
      "36900\n",
      "37000\n",
      "37100\n",
      "37200\n",
      "37300\n",
      "37400\n",
      "37500\n",
      "37600\n",
      "37700\n",
      "37800\n",
      "37900\n",
      "38000\n",
      "38100\n",
      "38200\n",
      "38300\n",
      "38400\n",
      "38500\n",
      "38600\n",
      "38700\n",
      "38800\n",
      "38900\n",
      "39000\n",
      "39100\n",
      "39200\n",
      "39300\n",
      "39400\n",
      "39500\n",
      "39600\n",
      "39700\n",
      "39800\n",
      "39900\n",
      "40000\n",
      "40100\n",
      "40200\n",
      "40300\n",
      "40400\n",
      "40500\n",
      "40600\n",
      "40700\n",
      "40800\n",
      "40900\n",
      "41000\n",
      "41100\n",
      "41200\n",
      "41300\n",
      "41400\n",
      "41500\n",
      "41600\n",
      "41700\n",
      "41800\n",
      "41900\n",
      "42000\n",
      "42100\n",
      "42200\n",
      "42300\n",
      "42400\n",
      "42500\n",
      "42600\n",
      "42700\n",
      "42800\n",
      "42900\n",
      "43000\n",
      "43100\n",
      "43200\n",
      "43300\n",
      "43400\n",
      "43500\n",
      "43600\n",
      "43700\n",
      "43800\n",
      "43900\n",
      "44000\n",
      "44100\n",
      "44200\n",
      "44300\n",
      "44400\n",
      "44500\n",
      "44600\n",
      "44700\n",
      "44800\n",
      "44900\n",
      "45000\n",
      "45100\n",
      "45200\n",
      "45300\n",
      "45400\n",
      "45500\n",
      "45600\n",
      "45700\n",
      "45800\n",
      "45900\n",
      "46000\n",
      "46100\n",
      "46200\n",
      "46300\n",
      "46400\n",
      "46500\n",
      "46600\n",
      "46700\n",
      "46800\n",
      "46900\n",
      "47000\n",
      "47100\n",
      "47200\n",
      "47300\n",
      "47400\n",
      "47500\n",
      "47600\n",
      "47700\n",
      "47800\n",
      "47900\n",
      "48000\n",
      "48100\n",
      "48200\n",
      "48300\n",
      "48400\n",
      "48500\n",
      "48600\n",
      "48700\n",
      "48800\n",
      "48900\n",
      "49000\n",
      "49100\n",
      "49200\n",
      "49300\n",
      "49400\n",
      "49500\n",
      "49600\n",
      "49700\n",
      "49800\n",
      "49900\n",
      "50000\n",
      "50100\n",
      "50200\n",
      "50300\n",
      "50400\n",
      "50500\n",
      "50600\n",
      "50700\n",
      "50800\n",
      "50900\n",
      "51000\n",
      "51100\n",
      "51200\n",
      "51300\n",
      "51400\n",
      "51500\n",
      "51600\n",
      "51700\n",
      "51800\n",
      "51900\n",
      "52000\n",
      "52100\n",
      "52200\n",
      "52300\n",
      "52400\n",
      "52500\n",
      "52600\n",
      "52700\n",
      "52800\n",
      "52900\n",
      "53000\n",
      "53100\n",
      "53200\n",
      "53300\n",
      "53400\n",
      "53500\n",
      "53600\n",
      "53700\n",
      "53800\n",
      "53900\n",
      "54000\n",
      "54100\n",
      "54200\n",
      "54300\n",
      "54400\n",
      "54500\n",
      "54600\n",
      "54700\n",
      "54800\n",
      "54900\n",
      "55000\n",
      "55100\n",
      "55200\n",
      "55300\n",
      "55400\n",
      "55500\n",
      "55600\n",
      "55700\n",
      "55800\n",
      "55900\n",
      "56000\n",
      "56100\n",
      "56200\n",
      "56300\n",
      "56400\n",
      "56500\n",
      "56600\n",
      "56700\n",
      "56800\n",
      "56900\n",
      "57000\n",
      "57100\n",
      "57200\n",
      "57300\n",
      "57400\n",
      "57500\n",
      "57600\n",
      "57700\n",
      "57800\n",
      "57900\n",
      "58000\n",
      "58100\n",
      "58200\n",
      "58300\n",
      "58400\n",
      "58500\n",
      "58600\n",
      "58700\n",
      "58800\n",
      "58900\n",
      "59000\n",
      "59100\n",
      "59200\n",
      "59300\n",
      "59400\n",
      "59500\n",
      "59600\n",
      "59700\n",
      "59800\n",
      "59900\n",
      "60000\n",
      "60100\n",
      "60200\n",
      "60300\n",
      "60400\n",
      "60500\n",
      "60600\n",
      "60700\n",
      "60800\n",
      "60900\n",
      "61000\n",
      "61100\n",
      "61200\n",
      "61300\n",
      "61400\n",
      "61500\n",
      "61600\n",
      "61700\n",
      "61800\n",
      "61900\n",
      "62000\n",
      "62100\n",
      "62200\n",
      "62300\n",
      "62400\n",
      "62500\n",
      "62600\n",
      "62700\n",
      "62800\n",
      "62900\n",
      "63000\n",
      "63100\n",
      "63200\n",
      "63300\n",
      "63400\n",
      "63500\n",
      "63600\n",
      "63700\n",
      "63800\n",
      "63900\n",
      "64000\n",
      "64100\n",
      "64200\n",
      "64300\n",
      "64400\n",
      "64500\n",
      "64600\n",
      "64700\n",
      "64800\n",
      "64900\n",
      "65000\n",
      "65100\n",
      "65200\n",
      "65300\n",
      "65400\n",
      "65500\n",
      "65600\n",
      "65700\n",
      "65800\n",
      "65900\n",
      "66000\n",
      "66100\n",
      "66200\n",
      "66300\n",
      "66400\n",
      "66500\n",
      "66600\n",
      "66700\n",
      "66800\n",
      "66900\n",
      "67000\n",
      "67100\n",
      "67200\n",
      "67300\n",
      "67400\n",
      "67500\n",
      "67600\n",
      "67700\n",
      "67800\n",
      "67900\n",
      "68000\n",
      "68100\n",
      "68200\n",
      "68300\n",
      "68400\n",
      "68500\n",
      "68600\n",
      "68700\n",
      "68800\n",
      "68900\n",
      "69000\n",
      "69100\n",
      "69200\n",
      "69300\n",
      "69400\n",
      "69500\n",
      "69600\n",
      "69700\n",
      "69800\n",
      "69900\n",
      "70000\n",
      "70100\n",
      "70200\n",
      "70300\n",
      "70400\n",
      "70500\n",
      "70600\n",
      "70700\n",
      "70800\n",
      "70900\n",
      "71000\n",
      "71100\n",
      "71200\n",
      "71300\n",
      "71400\n",
      "71500\n",
      "71600\n",
      "71700\n",
      "71800\n",
      "71900\n",
      "72000\n",
      "72100\n",
      "72200\n",
      "72300\n",
      "72400\n",
      "72500\n",
      "72600\n",
      "72700\n",
      "72800\n",
      "72900\n",
      "73000\n",
      "73100\n",
      "73200\n",
      "73300\n",
      "73400\n",
      "73500\n",
      "73600\n",
      "73700\n",
      "73800\n",
      "73900\n",
      "74000\n",
      "74100\n",
      "74200\n",
      "74300\n",
      "74400\n",
      "74500\n",
      "74600\n",
      "74700\n",
      "74800\n",
      "74900\n",
      "75000\n",
      "75100\n",
      "75200\n",
      "75300\n",
      "75400\n",
      "75500\n",
      "75600\n",
      "75700\n",
      "75800\n",
      "75900\n",
      "76000\n",
      "76100\n",
      "76200\n",
      "76300\n",
      "76400\n",
      "76500\n",
      "76600\n",
      "76700\n",
      "76800\n",
      "76900\n",
      "77000\n",
      "77100\n",
      "77200\n",
      "77300\n",
      "77400\n",
      "77500\n",
      "77600\n",
      "77700\n",
      "77800\n",
      "77900\n",
      "78000\n",
      "78100\n",
      "78200\n",
      "78300\n",
      "78400\n",
      "78500\n",
      "78600\n",
      "78700\n",
      "78800\n",
      "78900\n",
      "79000\n",
      "79100\n",
      "79200\n",
      "79300\n",
      "79400\n",
      "79500\n",
      "79600\n",
      "79700\n",
      "79800\n",
      "79900\n",
      "80000\n",
      "80100\n",
      "80200\n",
      "80300\n",
      "80400\n",
      "80500\n",
      "80600\n",
      "80700\n",
      "80800\n",
      "80900\n",
      "81000\n",
      "81100\n",
      "81200\n",
      "81300\n"
     ]
    }
   ],
   "source": [
    "train = QA_Dataset(train_cont, train_ques, train_ans, train_span,\n",
    "                   context_pad_len = 200, ques_pad_len = 20,\n",
    "                   autopad_context='max', autopad_ques = 'max', autopad_answer='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n"
     ]
    }
   ],
   "source": [
    "val = QA_Dataset(val_cont, val_ques, val_ans, val_span,\n",
    "                 context_pad_len=train.context_pad_len, \n",
    "                 ques_pad_len = train.ques_pad_len, \n",
    "                 answer_pad_len=train.answer_pad_len,\n",
    "                 autopad_context='max', autopad_ques='max', autopad_answer='max')\n",
    "\n",
    "test = QA_Dataset(test_cont, test_ques, test_ans, test_span,\n",
    "                  context_pad_len=train.context_pad_len, \n",
    "                  ques_pad_len = train.ques_pad_len,\n",
    "                  answer_pad_len=train.answer_pad_len,\n",
    "                  autopad_context='max', autopad_ques='max', autopad_answer='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_cont, train_ques, train_ans, train_span\n",
    "del val_cont, val_ques, val_ans, val_span\n",
    "del test_cont, test_ques, test_ans, test_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ids = list(zip(train.context_ids, train.question_ids, train.answer_one_hot_labels))\n",
    "val_ids = list(zip(val.context_ids,val.question_ids,val.answer_one_hot_labels))\n",
    "test_ids = list(zip(test.context_ids,test.question_ids, test.answer_one_hot_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_iter = DataIterator(train_ids,128)\n",
    "val_data_iter = DataIterator(val_ids,128)\n",
    "test_data_iter = DataIterator(test_ids,128)\n",
    "#deploy_data_iter = DataIterator(train_ids,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.ques_pad_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.context_pad_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "passage_max_len = train.context_pad_len\n",
    "question_max_len = train.ques_pad_len\n",
    "#output of lstm is n_steps or passage length by hidden size \n",
    "n_hidden = 50 \n",
    "num_indices = 2\n",
    "batch_size = 128\n",
    "n_hidden_layers = 2\n",
    "learning_rate = 0.001\n",
    "blend_units = 6\n",
    "train_iters = 100000\n",
    "keep_prob = 0.5\n",
    "display_step = 200\n",
    "val_interval = 1000\n",
    "word_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.22567 , -0.48367 , -0.28596 ,  0.078052,  0.52416 , -0.092551,\n",
       "        0.57707 , -0.83893 ,  0.2523  ,  0.67678 , -0.19255 , -0.60286 ,\n",
       "        1.2789  , -0.27485 ,  0.5941  ,  0.11142 , -0.08224 , -0.46565 ,\n",
       "       -0.060026, -0.37025 ,  1.244   ,  0.062624, -0.52119 , -0.097129,\n",
       "       -0.47283 ,  1.409   ,  1.2582  , -0.25652 , -0.041171, -0.98153 ,\n",
       "       -1.1759  ,  1.2136  , -0.14361 ,  0.95605 ,  0.23754 , -0.24656 ,\n",
       "        0.28346 ,  0.39402 ,  0.26789 , -0.41279 ,  0.54999 , -0.054683,\n",
       "        1.0312  ,  0.15117 , -0.38207 ,  0.087894,  0.67466 , -0.22228 ,\n",
       "        0.19923 ,  0.090407])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embedding_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "100\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "passage = tf.placeholder(tf.int32, [batch_size, passage_max_len], name='passage')\n",
    "question = tf.placeholder(tf.int32,[batch_size, question_max_len], name='question')\n",
    "#desired_output = tf.placeholder(tf.int32, [num_indices, batch_size, passage_max_len], name='desired_output')\n",
    "y = tf.placeholder(tf.float32,[num_indices,batch_size,passage_max_len])\n",
    "\n",
    "dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "#batch_size = tf.placeholder(tf.int32)\n",
    "\n",
    "embeddings = tf.constant(glove_embedding_matrix, name='embeddings', dtype=tf.float32)\n",
    "\n",
    "#preprocessing layer \n",
    "passage_embedded = tf.nn.embedding_lookup(embeddings, passage)\n",
    "question_embedded = tf.nn.embedding_lookup(embeddings, question)\n",
    "\n",
    "with tf.variable_scope('passage_lstm'):\n",
    "    passage_cell = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
    "    passage_cell = tf.nn.rnn_cell.DropoutWrapper(passage_cell, output_keep_prob=dropout_keep_prob)\n",
    "    passage_cell = tf.nn.rnn_cell.MultiRNNCell([passage_cell]*n_hidden_layers)\n",
    "    #H_p shape = batch_size, max_passage_len, n_hidden\n",
    "    H_p, _ = tf.nn.dynamic_rnn(passage_cell, passage_embedded, dtype=tf.float32) \n",
    "    \n",
    "with tf.variable_scope('question_lstm'):\n",
    "    question_cell = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
    "    question_cell = tf.nn.rnn_cell.DropoutWrapper(question_cell, output_keep_prob=dropout_keep_prob)\n",
    "    question_cell = tf.nn.rnn_cell.MultiRNNCell([question_cell]*n_hidden_layers)\n",
    "    H_q, _ = tf.nn.dynamic_rnn(question_cell, question_embedded, dtype=tf.float32)\n",
    "\n",
    "#Match LSTM layer \n",
    "#Weights and bias to compute G\n",
    "W_q = tf.Variable(tf.truncated_normal(shape=[n_hidden, n_hidden], stddev=0.1))\n",
    "W_p = tf.Variable(tf.truncated_normal(shape=[n_hidden, n_hidden], stddev=0.1))\n",
    "W_r = tf.Variable(tf.truncated_normal(shape=[n_hidden, n_hidden], stddev=0.1))\n",
    "b_p = tf.Variable(tf.truncated_normal(shape=[n_hidden], stddev=0.1))\n",
    "\n",
    "#Weight and bias to compute a\n",
    "w= tf.Variable(tf.truncated_normal(shape=[1, n_hidden], stddev=0.1))\n",
    "#b_alpha shape = 1,none\n",
    "b_alpha = tf.Variable(tf.constant(0.1, shape = [1]))\n",
    "\n",
    "#only calculate WH_q once \n",
    "#H_q shape = (batch_size&max_passage_len merge, n_hidden)\n",
    "H_q = tf.reshape(H_q, [-1, H_q.get_shape().as_list()[-1]])\n",
    "#H_q shape = (n_hidden, batch_size&max_passage_len merge)\n",
    "H_q = tf.transpose(H_q, [1,0])\n",
    "WH_q = tf.batch_matmul(W_q, H_q)\n",
    "WH_q = tf.reshape(WH_q, [n_hidden, question_max_len, -1])\n",
    "#WH_q shape =(batch_size, n_hidden, max_ques_len)\n",
    "WH_q = tf.transpose(WH_q, [2,0,1])\n",
    "\n",
    "H_r_forward = []\n",
    "H_r_forward_states = []\n",
    "H_r_backward = []\n",
    "H_r_backward_states = []\n",
    "\n",
    "with tf.variable_scope('forward_match_lstm'):\n",
    "    forward_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(n_hidden), output_keep_prob=dropout_keep_prob)\n",
    "    forward_state = forward_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    h = forward_state.h #h shape =(batch_size, n_hidden)\n",
    "    c = forward_state.c\n",
    "\n",
    "    for i in range(passage_max_len):\n",
    "        H_p_i = tf.slice(H_p, [0,i,0], [-1,1,-1])\n",
    "        H_p_i = tf.transpose(tf.squeeze(H_p_i,1),[1,0])\n",
    "        WH_p_i = tf.transpose(tf.matmul(W_p, H_p_i), [1,0])\n",
    "        h = tf.transpose(h,[1,0])\n",
    "        W_rh = tf.transpose(tf.matmul(W_r, h),[1,0])\n",
    "        G_tile = tf.tile((WH_p_i + W_rh + b_p), [question_max_len, 1])\n",
    "        G_tile = tf.reshape(G_tile, [-1, n_hidden, question_max_len])\n",
    "        G_forward = tf.tanh(WH_q + G_tile)\n",
    "        G_forward = tf.reshape(G_forward, [n_hidden, -1])\n",
    "        wG_forward = tf.matmul(w,G_forward)\n",
    "        a_tile = tf.tile(b_alpha, [question_max_len])\n",
    "        wG_forward = tf.reshape(tf.squeeze(wG_forward,0),[-1, question_max_len])\n",
    "        alpha_forward = tf.nn.softmax(wG_forward + a_tile)\n",
    "\n",
    "        H_q = tf.reshape(H_q,[-1, n_hidden, question_max_len])\n",
    "        alpha_forward = tf.expand_dims(alpha_forward, 2)\n",
    "        H_q_alpha_forward = tf.batch_matmul(H_q, alpha_forward)\n",
    "        H_q_alpha_forward = tf.reshape(H_q_alpha_forward,[-1, n_hidden])\n",
    "        H_p_i = tf.transpose(H_p_i,[1,0])\n",
    "        z_forward = tf.concat(1, [H_p_i, H_q_alpha_forward])\n",
    "\n",
    "        h,forward_state = forward_cell(z_forward, forward_state)\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        H_r_forward.append(h)\n",
    "        H_r_forward_states.append(forward_state.c)\n",
    "        if i%100 == 0:\n",
    "            print(i)\n",
    "\n",
    "    H_r_forward = tf.transpose(tf.stack(H_r_forward),[1,0,2])\n",
    "    \n",
    "with tf.variable_scope('backward_match_lstm'):\n",
    "    backward_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(n_hidden), output_keep_prob=dropout_keep_prob)\n",
    "    backward_state = backward_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    h = backward_state.h #h shape =(bathc_size, n_hidden)\n",
    "\n",
    "    for i in reversed(range(passage_max_len)):\n",
    "        H_p_i = tf.slice(H_p, [0,i,0], [-1,1,-1])\n",
    "        H_p_i = tf.transpose(tf.squeeze(H_p_i,1),[1,0])\n",
    "        WH_p_i = tf.transpose(tf.matmul(W_p, H_p_i), [1,0])\n",
    "        h = tf.transpose(h,[1,0])\n",
    "        W_rh = tf.transpose(tf.matmul(W_r, h),[1,0])\n",
    "        G_tile = tf.tile((WH_p_i + W_rh + b_p), [question_max_len, 1])\n",
    "        G_tile = tf.reshape(G_tile, [-1, n_hidden, question_max_len])\n",
    "        G_backward = tf.tanh(WH_q + G_tile)\n",
    "        G_backward = tf.reshape(G_backward, [n_hidden, -1])\n",
    "        wG_backward = tf.matmul(w,G_backward)\n",
    "        a_tile = tf.tile(b_alpha, [question_max_len])\n",
    "        wG_backward = tf.reshape(tf.squeeze(wG_backward,0),[-1, question_max_len])\n",
    "        alpha_backward = tf.nn.softmax(wG_backward + a_tile)\n",
    "\n",
    "#         H_q = tf.reshape(H_q, [self.n_hidden, self.n_steps_question, -1])\n",
    "#         H_q = tf.reshape(H_q, [-1, self.n_steps_question])\n",
    "#         alpha_backward = tf.transpose(alpha_backward, [1, 0])\n",
    "#         H_qalpha_backward = tf.matmul(H_q, alpha_backward)\n",
    "#         H_qalpha_backward = tf.reshape(H_qalpha_backward, [self.n_hidden, -1])\n",
    "        \n",
    "        H_q = tf.reshape(H_q,[-1, n_hidden, question_max_len])\n",
    "        alpha_backward = tf.expand_dims(alpha_backward, 2)\n",
    "        H_q_alpha_backward = tf.batch_matmul(H_q, alpha_backward)\n",
    "        H_q_alpha_backward = tf.reshape(H_q_alpha_backward,[-1, n_hidden])\n",
    "        H_p_i = tf.transpose(H_p_i,[1,0])\n",
    "        z_backward = tf.concat(1, [H_p_i, H_q_alpha_backward])\n",
    "\n",
    "        h,backward_state = backward_cell(z_backward, backward_state)\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        H_r_backward.append(h)\n",
    "        H_r_backward_states.append(backward_state.c)\n",
    "        if i%100==0:\n",
    "            print(i)\n",
    "    H_r_backward = list(reversed(H_r_backward))\n",
    "    H_r_backward = tf.transpose(tf.stack(H_r_backward),[1,0,2])\n",
    "    \n",
    "    H_r_backward_states = list(reversed(H_r_backward_states))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H_r_forward_try = tf.stack(H_r_forward_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H_r_forward_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H_r_states = tf.concat(2, [H_r_forward_states, H_r_backward_states])\n",
    "print(H_r_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "H_r = tf.concat(2, [H_r_forward, H_r_backward])\n",
    "print(H_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H_r[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"answer_pointer_lstm/transpose:0\", shape=(128, 200), dtype=float32)\n",
      "Tensor(\"answer_pointer_lstm/transpose_2:0\", shape=(128, 200), dtype=float32)\n",
      "Tensor(\"stack:0\", shape=(2, 128, 200), dtype=float32)\n",
      "Tensor(\"strided_slice:0\", shape=(128, 200), dtype=float32)\n",
      "Tensor(\"ArgMax:0\", shape=(128,), dtype=int64)\n",
      "Tensor(\"split:0\", shape=(1, 128, 200), dtype=float32)\n",
      "Tensor(\"Squeeze:0\", shape=(128, 200), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "ptr_outputs=  []\n",
    "ptr_output_dists = [] \n",
    "\n",
    "with tf.variable_scope('answer_pointer_lstm'):\n",
    "    pointer_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(n_hidden), output_keep_prob=dropout_keep_prob)\n",
    "    pointer_state = pointer_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    h = pointer_state.h\n",
    "    c = pointer_state.c\n",
    "    \n",
    "    for i in range(num_indices):\n",
    "        W_1 = tf.Variable(tf.truncated_normal(shape=[n_hidden, blend_units], stddev=0.1))\n",
    "        W_2 = tf.Variable(tf.truncated_normal(shape=[n_hidden, blend_units], stddev=0.1))\n",
    "        bias_ptr = tf.Variable(tf.truncated_normal(shape=[batch_size, blend_units], stddev=0.1))\n",
    "    \n",
    "        index_predists = []\n",
    "        enc_portions = []\n",
    "        v_blend = tf.Variable(tf.truncated_normal(shape=[blend_units,1], stddev=0.1))\n",
    "        \n",
    "        for passage_len_idx in range(passage_max_len):\n",
    "            enc_portion = tf.matmul(H_r_forward_states[passage_len_idx], W_1)\n",
    "            raw_blend = tf.nn.elu(enc_portion + bias_ptr)\n",
    "            scaled_blend = tf.matmul(raw_blend, v_blend)\n",
    "            index_predist = tf.reshape(scaled_blend,(batch_size,))\n",
    "            \n",
    "            enc_portions.append(enc_portion)\n",
    "            index_predists.append(index_predist)\n",
    "        \n",
    "        idx_predistribution = tf.transpose(tf.stack(index_predists))\n",
    "        print(idx_predistribution)\n",
    "        idx_distribution = tf.nn.softmax(idx_predistribution, dim=-1)\n",
    "        ptr_output_dists.append(idx_distribution)\n",
    "        idx = tf.argmax(idx_distribution,1)\n",
    "        \n",
    "        emb = tf.nn.embedding_lookup(tf.transpose(passage), idx)\n",
    "        ptr_output_raw = tf.diag_part(emb)\n",
    "        \n",
    "        ptr_output = tf.reshape(ptr_output_raw, (batch_size, 1))\n",
    "        ptr_outputs.append(ptr_output)\n",
    "\n",
    "idx_distributions = tf.stack(ptr_output_dists)\n",
    "print(idx_distributions)\n",
    "print(idx_distributions[0])\n",
    "\n",
    "start_label, end_label = tf.split(0,2,y)\n",
    "start_label_sq = tf.squeeze(start_label, 0)\n",
    "end_label_sq = tf.squeeze(end_label,0)\n",
    "\n",
    "#loss = tf.sqrt(tf.reduce_mean(tf.pow(idx_distributions - y, 2.0)))\n",
    "loss = tf.sqrt(tf.reduce_mean(tf.pow(idx_distributions[0] - start_label_sq, 2.0)))\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "print(tf.argmax(idx_distributions[0], axis=1))\n",
    "print(start_label)\n",
    "print(start_label_sq)\n",
    "\n",
    "correct_start_sq = tf.equal(tf.argmax(idx_distributions[0], axis=1), tf.argmax(start_label_sq, axis=1))\n",
    "\n",
    "start_accuracy_sq = tf.reduce_mean(tf.cast(correct_start_sq, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx_distributions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(tf.argmax(start_label_sq, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]), array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b = val_data_iter.next_batch()\n",
    "c,d,e = zip(*a)\n",
    "list(np.array(e).transpose(1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1, start_accuracy = 0.0000, train loss = 0.070533\n",
      "Iter 200, start_accuracy = 0.0000, train loss = 0.070532\n",
      "[ 0.00490392  0.00489391  0.00488492  0.00488723  0.00485942  0.00487259\n",
      "  0.0049107   0.00497246  0.00493481  0.0049076   0.00489393  0.00491842\n",
      "  0.00497048  0.00503315  0.00509935  0.00510728  0.00511322  0.00508387\n",
      "  0.00510849  0.00513402  0.00512207  0.00506867  0.00502403  0.00503975\n",
      "  0.00501464  0.00503333  0.00504001  0.0050378   0.00498071  0.00498\n",
      "  0.00494666  0.00491619  0.0049186   0.00494867  0.00498976  0.00500063\n",
      "  0.00502159  0.00504874  0.00505286  0.00503058  0.00504848  0.00500009\n",
      "  0.00502003  0.00504108  0.0050315   0.00506398  0.00500644  0.00504\n",
      "  0.00498007  0.00504858  0.0050146   0.00505585  0.00502949  0.00505869\n",
      "  0.00507093  0.0050127   0.00496037  0.00499378  0.00498711  0.00499867\n",
      "  0.00500694  0.00507314  0.00505453  0.00505048  0.00507539  0.00507492\n",
      "  0.00509129  0.00511683  0.00510225  0.00508904  0.00503645  0.00499461\n",
      "  0.0050165   0.00505171  0.00506197  0.00507695  0.00508152  0.00502755\n",
      "  0.00504542  0.00503326  0.00500938  0.00498385  0.00500089  0.00498717\n",
      "  0.00501138  0.00501215  0.00496551  0.00501028  0.00501448  0.004985\n",
      "  0.00504815  0.00505471  0.00502166  0.0050373   0.00504277  0.00502397\n",
      "  0.00500196  0.00501195  0.00507623  0.00505744  0.00504789  0.00500446\n",
      "  0.00499155  0.00501534  0.00500051  0.00497052  0.00497118  0.0049408\n",
      "  0.0049814   0.0049764   0.00492256  0.00486046  0.00489841  0.00491898\n",
      "  0.00492932  0.00494241  0.00490948  0.00490274  0.00493258  0.00499153\n",
      "  0.0049825   0.00497794  0.00498219  0.00496826  0.00500147  0.00502735\n",
      "  0.00501766  0.00494841  0.00493599  0.00494584  0.004972    0.00495986\n",
      "  0.00497323  0.00497812  0.00496784  0.0050237   0.00503149  0.00499568\n",
      "  0.00505126  0.00508187  0.00502378  0.00499785  0.00498578  0.00499827\n",
      "  0.00501076  0.00498472  0.0050369   0.00504077  0.00498117  0.00495583\n",
      "  0.00497025  0.00496084  0.00497377  0.00500642  0.00503543  0.00501228\n",
      "  0.00506299  0.00502149  0.00504019  0.00500294  0.00500191  0.00497053\n",
      "  0.0049863   0.00501055  0.00501455  0.00498014  0.0049963   0.00494184\n",
      "  0.0049503   0.00493518  0.00493165  0.00491255  0.00492028  0.00490742\n",
      "  0.00489154  0.00489942  0.00489038  0.00488948  0.00493427  0.00496434\n",
      "  0.00498202  0.00499156  0.00497795  0.00501392  0.00505763  0.00501159\n",
      "  0.00498822  0.00501299  0.00500743  0.00500982  0.00501365  0.00504236\n",
      "  0.00506849  0.00502357  0.00503277  0.00503216  0.0050424   0.00504769\n",
      "  0.00500677  0.00499001]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "19\n",
      "90\n",
      "Iter 400, start_accuracy = 0.0078, train loss = 0.070532\n",
      "[ 0.00499062  0.0050006   0.00499281  0.00498882  0.00498975  0.00500841\n",
      "  0.00499517  0.00501422  0.00504029  0.00500556  0.00502667  0.00507743\n",
      "  0.00508823  0.00506615  0.00504424  0.00506822  0.00507764  0.00512488\n",
      "  0.00513347  0.00511179  0.00509308  0.00506028  0.00509151  0.00504492\n",
      "  0.00504341  0.00504553  0.00501264  0.00494544  0.00497714  0.00500134\n",
      "  0.00495759  0.00498878  0.00499555  0.00499634  0.00495679  0.00500777\n",
      "  0.00498149  0.00499847  0.0049793   0.00498566  0.00497028  0.00497586\n",
      "  0.00498494  0.00500317  0.00503547  0.00505833  0.00508066  0.00504545\n",
      "  0.00508054  0.00506408  0.00502748  0.00503772  0.00504692  0.00503234\n",
      "  0.00504995  0.0051077   0.00510873  0.00507112  0.00506261  0.00510981\n",
      "  0.00508751  0.00507182  0.00510322  0.00515221  0.00519433  0.00516224\n",
      "  0.00518256  0.00513594  0.0050974   0.00514462  0.00507786  0.00505864\n",
      "  0.00514226  0.00511201  0.00504669  0.00497584  0.00501901  0.00506211\n",
      "  0.00505026  0.00503534  0.00503584  0.0050457   0.00503408  0.00505162\n",
      "  0.00504218  0.00504964  0.00501998  0.00499558  0.00498664  0.00495526\n",
      "  0.00495626  0.00495854  0.00498002  0.00494184  0.0049199   0.00491586\n",
      "  0.00493198  0.00496149  0.00496181  0.00492168  0.0049244   0.00487809\n",
      "  0.00487518  0.00490256  0.00494496  0.00495184  0.00497265  0.00496075\n",
      "  0.00497339  0.00499175  0.0049841   0.00500333  0.00498576  0.00498352\n",
      "  0.00502167  0.00498181  0.00498995  0.00501587  0.00503442  0.00506735\n",
      "  0.00502043  0.00503263  0.00500403  0.00495786  0.00498804  0.00502693\n",
      "  0.00496335  0.00496107  0.00494196  0.00496606  0.00489305  0.00490722\n",
      "  0.00489335  0.00489078  0.00495311  0.00492536  0.00498182  0.00492167\n",
      "  0.00492073  0.00492123  0.00499592  0.00498701  0.0049921   0.00504027\n",
      "  0.00498935  0.00495842  0.00499338  0.00498965  0.00504516  0.00504872\n",
      "  0.0050114   0.00496312  0.004961    0.00496155  0.0049898   0.00491818\n",
      "  0.00484821  0.00483574  0.00485843  0.00480178  0.004888    0.00492834\n",
      "  0.00500598  0.0050273   0.00502486  0.00505515  0.00503513  0.00497277\n",
      "  0.00491996  0.00492556  0.00491744  0.0049421   0.00499724  0.00502044\n",
      "  0.00501707  0.00499887  0.00496076  0.00498122  0.00495555  0.0049694\n",
      "  0.00493053  0.00490833  0.00490088  0.00494621  0.00494884  0.00490088\n",
      "  0.00500613  0.00501264  0.00497488  0.00496926  0.0049394   0.00493999\n",
      "  0.00489082  0.00493797  0.00493971  0.00493416  0.00500785  0.00504621\n",
      "  0.00504957  0.0050304 ]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "64\n",
      "32\n",
      "Iter 600, start_accuracy = 0.0156, train loss = 0.070532\n",
      "[ 0.00490176  0.00491497  0.0049004   0.00489662  0.00488804  0.00493682\n",
      "  0.00494712  0.00495011  0.00497154  0.00497021  0.00495177  0.00493792\n",
      "  0.00492505  0.00496395  0.00494149  0.00497446  0.00502239  0.00502398\n",
      "  0.00499417  0.00498306  0.00501252  0.00499799  0.00495738  0.0049438\n",
      "  0.0049759   0.00501649  0.00503297  0.00504796  0.00504905  0.0050248\n",
      "  0.00500974  0.00498838  0.00500114  0.00505737  0.00507584  0.00504477\n",
      "  0.00505923  0.00508796  0.00513677  0.00512683  0.00509884  0.0051\n",
      "  0.00506808  0.00507833  0.00504133  0.00504841  0.00500048  0.00502728\n",
      "  0.00504183  0.00497532  0.00501138  0.00501085  0.00501764  0.0050087\n",
      "  0.00497746  0.00500178  0.00500503  0.00499949  0.00500795  0.00502711\n",
      "  0.00500503  0.00496082  0.0049572   0.00494756  0.00497465  0.00500099\n",
      "  0.00497419  0.004962    0.00495659  0.00496236  0.00498343  0.00498525\n",
      "  0.00497085  0.0049558   0.0049865   0.00501808  0.00503901  0.00506451\n",
      "  0.00501447  0.00503387  0.00500391  0.00496481  0.00497865  0.00500887\n",
      "  0.0050397   0.00502468  0.00502307  0.0050861   0.00505429  0.00506506\n",
      "  0.0050824   0.0050595   0.00509509  0.00505748  0.00507302  0.00505691\n",
      "  0.00506144  0.00506772  0.00506998  0.00503006  0.0050198   0.0050259\n",
      "  0.00500957  0.00499741  0.0050314   0.00501496  0.00506518  0.00508103\n",
      "  0.00503011  0.00498063  0.00501958  0.00502202  0.00502987  0.00497673\n",
      "  0.00502442  0.00503468  0.00499971  0.00499622  0.00497473  0.00495586\n",
      "  0.00497397  0.00498227  0.00494398  0.00494689  0.00494759  0.00493078\n",
      "  0.00490925  0.00491763  0.00490793  0.00488953  0.00491863  0.00497285\n",
      "  0.00499171  0.00498738  0.00496763  0.00498233  0.00499776  0.00501566\n",
      "  0.00502746  0.00505961  0.00503692  0.0050623   0.00505727  0.00505906\n",
      "  0.00506138  0.00507228  0.00504098  0.00508769  0.00504651  0.00502047\n",
      "  0.00501302  0.00501793  0.00497438  0.00503374  0.00500299  0.00498627\n",
      "  0.00497572  0.00499906  0.00498389  0.00500901  0.00495538  0.0049293\n",
      "  0.00492983  0.00493768  0.00494161  0.00498873  0.00498157  0.00499271\n",
      "  0.00497211  0.00500907  0.00501355  0.00502694  0.00498982  0.00496742\n",
      "  0.00498513  0.00501827  0.00503056  0.00497051  0.00502077  0.00501178\n",
      "  0.00498987  0.00494382  0.00495267  0.00493774  0.00495004  0.0049535\n",
      "  0.00495124  0.0049441   0.00497083  0.00497296  0.00499771  0.00499259\n",
      "  0.00498756  0.00493788  0.00497046  0.0050099   0.00501581  0.00497442\n",
      "  0.00498072  0.00494608]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.]\n",
      "38\n",
      "199\n",
      "Iter 800, start_accuracy = 0.0000, train loss = 0.070533\n",
      "[ 0.00495229  0.00496759  0.00496743  0.00499192  0.00499469  0.00502612\n",
      "  0.00500836  0.00505944  0.00507251  0.00509632  0.00503539  0.00507526\n",
      "  0.00503547  0.00501891  0.0050228   0.00502384  0.00504335  0.00503003\n",
      "  0.00500532  0.00501053  0.00497607  0.00496022  0.00494144  0.00496161\n",
      "  0.00499498  0.00498714  0.00497758  0.00495925  0.00497652  0.00495907\n",
      "  0.0049772   0.00503419  0.00503365  0.00508803  0.00509186  0.00508421\n",
      "  0.00507323  0.00503673  0.00501018  0.00502073  0.00501089  0.00506719\n",
      "  0.0050677   0.00499871  0.00506108  0.00503631  0.00504592  0.00505404\n",
      "  0.00501748  0.00497686  0.00500022  0.00500358  0.00497961  0.00501407\n",
      "  0.00500388  0.00496567  0.00500413  0.0049538   0.00492078  0.00490988\n",
      "  0.00494392  0.00490996  0.00489842  0.00494907  0.00498252  0.00502192\n",
      "  0.00498352  0.00503313  0.00504751  0.00505336  0.00499344  0.00501527\n",
      "  0.00501646  0.00496844  0.00496206  0.00502899  0.00498065  0.00499584\n",
      "  0.00499412  0.00500604  0.00495794  0.00494222  0.00492904  0.00486443\n",
      "  0.0049141   0.00491949  0.00491117  0.00490385  0.00492089  0.00490513\n",
      "  0.00490385  0.00491189  0.00491901  0.00492757  0.00492692  0.00491254\n",
      "  0.00495371  0.0049594   0.00497146  0.00495608  0.00494039  0.00494517\n",
      "  0.00495835  0.00494673  0.00500989  0.00494137  0.00491985  0.00492684\n",
      "  0.00500924  0.0049943   0.00501703  0.00497877  0.0050425   0.00507377\n",
      "  0.0050344   0.00504128  0.00506987  0.00511555  0.00509387  0.00510345\n",
      "  0.00507878  0.00510342  0.00509827  0.00508221  0.00506851  0.00503358\n",
      "  0.00498892  0.00496386  0.0049602   0.00495158  0.00500102  0.00499415\n",
      "  0.00500322  0.00498093  0.00503167  0.00507254  0.00507493  0.00513602\n",
      "  0.00511677  0.00511843  0.00513793  0.00515148  0.00512805  0.00503417\n",
      "  0.00503873  0.00509399  0.00505746  0.00510966  0.00510382  0.00511198\n",
      "  0.00508881  0.00508314  0.00500742  0.00503375  0.00503697  0.00511529\n",
      "  0.00508254  0.00504578  0.00504919  0.00498507  0.00502848  0.00498301\n",
      "  0.00495241  0.00494933  0.00494284  0.00502209  0.00502353  0.00495764\n",
      "  0.00495078  0.00498156  0.0049651   0.00493405  0.00492129  0.00493049\n",
      "  0.00496922  0.00497386  0.00493548  0.00491609  0.00500027  0.00501322\n",
      "  0.00493149  0.00494627  0.00499929  0.00503874  0.00501429  0.00503064\n",
      "  0.00502351  0.00497012  0.0049839   0.00498971  0.00503027  0.00500755\n",
      "  0.00494531  0.00488243  0.00492851  0.00493526  0.0049051   0.00485444\n",
      "  0.00484695  0.00490002]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "141\n",
      "86\n",
      "Iter 1000, start_accuracy = 0.0078, train loss = 0.070533\n",
      "[ 0.00503366  0.00501226  0.00500198  0.00500538  0.00499907  0.00502\n",
      "  0.00507013  0.00509645  0.00506923  0.005121    0.00513155  0.00510791\n",
      "  0.0051075   0.00508923  0.00512967  0.00507524  0.00509065  0.00507275\n",
      "  0.00502486  0.00502687  0.00501869  0.00499389  0.00500163  0.00496641\n",
      "  0.00498875  0.00499375  0.00503528  0.00497899  0.00502928  0.00508308\n",
      "  0.00505718  0.0050594   0.00505797  0.00504934  0.00503338  0.00508565\n",
      "  0.00510105  0.00501589  0.00500495  0.00500302  0.00493433  0.00491729\n",
      "  0.00494755  0.00488557  0.00492682  0.00495874  0.00494597  0.00494716\n",
      "  0.00491902  0.00490867  0.00484219  0.00491735  0.00498134  0.004938\n",
      "  0.00491854  0.00493038  0.00489398  0.00484719  0.0048548   0.00490834\n",
      "  0.00493724  0.00488707  0.00486802  0.00487761  0.00485903  0.00481262\n",
      "  0.00485477  0.00490508  0.00493902  0.00495471  0.00494393  0.00493589\n",
      "  0.00498477  0.00502396  0.00504667  0.00507556  0.00508148  0.00508916\n",
      "  0.00506946  0.00507313  0.00504227  0.00501428  0.0049683   0.00495926\n",
      "  0.00494506  0.00498625  0.00502863  0.00505461  0.00511632  0.00511086\n",
      "  0.00509429  0.00510334  0.00510372  0.00501201  0.00497132  0.00492384\n",
      "  0.00491994  0.00488069  0.00493885  0.0049217   0.00495141  0.00489669\n",
      "  0.00493292  0.00496494  0.0049065   0.00496165  0.00497678  0.00495184\n",
      "  0.0049428   0.00496303  0.00494477  0.00491742  0.00490591  0.00489669\n",
      "  0.00495793  0.00499812  0.0049385   0.00499446  0.00497577  0.00494091\n",
      "  0.00490638  0.00486539  0.00484363  0.00485095  0.00490353  0.00498863\n",
      "  0.00500808  0.00506691  0.005076    0.00506008  0.00504481  0.00499975\n",
      "  0.00494971  0.0049591   0.00490216  0.00489381  0.00490473  0.00500546\n",
      "  0.00501315  0.00500508  0.00505322  0.00505645  0.00511627  0.00514486\n",
      "  0.00505295  0.00498687  0.00498342  0.00505365  0.00500156  0.00504235\n",
      "  0.00505002  0.00504465  0.00508654  0.00508546  0.00511379  0.00509652\n",
      "  0.0050927   0.0050611   0.0050306   0.00494626  0.00494201  0.00491727\n",
      "  0.00493501  0.0049284   0.00500814  0.00497706  0.00503321  0.00508308\n",
      "  0.00512296  0.00512426  0.0050358   0.00504128  0.00500964  0.00495318\n",
      "  0.00496524  0.00499885  0.00495197  0.00493114  0.00500741  0.0050712\n",
      "  0.00504465  0.00505186  0.00504302  0.00510122  0.00507242  0.00503451\n",
      "  0.00504825  0.00503482  0.0050752   0.00506684  0.00500162  0.00501796\n",
      "  0.00504167  0.00506478  0.00514365  0.00512486  0.00508568  0.00512969\n",
      "  0.00505534  0.00497833]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "143\n",
      "12\n",
      "Iter 1000, val_start_accuracy = 0.0000, val loss = 0.070533\n",
      "Iter 1200, start_accuracy = 0.0078, train loss = 0.070532\n",
      "[ 0.00495378  0.00493752  0.00494939  0.00497697  0.0049726   0.00495859\n",
      "  0.0049929   0.00496119  0.00500409  0.00503673  0.00503373  0.00505328\n",
      "  0.0050671   0.00506675  0.00502085  0.00503339  0.00503197  0.00503288\n",
      "  0.00504659  0.00505127  0.0050588   0.00513884  0.0051014   0.00509265\n",
      "  0.00508889  0.00509929  0.00507621  0.00505878  0.00506422  0.0050694\n",
      "  0.00508391  0.00510376  0.00506903  0.0050169   0.005054    0.00505772\n",
      "  0.00504485  0.00503147  0.00502885  0.00503777  0.00501783  0.00502122\n",
      "  0.00502782  0.00504251  0.00507077  0.00501197  0.0049836   0.00499036\n",
      "  0.00499136  0.00499157  0.00508274  0.00515072  0.00514157  0.00512076\n",
      "  0.00508731  0.00508505  0.00507873  0.00509494  0.00508811  0.00508231\n",
      "  0.00507481  0.0050302   0.0050497   0.00506325  0.00503928  0.00498043\n",
      "  0.00492814  0.00490553  0.00487392  0.00488641  0.0049264   0.00494207\n",
      "  0.00493668  0.00496691  0.00496558  0.00495938  0.00499343  0.00498958\n",
      "  0.00496236  0.00492992  0.00497692  0.00498625  0.00500712  0.00496667\n",
      "  0.00493044  0.00491633  0.00493109  0.00497593  0.0050043   0.00500074\n",
      "  0.00500555  0.0050544   0.00507493  0.00506906  0.00505309  0.00499667\n",
      "  0.00501593  0.00501805  0.00497676  0.00498913  0.00496016  0.00497274\n",
      "  0.0049394   0.00490923  0.00485046  0.00488614  0.0048518   0.00486369\n",
      "  0.00484614  0.00485468  0.00489335  0.00489041  0.00488588  0.00484964\n",
      "  0.0048519   0.00486546  0.00488486  0.00492441  0.004949    0.00495165\n",
      "  0.00501109  0.00506032  0.0050735   0.0049977   0.00501349  0.0050727\n",
      "  0.00504047  0.00502228  0.00502256  0.00497129  0.00499917  0.00504388\n",
      "  0.00507724  0.00513075  0.00508421  0.00506414  0.00501364  0.00504597\n",
      "  0.00499701  0.00497256  0.00497757  0.00501358  0.00502166  0.00501419\n",
      "  0.00502186  0.00502352  0.00502195  0.00505972  0.00494128  0.00493123\n",
      "  0.00489746  0.00493565  0.0049363   0.00496997  0.00504328  0.00506862\n",
      "  0.00507212  0.00504539  0.00497971  0.00497702  0.00498642  0.00500152\n",
      "  0.00499562  0.00496297  0.00499887  0.00494478  0.0050017   0.00507508\n",
      "  0.00500067  0.00503242  0.00501875  0.00501173  0.00504319  0.00500583\n",
      "  0.00497432  0.00500661  0.0049831   0.00494135  0.00501909  0.00498105\n",
      "  0.00495987  0.00494591  0.00491509  0.00494373  0.00500937  0.00497022\n",
      "  0.00492865  0.00500217  0.00494448  0.0049914   0.00498729  0.00495483\n",
      "  0.0050104   0.00503274  0.00501502  0.0049451   0.00491135  0.00496243\n",
      "  0.00497988  0.00498507]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "51\n",
      "11\n",
      "Iter 1400, start_accuracy = 0.0156, train loss = 0.070532\n",
      "[ 0.00486951  0.00489119  0.00489947  0.00488557  0.00491834  0.00494596\n",
      "  0.00494138  0.00498006  0.00494081  0.00495315  0.00496289  0.00497909\n",
      "  0.00501711  0.00502948  0.00503196  0.00505409  0.00505221  0.00505728\n",
      "  0.00509591  0.0050804   0.00506657  0.00505559  0.00505306  0.00502028\n",
      "  0.00497929  0.00492309  0.00497598  0.00498223  0.00497634  0.00497135\n",
      "  0.00493794  0.00493116  0.00494971  0.00491216  0.00489489  0.00487041\n",
      "  0.00486323  0.00492243  0.00492716  0.00493557  0.00498106  0.00500296\n",
      "  0.00501583  0.00505454  0.00505716  0.00503136  0.0050057   0.00505358\n",
      "  0.0050602   0.00507884  0.00504652  0.0050445   0.005034    0.00504308\n",
      "  0.00509544  0.00509376  0.00507079  0.0050772   0.00506577  0.00506546\n",
      "  0.00505745  0.00503913  0.00505781  0.00505768  0.00505974  0.00499744\n",
      "  0.00497683  0.00500068  0.00499695  0.00496007  0.00492713  0.00491887\n",
      "  0.00488265  0.00487331  0.00489203  0.00492658  0.00496074  0.00494999\n",
      "  0.00503199  0.00502553  0.00504431  0.00507595  0.00506957  0.00504764\n",
      "  0.00504972  0.00507701  0.00509891  0.00511232  0.00513924  0.00512373\n",
      "  0.00509727  0.00512873  0.00513835  0.00510339  0.00501799  0.00503812\n",
      "  0.00505555  0.00499286  0.00494958  0.00490498  0.00492109  0.0048917\n",
      "  0.00490409  0.00492677  0.00492528  0.0049827   0.00496624  0.00495662\n",
      "  0.00495901  0.00496088  0.00500594  0.00502743  0.00504453  0.00503675\n",
      "  0.00509102  0.00512538  0.00507485  0.00507242  0.00510552  0.00514012\n",
      "  0.00510348  0.00510909  0.00510033  0.0051039   0.00511813  0.00509607\n",
      "  0.00507595  0.00507165  0.00506166  0.00502977  0.00504683  0.00503072\n",
      "  0.00504197  0.00508108  0.0051129   0.00509863  0.00507538  0.00503904\n",
      "  0.0050809   0.00510691  0.00513064  0.00508873  0.00501376  0.00507039\n",
      "  0.00504998  0.00504199  0.00504171  0.00502552  0.0050464   0.00502785\n",
      "  0.00503209  0.00499382  0.00497135  0.00494701  0.00490707  0.00491371\n",
      "  0.00490925  0.00490616  0.00494875  0.00491762  0.00489913  0.00490939\n",
      "  0.0049391   0.00492911  0.00494025  0.00495433  0.00499543  0.00495538\n",
      "  0.00494406  0.00494528  0.00494795  0.00496161  0.00493291  0.00497764\n",
      "  0.00495887  0.00498898  0.00497335  0.00502668  0.00501554  0.00499929\n",
      "  0.00498009  0.00493796  0.00495245  0.00496497  0.00496239  0.00492792\n",
      "  0.00493821  0.00490976  0.00488108  0.00489703  0.00490064  0.004852\n",
      "  0.00486126  0.00488758  0.0049236   0.00493881  0.0049631   0.00499076\n",
      "  0.00494469  0.00493505]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "119\n",
      "156\n",
      "Iter 1600, start_accuracy = 0.0156, train loss = 0.070532\n",
      "[ 0.00489731  0.00490052  0.00492197  0.00492528  0.00493019  0.00495294\n",
      "  0.00495445  0.00493572  0.00490248  0.00491308  0.0049179   0.00495594\n",
      "  0.00498801  0.00496513  0.00493984  0.00493161  0.00493533  0.00492028\n",
      "  0.00495144  0.00498311  0.00495762  0.0049827   0.00499469  0.00502949\n",
      "  0.00504492  0.00508752  0.00506568  0.0050654   0.00507478  0.00506461\n",
      "  0.00508782  0.005069    0.0050364   0.00507664  0.00503967  0.00504167\n",
      "  0.00503467  0.00507909  0.00504245  0.00501046  0.00504105  0.00505021\n",
      "  0.00510574  0.00509761  0.00511562  0.00510852  0.0051285   0.00514037\n",
      "  0.00512601  0.00513626  0.00513471  0.00514574  0.00511104  0.00510457\n",
      "  0.00507621  0.00507288  0.00505869  0.0050178   0.00500943  0.00503755\n",
      "  0.00513138  0.00513184  0.00512642  0.00508761  0.00513149  0.00507835\n",
      "  0.00510122  0.00511625  0.00509863  0.0050475   0.005076    0.00510367\n",
      "  0.00508945  0.00511664  0.00509506  0.00513717  0.00506925  0.00506496\n",
      "  0.00506132  0.00512652  0.00511993  0.00510781  0.00509135  0.00509293\n",
      "  0.00507891  0.00505935  0.00504393  0.005021    0.00503911  0.00504044\n",
      "  0.00503416  0.00502909  0.00506127  0.00506728  0.00510512  0.0051194\n",
      "  0.00508324  0.0050654   0.00504903  0.00503366  0.00500856  0.0050293\n",
      "  0.00503232  0.00504314  0.00500055  0.00499907  0.00496542  0.00496424\n",
      "  0.00496998  0.00496168  0.0049586   0.00498603  0.00497321  0.00499392\n",
      "  0.0050008   0.00499432  0.0050177   0.00504095  0.00501855  0.00500597\n",
      "  0.00498168  0.00500034  0.00501741  0.00501878  0.00501418  0.0050188\n",
      "  0.00496693  0.00494846  0.00494607  0.00495767  0.00489784  0.00486183\n",
      "  0.00476365  0.00472617  0.00478379  0.00483963  0.00480962  0.00480824\n",
      "  0.00481404  0.00486433  0.00486622  0.00486654  0.00492586  0.00490949\n",
      "  0.00490598  0.00489025  0.00493284  0.00497379  0.00493416  0.00495739\n",
      "  0.00498141  0.00500372  0.00506957  0.00507254  0.00501124  0.00495749\n",
      "  0.00493567  0.0049402   0.00494942  0.00492166  0.00488046  0.00486455\n",
      "  0.00487914  0.00488577  0.00493223  0.00493081  0.00491443  0.00492075\n",
      "  0.00485339  0.00487632  0.00491141  0.00493541  0.00498662  0.00496455\n",
      "  0.00490361  0.004933    0.00501681  0.00501481  0.00498277  0.00497395\n",
      "  0.00501118  0.00498802  0.00499179  0.00497089  0.00497749  0.00495737\n",
      "  0.00499326  0.0050116   0.00499983  0.00498162  0.00494409  0.00500174\n",
      "  0.00495321  0.00497367  0.00491494  0.00494336  0.00501628  0.00503447\n",
      "  0.00501855  0.00498713]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "51\n",
      "12\n",
      "Iter 1800, start_accuracy = 0.0156, train loss = 0.070532\n",
      "[ 0.00500886  0.00502304  0.00501456  0.00503442  0.00505835  0.0050459\n",
      "  0.00502299  0.00502468  0.00502523  0.00502565  0.00501952  0.00500062\n",
      "  0.00499384  0.00502455  0.00503457  0.00503418  0.00504031  0.00502661\n",
      "  0.00501969  0.00493418  0.00499294  0.00504891  0.00500165  0.00508277\n",
      "  0.00506665  0.00498859  0.00501424  0.00503141  0.00501889  0.00506493\n",
      "  0.00507658  0.00508483  0.00502499  0.00499321  0.00499852  0.00495647\n",
      "  0.00497736  0.00500163  0.0050568   0.00508483  0.00508913  0.00514109\n",
      "  0.00509215  0.00505335  0.00504336  0.00507856  0.00505525  0.0050631\n",
      "  0.00501865  0.00502209  0.00505618  0.00507508  0.00513621  0.00513661\n",
      "  0.00509307  0.00504252  0.00504085  0.00502659  0.00501725  0.00499065\n",
      "  0.00501673  0.00496797  0.00498201  0.00496313  0.00493023  0.00498034\n",
      "  0.00505015  0.00501981  0.00496458  0.00499254  0.00506539  0.00502016\n",
      "  0.00503172  0.00504819  0.00504262  0.00503847  0.00499834  0.0050759\n",
      "  0.00500042  0.00497597  0.00501237  0.00505251  0.0050835   0.00509362\n",
      "  0.00509151  0.00505885  0.00504897  0.00507858  0.0050555   0.00504884\n",
      "  0.00498508  0.00499119  0.00500356  0.00501856  0.00501282  0.0050545\n",
      "  0.00507894  0.00511372  0.00505866  0.00511629  0.00504815  0.00508586\n",
      "  0.00507833  0.0051311   0.00510116  0.00510136  0.00508993  0.00504844\n",
      "  0.00504707  0.00502013  0.0049644   0.00487383  0.00484154  0.00486247\n",
      "  0.00490291  0.00485097  0.00483221  0.00489285  0.00479823  0.00479802\n",
      "  0.00486838  0.00489995  0.00483351  0.00487966  0.00489387  0.00487697\n",
      "  0.00490031  0.00486241  0.00488346  0.00486537  0.00487271  0.00490526\n",
      "  0.00494023  0.00499648  0.00497766  0.00498127  0.00499662  0.00500911\n",
      "  0.0049862   0.00499668  0.0049669   0.00496127  0.00494789  0.00493202\n",
      "  0.00501483  0.00498326  0.00500626  0.00503663  0.00502395  0.00504715\n",
      "  0.00503223  0.00506454  0.00510845  0.00510861  0.00505255  0.00506883\n",
      "  0.0050653   0.00504938  0.00504282  0.0050699   0.00503115  0.00502737\n",
      "  0.0050182   0.00496456  0.00499449  0.00493979  0.00488275  0.00488601\n",
      "  0.00486401  0.00491002  0.00492623  0.00495429  0.00499342  0.00500206\n",
      "  0.00503022  0.00501191  0.00508748  0.00508614  0.00500179  0.00497002\n",
      "  0.00493609  0.00490892  0.00491176  0.00491176  0.00487531  0.00491033\n",
      "  0.00490667  0.00494202  0.00493194  0.00494807  0.00484251  0.00488637\n",
      "  0.00488812  0.00492138  0.00497825  0.00496849  0.00492402  0.00499863\n",
      "  0.00499125  0.00498648]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "41\n",
      "80\n",
      "Iter 2000, start_accuracy = 0.0000, train loss = 0.070532\n",
      "[ 0.00493781  0.0049492   0.00493413  0.00492658  0.00491367  0.00493921\n",
      "  0.00495731  0.00498681  0.00496868  0.00498406  0.00498583  0.00499599\n",
      "  0.00500763  0.00501729  0.00500788  0.00500104  0.00495757  0.00496012\n",
      "  0.00502787  0.00505252  0.00504719  0.0051007   0.00512767  0.00513261\n",
      "  0.00513357  0.00508135  0.00503892  0.00502225  0.00503265  0.00498548\n",
      "  0.00499754  0.00496869  0.00494913  0.00496057  0.00493759  0.00496862\n",
      "  0.00497124  0.00501653  0.00508936  0.00513971  0.00511663  0.00511378\n",
      "  0.00513299  0.00511255  0.00504652  0.00501037  0.0050419   0.00500565\n",
      "  0.00498985  0.00503901  0.00501963  0.00500065  0.00507034  0.00503483\n",
      "  0.00512069  0.00508171  0.00509408  0.00511083  0.00509658  0.0050149\n",
      "  0.00498177  0.004993    0.0050451   0.00504721  0.00502587  0.00501863\n",
      "  0.00500542  0.0049916   0.00501962  0.00497515  0.00498464  0.00500064\n",
      "  0.00499672  0.00498473  0.00499646  0.00501776  0.00498592  0.00502006\n",
      "  0.0049988   0.00495831  0.00497959  0.00495884  0.00499855  0.00498701\n",
      "  0.00502115  0.00503367  0.0050323   0.00506622  0.00504061  0.00505679\n",
      "  0.00503733  0.00498871  0.0049432   0.00493468  0.00492874  0.00495052\n",
      "  0.00498206  0.00500487  0.00495784  0.004963    0.00501869  0.00497909\n",
      "  0.00497101  0.00501629  0.0049997   0.00502172  0.00498093  0.00498169\n",
      "  0.00496825  0.00494617  0.00493912  0.00496099  0.00494095  0.00494125\n",
      "  0.00490631  0.00490765  0.00492351  0.00489523  0.00489894  0.004933\n",
      "  0.00497009  0.00497726  0.00500161  0.00499071  0.00497904  0.00496285\n",
      "  0.00491899  0.00489832  0.0049205   0.00492893  0.00498358  0.00501054\n",
      "  0.00501986  0.00503511  0.00503532  0.00501255  0.00501923  0.00504204\n",
      "  0.00505701  0.00506432  0.00508064  0.00515787  0.00517893  0.00521078\n",
      "  0.00518096  0.0051472   0.00517751  0.0051483   0.00509329  0.00503568\n",
      "  0.00499971  0.00497675  0.00498201  0.00496804  0.00495024  0.0049811\n",
      "  0.00501629  0.00501668  0.00504429  0.00501824  0.00499043  0.00499166\n",
      "  0.00497079  0.00500549  0.00500018  0.00493151  0.00492831  0.00496826\n",
      "  0.00493183  0.00491242  0.00493783  0.00495328  0.00498917  0.00493621\n",
      "  0.00493008  0.00490845  0.00497508  0.0050136   0.00503523  0.00506224\n",
      "  0.00504582  0.00507807  0.005089    0.00501905  0.00498793  0.0049878\n",
      "  0.0049486   0.0049892   0.00496945  0.00496081  0.00488983  0.00486158\n",
      "  0.00487912  0.00489096  0.00494513  0.00489388  0.00490248  0.00488634\n",
      "  0.00485512  0.00484576]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "143\n",
      "112\n",
      "Iter 2000, val_start_accuracy = 0.0156, val loss = 0.070533\n",
      "Iter 2200, start_accuracy = 0.0078, train loss = 0.070533\n",
      "[ 0.00499963  0.00499986  0.00502018  0.00502928  0.0049966   0.0050159\n",
      "  0.00500516  0.00499872  0.00504315  0.0050467   0.00505461  0.00506057\n",
      "  0.00504502  0.00504034  0.00504505  0.00503624  0.00502068  0.00503368\n",
      "  0.00504482  0.00504244  0.00505826  0.00499595  0.00502351  0.00504737\n",
      "  0.00503832  0.00501068  0.0050138   0.00500286  0.00500582  0.00498565\n",
      "  0.00496371  0.00494537  0.00492918  0.00491114  0.0049182   0.00494364\n",
      "  0.00494204  0.00493774  0.00495701  0.00494859  0.00493859  0.00496159\n",
      "  0.00491327  0.00491012  0.00491631  0.0049177   0.00493417  0.00496675\n",
      "  0.00495036  0.00493473  0.0049547   0.00495777  0.00498294  0.00501009\n",
      "  0.00500647  0.00503391  0.00508926  0.00508502  0.00506658  0.00505347\n",
      "  0.00502606  0.00502176  0.00503288  0.00500326  0.00503647  0.00508768\n",
      "  0.005073    0.00505045  0.00504531  0.00505108  0.00503813  0.00501848\n",
      "  0.00505951  0.00504938  0.00509556  0.00510692  0.00507742  0.00508074\n",
      "  0.00505294  0.00499967  0.00498946  0.00497843  0.0049848   0.00499277\n",
      "  0.00498364  0.00499779  0.00497479  0.00499455  0.00497044  0.00499442\n",
      "  0.00501756  0.00505829  0.00502651  0.00497214  0.00497112  0.00493505\n",
      "  0.00495473  0.00493828  0.0049388   0.00497233  0.00498904  0.00501729\n",
      "  0.00503984  0.0050274   0.00500364  0.00503023  0.00508446  0.00508828\n",
      "  0.00510444  0.0050935   0.00508644  0.00507692  0.00510946  0.00507616\n",
      "  0.00503344  0.00502107  0.0050203   0.00500158  0.00499184  0.00498561\n",
      "  0.00496623  0.00497167  0.00496948  0.00493918  0.00494159  0.00498827\n",
      "  0.00496898  0.00499201  0.00498832  0.0049703   0.00497687  0.00497909\n",
      "  0.00501279  0.00501827  0.00502625  0.00499342  0.00502141  0.00505149\n",
      "  0.00506788  0.00504357  0.00501684  0.0049683   0.00496225  0.00497918\n",
      "  0.00496731  0.00494111  0.00495482  0.0049708   0.00501607  0.00493117\n",
      "  0.00489332  0.00489263  0.00492283  0.005008    0.00494106  0.00497596\n",
      "  0.00497697  0.00507248  0.00501114  0.00511168  0.00511908  0.0050577\n",
      "  0.00499508  0.00499098  0.00497155  0.00499471  0.00495008  0.00492277\n",
      "  0.00494049  0.00498961  0.00495519  0.00491139  0.00495125  0.00490724\n",
      "  0.00492538  0.00492129  0.00495929  0.00498016  0.00498474  0.0050226\n",
      "  0.0050298   0.00504283  0.00500025  0.0050257   0.00505578  0.00502429\n",
      "  0.0049152   0.00489214  0.00487394  0.00493214  0.0049225   0.0049598\n",
      "  0.00500026  0.00502396  0.00505137  0.00508824  0.00504276  0.0049922\n",
      "  0.00500968  0.00499078]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "160\n",
      "47\n",
      "Iter 2400, start_accuracy = 0.0078, train loss = 0.070533\n",
      "[ 0.00492503  0.00493028  0.00490996  0.00492493  0.00492912  0.00493308\n",
      "  0.00492928  0.0049416   0.00495068  0.00498553  0.00500999  0.00501443\n",
      "  0.00497845  0.00500015  0.00499305  0.00497315  0.00499695  0.00496918\n",
      "  0.00496152  0.00494254  0.00493952  0.0049162   0.00494651  0.00494303\n",
      "  0.00497838  0.00495592  0.0049972   0.00504023  0.00503931  0.00506967\n",
      "  0.00508817  0.00512382  0.00512383  0.00508384  0.00510622  0.0050761\n",
      "  0.0050649   0.00506119  0.00506026  0.00505831  0.00503985  0.00504634\n",
      "  0.00505804  0.005028    0.00505219  0.00502944  0.00504562  0.00503977\n",
      "  0.00502373  0.00500105  0.00504319  0.00508653  0.00512334  0.00508281\n",
      "  0.00509344  0.00511144  0.00508549  0.00508815  0.00509405  0.00501817\n",
      "  0.00500569  0.00504237  0.0050703   0.00508302  0.00502306  0.00500853\n",
      "  0.00504278  0.00507016  0.00505804  0.00503085  0.0050453   0.00502031\n",
      "  0.00506293  0.00511579  0.00510912  0.0051407   0.00513295  0.00512182\n",
      "  0.00509274  0.00506823  0.00504399  0.0050371   0.00505851  0.00503064\n",
      "  0.00502758  0.0050149   0.00502026  0.00503764  0.00499942  0.00498615\n",
      "  0.00501676  0.00504261  0.00502396  0.00498901  0.00505827  0.00506989\n",
      "  0.00509315  0.00509079  0.00506141  0.00507545  0.00504316  0.00505556\n",
      "  0.00503588  0.00502009  0.00499926  0.00501039  0.00500129  0.00502678\n",
      "  0.00506544  0.00512423  0.00508809  0.00505642  0.00505172  0.0051015\n",
      "  0.00512194  0.0050664   0.00500757  0.00501144  0.00497542  0.004945\n",
      "  0.00497108  0.0049983   0.00502492  0.00501956  0.00500685  0.00498159\n",
      "  0.00496552  0.00493802  0.00492433  0.00490601  0.004935    0.00497096\n",
      "  0.00494712  0.00497009  0.00487057  0.00485157  0.00487487  0.00481567\n",
      "  0.00475665  0.00474296  0.00480969  0.00483495  0.00486529  0.00488555\n",
      "  0.00493647  0.00494015  0.00493802  0.00491402  0.00490763  0.00493207\n",
      "  0.00489046  0.0049318   0.00494867  0.00491029  0.00487139  0.00487674\n",
      "  0.00492716  0.00499085  0.00500578  0.0049806   0.00495713  0.00491672\n",
      "  0.00498095  0.00501859  0.00500446  0.00494447  0.00495013  0.00495883\n",
      "  0.0050151   0.00498538  0.00504015  0.00502235  0.00495778  0.00493391\n",
      "  0.00491544  0.0048576   0.00490385  0.0049209   0.00497434  0.00504671\n",
      "  0.00497095  0.00495219  0.0049398   0.00493287  0.00494741  0.00496905\n",
      "  0.00494225  0.00496309  0.00495886  0.00499724  0.00497286  0.00501454\n",
      "  0.00499102  0.00491619  0.00499195  0.00501258  0.00511594  0.00511487\n",
      "  0.00511669  0.00511372]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "75\n",
      "27\n",
      "Iter 2600, start_accuracy = 0.0078, train loss = 0.070532\n",
      "[ 0.00496912  0.00495976  0.00497818  0.00498448  0.00497851  0.00494158\n",
      "  0.00497069  0.00499419  0.00502953  0.00501982  0.00500877  0.00505101\n",
      "  0.00507027  0.00505087  0.00507407  0.00511177  0.00504895  0.00504698\n",
      "  0.00508945  0.00505638  0.00508507  0.0050925   0.00510278  0.00509709\n",
      "  0.0050921   0.00504876  0.00501788  0.00500917  0.00498154  0.00496941\n",
      "  0.00500058  0.00500771  0.00496991  0.00500764  0.00499261  0.00501028\n",
      "  0.00497409  0.00499891  0.00497495  0.00498538  0.00497401  0.00493496\n",
      "  0.00498575  0.00497385  0.00499669  0.00503179  0.00503933  0.00508242\n",
      "  0.00503989  0.00503996  0.00503868  0.00503177  0.00495449  0.00497667\n",
      "  0.00499414  0.00500619  0.00499955  0.00499914  0.00503426  0.00506006\n",
      "  0.00504862  0.00504786  0.00502453  0.00501705  0.00506634  0.00507711\n",
      "  0.005088    0.00510886  0.00517837  0.00514528  0.00517048  0.00512244\n",
      "  0.00509594  0.00504991  0.00505426  0.00502666  0.0050132   0.00503735\n",
      "  0.00508551  0.00508282  0.00504038  0.00506288  0.00507313  0.00509862\n",
      "  0.00505629  0.00504059  0.00502756  0.00498285  0.00502837  0.00503383\n",
      "  0.00508331  0.00504782  0.00503295  0.00503043  0.00500818  0.00500939\n",
      "  0.00505392  0.00503824  0.00504979  0.00500106  0.00502252  0.00502948\n",
      "  0.00500726  0.00504274  0.00503412  0.00501364  0.00501404  0.00500486\n",
      "  0.00499915  0.00500637  0.00495533  0.00492098  0.00493763  0.00498731\n",
      "  0.00498823  0.004977    0.00491617  0.00491977  0.00496541  0.00494474\n",
      "  0.00494382  0.00494995  0.00494709  0.00492995  0.00502319  0.00499044\n",
      "  0.0049386   0.00496554  0.00498492  0.00495854  0.00498377  0.00498491\n",
      "  0.00499194  0.00499726  0.00499331  0.00502392  0.00498704  0.00503391\n",
      "  0.00496104  0.00503621  0.00501122  0.0050202   0.00499377  0.00499451\n",
      "  0.00496008  0.00496787  0.00499715  0.00494353  0.00498026  0.00500165\n",
      "  0.00495573  0.00494583  0.0049815   0.00498655  0.00495372  0.00493158\n",
      "  0.00494504  0.00493934  0.00498414  0.00495877  0.00501846  0.00497645\n",
      "  0.00498923  0.00498515  0.00499486  0.00499574  0.00497829  0.00498566\n",
      "  0.00497791  0.00503751  0.00495656  0.00490837  0.00489103  0.00496412\n",
      "  0.00497704  0.00491305  0.00495507  0.00492394  0.00492818  0.00494991\n",
      "  0.00501143  0.00501815  0.00502587  0.00497753  0.00494386  0.00491873\n",
      "  0.00486485  0.00483343  0.00488619  0.00482918  0.00489389  0.00486405\n",
      "  0.00487203  0.00488178  0.00488604  0.00491718  0.00494537  0.004989\n",
      "  0.00496945  0.00501442]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "68\n",
      "62\n",
      "Iter 2800, start_accuracy = 0.0000, train loss = 0.070532\n",
      "[ 0.00489078  0.00490465  0.00492593  0.00495837  0.00495715  0.00493196\n",
      "  0.00497849  0.00498867  0.0049801   0.00499803  0.00498566  0.00498886\n",
      "  0.00496779  0.00495019  0.00498049  0.00495712  0.00499283  0.00502899\n",
      "  0.00505801  0.00507768  0.00510552  0.00509113  0.00508924  0.00504375\n",
      "  0.00503861  0.00500818  0.00505691  0.00505079  0.00501658  0.00501818\n",
      "  0.00503965  0.00504681  0.00506263  0.00504717  0.0050802   0.00512674\n",
      "  0.00511314  0.00510374  0.00504533  0.00503002  0.00499545  0.00499596\n",
      "  0.00495232  0.00496123  0.00497885  0.00503008  0.00499097  0.00498412\n",
      "  0.00494777  0.00495618  0.00496092  0.00496249  0.00496703  0.00497162\n",
      "  0.00497884  0.00503532  0.00499115  0.00507384  0.00506246  0.00508319\n",
      "  0.00504531  0.00504874  0.00501163  0.00510331  0.00511523  0.00508982\n",
      "  0.00512927  0.00510587  0.00516887  0.00519851  0.00514981  0.00514936\n",
      "  0.00513106  0.00510411  0.00513654  0.00512576  0.00512619  0.00508351\n",
      "  0.00515226  0.00517507  0.00513973  0.0051023   0.0050635   0.00497121\n",
      "  0.00501228  0.00505839  0.0050382   0.00504789  0.00510572  0.00516367\n",
      "  0.00519937  0.00516298  0.00514846  0.00516511  0.00514504  0.00511034\n",
      "  0.00507651  0.00503315  0.00501901  0.00497872  0.00494635  0.00498549\n",
      "  0.00496868  0.00500395  0.00498436  0.00502258  0.00504995  0.00504311\n",
      "  0.00495217  0.00488462  0.00480658  0.0048003   0.004852    0.00488765\n",
      "  0.00488181  0.00494213  0.00484426  0.00482765  0.00483171  0.00484007\n",
      "  0.00485024  0.00495649  0.0049215   0.00497385  0.0049355   0.00501915\n",
      "  0.00501955  0.00500383  0.00501429  0.00499199  0.00497785  0.00493423\n",
      "  0.00492953  0.00489745  0.0049008   0.0049094   0.0049256   0.00498573\n",
      "  0.00492874  0.00486416  0.00492483  0.00497174  0.00492189  0.0049187\n",
      "  0.00495853  0.00491649  0.00488416  0.00498716  0.00508273  0.00504764\n",
      "  0.00501939  0.00504055  0.00502367  0.00500145  0.00497231  0.00499937\n",
      "  0.00502663  0.00496014  0.00499235  0.00495302  0.00491492  0.00493806\n",
      "  0.00495934  0.00499472  0.00501509  0.00497795  0.00491818  0.00494283\n",
      "  0.00495359  0.00495508  0.00494954  0.00496895  0.00501643  0.00503195\n",
      "  0.00498665  0.00500614  0.00495534  0.004873    0.00485733  0.00486067\n",
      "  0.00487831  0.00493264  0.00497863  0.00502107  0.00502811  0.00510032\n",
      "  0.00508003  0.00504453  0.00502033  0.0049958   0.00498158  0.00496131\n",
      "  0.00492349  0.00492204  0.00490586  0.00491284  0.00484345  0.00491997\n",
      "  0.00495023  0.00493806]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "90\n",
      "34\n",
      "Iter 3000, start_accuracy = 0.0156, train loss = 0.070533\n",
      "[ 0.00492136  0.00492491  0.00492401  0.00492714  0.00490648  0.00493474\n",
      "  0.0049413   0.00491726  0.00491996  0.00492298  0.00492594  0.00498562\n",
      "  0.0049801   0.00494907  0.00497965  0.00504439  0.00502489  0.00498463\n",
      "  0.00496033  0.00496823  0.00496719  0.00495142  0.00495853  0.00499025\n",
      "  0.00500072  0.00498399  0.00497533  0.00501436  0.00503263  0.00498218\n",
      "  0.00496946  0.00503663  0.00503801  0.00496243  0.00497584  0.00496566\n",
      "  0.00499122  0.00496364  0.00499372  0.00499807  0.00505076  0.00508166\n",
      "  0.00508936  0.00505907  0.00508568  0.00503077  0.00506651  0.00507884\n",
      "  0.00507248  0.005044    0.00507705  0.00504657  0.0050506   0.00504186\n",
      "  0.00505263  0.00508099  0.00507511  0.00502578  0.00494834  0.00491066\n",
      "  0.00492764  0.00491193  0.00490032  0.00488832  0.00490304  0.00493294\n",
      "  0.00493398  0.00495317  0.00494866  0.00493979  0.00492115  0.00496045\n",
      "  0.00491366  0.00493662  0.00497241  0.00498155  0.00502085  0.00503519\n",
      "  0.00503344  0.00506882  0.0050677   0.00503858  0.00504362  0.00503187\n",
      "  0.00503657  0.00506409  0.0050602   0.00506117  0.00508483  0.00501067\n",
      "  0.00496484  0.00497989  0.00499344  0.00503475  0.00504281  0.00504534\n",
      "  0.00502177  0.0049572   0.00500422  0.00506859  0.00504586  0.00502198\n",
      "  0.005001    0.00502847  0.00497706  0.00500879  0.0049773   0.00496061\n",
      "  0.00493989  0.00495949  0.00494307  0.00497363  0.00496229  0.00494425\n",
      "  0.00496192  0.00497522  0.00497068  0.00497176  0.00500913  0.00498932\n",
      "  0.00501405  0.00503167  0.00505982  0.00509677  0.00508525  0.00510437\n",
      "  0.00510253  0.00509828  0.00502858  0.00506751  0.00509748  0.00508606\n",
      "  0.00510478  0.00504804  0.00503348  0.00501303  0.00503766  0.0050051\n",
      "  0.00498453  0.00502475  0.00505576  0.00500642  0.00498979  0.0049635\n",
      "  0.00494718  0.00491124  0.00491157  0.00488079  0.00483612  0.00488462\n",
      "  0.00488115  0.00490244  0.00492053  0.00496972  0.00497709  0.00495026\n",
      "  0.00500322  0.00498867  0.00496006  0.00497321  0.00497538  0.00500594\n",
      "  0.00501476  0.00505131  0.00503955  0.00496664  0.0049764   0.00504636\n",
      "  0.00500915  0.00502676  0.00502452  0.00504986  0.00505411  0.00508033\n",
      "  0.00505211  0.00501536  0.00505764  0.00507909  0.00503481  0.00505688\n",
      "  0.00503206  0.00496597  0.00498003  0.00498378  0.00498528  0.00500697\n",
      "  0.00501955  0.00501619  0.00505726  0.0050228   0.00503501  0.00503854\n",
      "  0.00506146  0.00506438  0.0050132   0.005015    0.00501838  0.00502669\n",
      "  0.00497967  0.00494302]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "132\n",
      "10\n",
      "Iter 3000, val_start_accuracy = 0.0000, val loss = 0.070533\n",
      "Iter 3200, start_accuracy = 0.0000, train loss = 0.070533\n",
      "[ 0.00494187  0.00493053  0.00495118  0.00494767  0.00496211  0.00495179\n",
      "  0.00493751  0.00492633  0.0049678   0.00497395  0.00498957  0.00499477\n",
      "  0.00500209  0.00499801  0.00501484  0.00499048  0.00499255  0.00502875\n",
      "  0.00503242  0.00499977  0.00499283  0.00499365  0.00502576  0.00500713\n",
      "  0.00501066  0.00503347  0.00502537  0.00502352  0.00501393  0.00503045\n",
      "  0.00504781  0.00505697  0.00510687  0.0050594   0.00505429  0.00506124\n",
      "  0.00504288  0.00503146  0.00504834  0.00502168  0.00502107  0.00498233\n",
      "  0.00494799  0.00496855  0.004991    0.00497906  0.00498345  0.00497917\n",
      "  0.00496485  0.00494689  0.00499439  0.00498428  0.0049858   0.00500205\n",
      "  0.00501726  0.00503993  0.00500335  0.00505996  0.00511013  0.0050964\n",
      "  0.0050813   0.00505928  0.00503386  0.0050096   0.00499375  0.00498747\n",
      "  0.0049607   0.00495299  0.00490869  0.00492148  0.00497772  0.00495033\n",
      "  0.00496378  0.00505297  0.0050411   0.00501711  0.00499208  0.0050107\n",
      "  0.00505432  0.00503564  0.0050486   0.00504898  0.00504517  0.00505611\n",
      "  0.005099    0.00511245  0.0050781   0.00509492  0.00508808  0.00508987\n",
      "  0.00507634  0.0050981   0.00509142  0.00515181  0.00511136  0.00508614\n",
      "  0.00505237  0.00507007  0.00503671  0.00507453  0.00502732  0.00503479\n",
      "  0.00498981  0.00496583  0.00500097  0.00501832  0.00501386  0.00499497\n",
      "  0.00497413  0.00495639  0.00494333  0.00494413  0.00493223  0.00491057\n",
      "  0.00494279  0.00495125  0.00492865  0.00495765  0.00496895  0.00492506\n",
      "  0.00494156  0.00494462  0.00494689  0.0049904   0.0050047   0.00498922\n",
      "  0.00496847  0.00497791  0.00498335  0.00502171  0.00507347  0.00506207\n",
      "  0.00508633  0.00513015  0.00510021  0.00501343  0.00501109  0.00500981\n",
      "  0.00502285  0.00500092  0.00501212  0.00498095  0.00497846  0.0049915\n",
      "  0.00496067  0.00495785  0.0049364   0.00496719  0.00500686  0.00502825\n",
      "  0.00505457  0.00510121  0.00498808  0.00497099  0.00497698  0.00496468\n",
      "  0.00499633  0.00498147  0.00499605  0.00496581  0.00498175  0.00495333\n",
      "  0.00494652  0.0049439   0.00497041  0.00499739  0.00495356  0.00497086\n",
      "  0.00497641  0.00495386  0.00490127  0.00489811  0.00484563  0.00491762\n",
      "  0.00496205  0.00493434  0.0049543   0.00498947  0.00500545  0.00498452\n",
      "  0.00503701  0.00500993  0.005001    0.00496838  0.00494903  0.00498538\n",
      "  0.00501327  0.005041    0.00493596  0.00494094  0.00486559  0.00490844\n",
      "  0.00492218  0.00495017  0.00497035  0.00499476  0.00500644  0.00501878\n",
      "  0.00503714  0.00503224]\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "93\n",
      "3\n",
      "Iter 3400, start_accuracy = 0.0234, train loss = 0.070532\n",
      "[ 0.00493449  0.00492566  0.00493312  0.004953    0.00494713  0.00494998\n",
      "  0.00497858  0.00500452  0.00499899  0.00498912  0.00498434  0.00497794\n",
      "  0.00497826  0.00500856  0.00503273  0.00505472  0.00503992  0.00507171\n",
      "  0.00504218  0.00507624  0.00501896  0.00507567  0.0050823   0.00512267\n",
      "  0.00514151  0.00509195  0.00508205  0.00500111  0.00497942  0.00502117\n",
      "  0.00500418  0.00506115  0.00504685  0.00506661  0.00508665  0.00507967\n",
      "  0.00517023  0.00520794  0.00517887  0.00517091  0.0051709   0.00512799\n",
      "  0.00509745  0.0050528   0.00502171  0.00504425  0.00503546  0.00498557\n",
      "  0.00499854  0.00499381  0.00500023  0.00499259  0.00501314  0.00502086\n",
      "  0.00499318  0.00496873  0.0049849   0.00496745  0.00499336  0.00500589\n",
      "  0.00498356  0.00499096  0.0049882   0.00499362  0.00500261  0.00500139\n",
      "  0.00500519  0.00501445  0.00500274  0.00499977  0.00497318  0.00496522\n",
      "  0.00494848  0.00496248  0.00496945  0.00500251  0.00499941  0.00500455\n",
      "  0.00499722  0.00501615  0.0049676   0.00499317  0.00497224  0.0050512\n",
      "  0.00505332  0.00503949  0.00500993  0.00504862  0.00509097  0.00509403\n",
      "  0.00505617  0.00507509  0.00504961  0.00504494  0.00502915  0.00508343\n",
      "  0.00508472  0.00501182  0.00498549  0.00497196  0.00498365  0.00493522\n",
      "  0.00495217  0.0049842   0.00501404  0.00499707  0.00496288  0.00496883\n",
      "  0.00494841  0.00495544  0.00497224  0.0049399   0.00491889  0.00493936\n",
      "  0.00495221  0.0049716   0.00500298  0.00496461  0.00497413  0.00498325\n",
      "  0.0049908   0.00495324  0.00497842  0.00499006  0.00494913  0.00499002\n",
      "  0.00496043  0.00499282  0.00502743  0.00502482  0.00498664  0.00493984\n",
      "  0.00491358  0.00499956  0.00496009  0.00499141  0.00500274  0.0050183\n",
      "  0.00505183  0.00508083  0.0050496   0.00503123  0.00502397  0.0050424\n",
      "  0.00501446  0.00498546  0.00491156  0.00493568  0.00491349  0.00485571\n",
      "  0.00486491  0.004943    0.00492582  0.00492985  0.00495955  0.00499039\n",
      "  0.00491967  0.00496989  0.0050299   0.00501407  0.00498669  0.00504082\n",
      "  0.00509521  0.00507163  0.00504405  0.00500398  0.00503268  0.00496107\n",
      "  0.00495035  0.00496379  0.00496294  0.00495236  0.00497105  0.00499927\n",
      "  0.00503018  0.00501269  0.00501529  0.00504012  0.00501089  0.00503346\n",
      "  0.00501329  0.00492305  0.00496866  0.00497825  0.00495619  0.00487772\n",
      "  0.00491801  0.00493017  0.00490647  0.0049679   0.00497227  0.00494498\n",
      "  0.00501943  0.00503255  0.00496409  0.00490632  0.00490288  0.0048615\n",
      "  0.00488227  0.00488785]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "37\n",
      "43\n",
      "Iter 3600, start_accuracy = 0.0000, train loss = 0.070532\n",
      "[ 0.00495316  0.00493404  0.00493127  0.00495055  0.00494376  0.00494677\n",
      "  0.00496748  0.00494824  0.00493145  0.00496368  0.00496359  0.00499459\n",
      "  0.00501601  0.00501088  0.00504897  0.00507528  0.00508574  0.00506474\n",
      "  0.00505157  0.00509241  0.00507712  0.00504937  0.00506038  0.0050128\n",
      "  0.00504966  0.00503092  0.00499013  0.00501567  0.00501807  0.005012\n",
      "  0.00502634  0.00509432  0.00511269  0.00512913  0.0051439   0.00510929\n",
      "  0.00515126  0.00515074  0.00516502  0.00512411  0.00508142  0.00507421\n",
      "  0.00503797  0.00503819  0.00502082  0.00500349  0.00497102  0.00495542\n",
      "  0.00497346  0.00495969  0.00495067  0.00494415  0.00495727  0.0049779\n",
      "  0.00503259  0.00502422  0.00497815  0.00494087  0.00491654  0.00492247\n",
      "  0.00495778  0.00496325  0.004897    0.00490043  0.00496179  0.00500968\n",
      "  0.00502074  0.00498479  0.00496898  0.00497769  0.0049916   0.00496972\n",
      "  0.00500752  0.00500242  0.0049955   0.00497668  0.00499541  0.00496528\n",
      "  0.00498625  0.00498744  0.00495128  0.0049936   0.00497482  0.00495266\n",
      "  0.00499571  0.00506738  0.00506272  0.00507095  0.00513968  0.00511883\n",
      "  0.00508833  0.00502782  0.00503843  0.00505503  0.00505907  0.00504191\n",
      "  0.00503526  0.00496621  0.00498268  0.00498156  0.00501147  0.00500873\n",
      "  0.00503507  0.00506568  0.00501241  0.00501719  0.00503143  0.00500763\n",
      "  0.00500702  0.00497439  0.00498155  0.00494411  0.00493468  0.00498391\n",
      "  0.00500343  0.00495655  0.0049617   0.00495363  0.00498114  0.00498199\n",
      "  0.00499853  0.0050827   0.00502896  0.00506007  0.00504058  0.00507173\n",
      "  0.0050913   0.00506436  0.00508573  0.00508943  0.00500465  0.00502497\n",
      "  0.00503529  0.0050373   0.00502467  0.00501717  0.00500213  0.00496183\n",
      "  0.00498288  0.00499079  0.00497691  0.00495945  0.00490831  0.00490982\n",
      "  0.00491694  0.00498275  0.00500923  0.00502841  0.00500064  0.00498649\n",
      "  0.00498782  0.00499333  0.00501085  0.00501615  0.0050281   0.00498619\n",
      "  0.00500709  0.00501424  0.00496405  0.00499582  0.00495329  0.00494079\n",
      "  0.00498132  0.00496315  0.0050162   0.00504267  0.00502006  0.00498805\n",
      "  0.00499339  0.00498554  0.00492223  0.00496055  0.00498968  0.00501541\n",
      "  0.00501567  0.00501294  0.0050684   0.0050511   0.00504393  0.00497259\n",
      "  0.00497707  0.00504262  0.00508742  0.00503124  0.0049862   0.00490453\n",
      "  0.00492566  0.00492422  0.0048853   0.00485561  0.00490762  0.00490155\n",
      "  0.00488966  0.00492923  0.0049326   0.00488152  0.00487724  0.00490302\n",
      "  0.00487347  0.00483442]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "38\n",
      "33\n",
      "Iter 3800, start_accuracy = 0.0156, train loss = 0.070532\n",
      "[ 0.00498173  0.00497294  0.00495085  0.00492676  0.0049428   0.00495949\n",
      "  0.00496931  0.00497804  0.00502207  0.00504949  0.00503151  0.0050302\n",
      "  0.00504125  0.00506764  0.00506483  0.00510587  0.00509759  0.00511676\n",
      "  0.00510885  0.00514994  0.00511987  0.00508581  0.00504802  0.00505931\n",
      "  0.00509755  0.00509623  0.00509046  0.00507183  0.0050738   0.00504995\n",
      "  0.00508731  0.00507702  0.00504573  0.00498751  0.00504221  0.00503999\n",
      "  0.00505344  0.00508616  0.00506923  0.00509957  0.00510613  0.00511426\n",
      "  0.00506382  0.00508418  0.00509638  0.00507485  0.00502682  0.00501634\n",
      "  0.00498621  0.00504143  0.00502141  0.00500377  0.00501302  0.00504569\n",
      "  0.00502817  0.00503583  0.00500872  0.00503756  0.0049748   0.00497003\n",
      "  0.0049608   0.00496826  0.00496049  0.00495063  0.00494962  0.00492687\n",
      "  0.00495028  0.00497952  0.0050326   0.00502522  0.00502647  0.00499689\n",
      "  0.00499865  0.00499903  0.00500611  0.00498682  0.00491322  0.0049309\n",
      "  0.00495884  0.004965    0.00496512  0.00495258  0.00493215  0.00495276\n",
      "  0.00491141  0.00490917  0.00491731  0.00496277  0.00501072  0.0050146\n",
      "  0.00497759  0.00498589  0.00497032  0.00496657  0.00496412  0.00496648\n",
      "  0.00498728  0.00494701  0.00495373  0.00496343  0.00495438  0.00496372\n",
      "  0.00501851  0.00501309  0.00496342  0.00493423  0.00497808  0.00498909\n",
      "  0.00499823  0.00498395  0.00497228  0.00499253  0.00496314  0.00499949\n",
      "  0.00499395  0.00500488  0.00499856  0.00498966  0.00497076  0.00495284\n",
      "  0.00495884  0.00493455  0.00495466  0.00499529  0.00499669  0.00496109\n",
      "  0.0049876   0.00501034  0.00501174  0.00503659  0.00503071  0.00501707\n",
      "  0.00503159  0.005013    0.00502008  0.00501425  0.00500037  0.0050292\n",
      "  0.00499414  0.00502232  0.00500003  0.0050384   0.00499989  0.00500738\n",
      "  0.00501734  0.00502634  0.00498599  0.00497333  0.00496649  0.00495591\n",
      "  0.00495992  0.00495094  0.00498951  0.00499426  0.00494561  0.00489358\n",
      "  0.00490917  0.0049628   0.0049613   0.00490643  0.00490139  0.00491534\n",
      "  0.00494392  0.00498098  0.00493052  0.00495833  0.00497437  0.00496931\n",
      "  0.00489536  0.00485654  0.00488227  0.0049227   0.00494223  0.00496989\n",
      "  0.00497227  0.00499592  0.00498387  0.00501201  0.00503596  0.00501251\n",
      "  0.00503186  0.00501509  0.00501833  0.00500736  0.00502703  0.00502821\n",
      "  0.00492126  0.00495073  0.00499104  0.00500267  0.00496819  0.00501533\n",
      "  0.00504837  0.00507192  0.00505659  0.00508007  0.00506314  0.00507514\n",
      "  0.00504643  0.00503052]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "19\n",
      "66\n",
      "Iter 4000, start_accuracy = 0.0234, train loss = 0.070532\n",
      "[ 0.00487868  0.00489228  0.00489045  0.00490142  0.00491905  0.00492153\n",
      "  0.00496427  0.00495076  0.00494938  0.00494914  0.00496931  0.00500286\n",
      "  0.00497686  0.00497095  0.00496427  0.0049742   0.00495346  0.00496938\n",
      "  0.00495754  0.00496253  0.00497776  0.00495412  0.00493651  0.00490764\n",
      "  0.00489877  0.00492939  0.00493472  0.00495407  0.00495929  0.00496681\n",
      "  0.00498976  0.00498316  0.00497736  0.00499584  0.00496649  0.00495193\n",
      "  0.00494355  0.00493759  0.00494378  0.00496251  0.00500711  0.00503005\n",
      "  0.00501072  0.004991    0.00495521  0.00492743  0.00496629  0.00498325\n",
      "  0.00498692  0.0050076   0.00500435  0.00503894  0.00506807  0.00504211\n",
      "  0.00504396  0.00503342  0.00509011  0.00508006  0.00510209  0.00509832\n",
      "  0.00506918  0.00506172  0.00509958  0.00505475  0.0050412   0.00505865\n",
      "  0.00503721  0.00502605  0.00497162  0.00497153  0.00494206  0.00494866\n",
      "  0.00494681  0.00494427  0.00492848  0.00494051  0.00494502  0.00495559\n",
      "  0.00495497  0.00495318  0.00498739  0.00501086  0.0050194   0.0050084\n",
      "  0.00502585  0.00504762  0.00506761  0.00505449  0.00501059  0.00498831\n",
      "  0.0049975   0.00502937  0.0050349   0.00506277  0.00500103  0.00498409\n",
      "  0.00497647  0.00492991  0.00493456  0.00491534  0.00492125  0.0049504\n",
      "  0.00495614  0.00498878  0.005006    0.0049577   0.00500577  0.00506358\n",
      "  0.00508377  0.00510212  0.00509844  0.00511627  0.00509075  0.0050554\n",
      "  0.00503311  0.00503219  0.00501757  0.00498917  0.00498545  0.00497314\n",
      "  0.00494484  0.00494788  0.0049529   0.00496193  0.00496569  0.00495146\n",
      "  0.00490229  0.00492973  0.00494115  0.00493783  0.00498261  0.00501703\n",
      "  0.00500247  0.00496917  0.00498033  0.00499368  0.00502603  0.0049977\n",
      "  0.00503013  0.00502022  0.00502509  0.00499122  0.00505518  0.00503572\n",
      "  0.00503972  0.00502163  0.0050436   0.00503939  0.00507993  0.00513815\n",
      "  0.0050935   0.00509604  0.00506952  0.00501856  0.00502018  0.00505795\n",
      "  0.00505964  0.00506028  0.0050517   0.00503282  0.00500445  0.0049727\n",
      "  0.00496212  0.00496186  0.00494438  0.0050082   0.0050462   0.00506271\n",
      "  0.00509111  0.00511194  0.00508719  0.0050199   0.00505459  0.00506191\n",
      "  0.00508381  0.00507978  0.00501617  0.00500958  0.00502043  0.00503181\n",
      "  0.00505119  0.00506307  0.0050424   0.00502753  0.00502096  0.00504027\n",
      "  0.00500707  0.00502166  0.00498679  0.00494985  0.00493452  0.00491977\n",
      "  0.00496591  0.00498498  0.0049861   0.00500928  0.0050296   0.00500367\n",
      "  0.00500274  0.00501615]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "149\n",
      "47\n",
      "Iter 4000, val_start_accuracy = 0.0000, val loss = 0.070533\n",
      "Iter 4200, start_accuracy = 0.0000, train loss = 0.070532\n",
      "[ 0.00496707  0.00496023  0.00496557  0.00494754  0.00494343  0.0049331\n",
      "  0.00494782  0.00496577  0.00499878  0.00501334  0.00505135  0.00501506\n",
      "  0.00501882  0.0049971   0.00501558  0.00500544  0.0050243   0.00502605\n",
      "  0.00497893  0.0050037   0.00498059  0.0050087   0.00501928  0.00498356\n",
      "  0.00500347  0.00496901  0.00501319  0.00500761  0.00503319  0.00505737\n",
      "  0.00503831  0.00505609  0.00511266  0.00507977  0.0051024   0.00508265\n",
      "  0.00504726  0.00509086  0.00505513  0.00505284  0.00502357  0.00501034\n",
      "  0.00498982  0.00500774  0.0050088   0.00501862  0.00501651  0.00508001\n",
      "  0.0051022   0.00504221  0.00504488  0.00499336  0.00500541  0.00503065\n",
      "  0.005003    0.00504528  0.00502489  0.00500031  0.00501305  0.00504595\n",
      "  0.00502286  0.00503922  0.00503213  0.00501298  0.00502018  0.00500732\n",
      "  0.00503141  0.00499896  0.00502134  0.00500067  0.00501051  0.0050102\n",
      "  0.00500776  0.00500764  0.00497689  0.00504383  0.00505671  0.00503931\n",
      "  0.00501437  0.0049883   0.00494795  0.00497758  0.00494616  0.0049404\n",
      "  0.00498676  0.00497387  0.0049528   0.00496859  0.00496677  0.00499536\n",
      "  0.00500445  0.00501262  0.00505334  0.00505306  0.005079    0.00507929\n",
      "  0.00505896  0.0050707   0.00504511  0.00503234  0.00500983  0.00502551\n",
      "  0.00502535  0.00501654  0.00501698  0.00498707  0.00502543  0.00504425\n",
      "  0.00501888  0.00504721  0.00500478  0.00499043  0.004963    0.00499298\n",
      "  0.00505051  0.00505571  0.00507065  0.00511915  0.00510237  0.00513774\n",
      "  0.00511511  0.00511728  0.00515838  0.00515397  0.00515717  0.00512322\n",
      "  0.00510697  0.00504894  0.00506066  0.00507718  0.00506021  0.00507977\n",
      "  0.00505747  0.00505406  0.0050571   0.00507369  0.00505251  0.00501614\n",
      "  0.00501756  0.00500738  0.00500618  0.00500422  0.00502402  0.00501037\n",
      "  0.00501395  0.00499292  0.00496167  0.00493512  0.00490889  0.00490092\n",
      "  0.00494181  0.00490902  0.00491449  0.00495504  0.004881    0.00488693\n",
      "  0.00490691  0.00494469  0.0049413   0.00496796  0.00494079  0.00491835\n",
      "  0.00495049  0.00492686  0.0049188   0.00493503  0.00495283  0.00494569\n",
      "  0.00489584  0.00487801  0.00486374  0.00486572  0.00492231  0.00495342\n",
      "  0.00495039  0.00494975  0.00492388  0.00496513  0.00498216  0.00495495\n",
      "  0.00491546  0.00492584  0.00495335  0.00498698  0.004949    0.0049076\n",
      "  0.00490817  0.00490409  0.00486526  0.00491452  0.00495388  0.00498375\n",
      "  0.0050008   0.00492461  0.00492913  0.00492684  0.00490102  0.00487654\n",
      "  0.00489869  0.00490655]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "122\n",
      "62\n",
      "Iter 4400, start_accuracy = 0.0000, train loss = 0.070532\n",
      "[ 0.00492683  0.00493542  0.00494297  0.00495825  0.00497852  0.00496904\n",
      "  0.00498358  0.00499785  0.00502599  0.00501634  0.00503452  0.00506431\n",
      "  0.00504153  0.00506011  0.0050475   0.00506898  0.00505146  0.00506119\n",
      "  0.00505254  0.00503508  0.00502749  0.00500678  0.00499156  0.0050115\n",
      "  0.00501296  0.00502249  0.00499246  0.00500182  0.0050531   0.00505994\n",
      "  0.00503762  0.00506012  0.00507721  0.00508398  0.00512437  0.00512654\n",
      "  0.00516986  0.00514718  0.00514959  0.00509566  0.00508201  0.00505538\n",
      "  0.00505295  0.00499024  0.00502565  0.005019    0.00504653  0.00500879\n",
      "  0.00502072  0.00505064  0.00502001  0.0050191   0.0050432   0.00501975\n",
      "  0.0050282   0.00504585  0.00504735  0.00504324  0.00504175  0.00504211\n",
      "  0.00508236  0.00508412  0.00508297  0.00506391  0.00506057  0.00506473\n",
      "  0.00505051  0.00503066  0.0049786   0.00500143  0.00499185  0.00498013\n",
      "  0.00497339  0.00497855  0.00500233  0.00501275  0.00501736  0.00502568\n",
      "  0.00505736  0.00505065  0.00501959  0.00498787  0.00498818  0.00498056\n",
      "  0.00496833  0.004985    0.00504418  0.00500605  0.00498343  0.00496022\n",
      "  0.00499153  0.00499613  0.00498409  0.0049901   0.00500327  0.00497038\n",
      "  0.00498636  0.00499904  0.00495918  0.00495529  0.00494974  0.00491681\n",
      "  0.00495973  0.00497004  0.0050229   0.00500559  0.00500666  0.00503899\n",
      "  0.00498916  0.00496592  0.00495295  0.00497675  0.0049695   0.0050022\n",
      "  0.00499769  0.00498643  0.0050282   0.00504321  0.0050509   0.00501732\n",
      "  0.00499986  0.00502347  0.00503145  0.00503252  0.00503945  0.00499119\n",
      "  0.00497699  0.00500762  0.00500423  0.00500204  0.00499565  0.00497607\n",
      "  0.00496783  0.00496499  0.00495808  0.00497966  0.00494569  0.00491661\n",
      "  0.00496555  0.00497596  0.00494802  0.00494373  0.00493618  0.0049304\n",
      "  0.00495555  0.00498917  0.00498405  0.00497374  0.00498614  0.00498606\n",
      "  0.00499206  0.00500153  0.00502263  0.00506512  0.00507964  0.00510918\n",
      "  0.00510078  0.00505138  0.00501062  0.00504254  0.00505729  0.00511982\n",
      "  0.00507433  0.00498946  0.00495789  0.00496844  0.00493839  0.0049405\n",
      "  0.00491661  0.00491199  0.00493307  0.00495268  0.00494876  0.00494176\n",
      "  0.00492657  0.00489652  0.00484433  0.00484706  0.00493984  0.00497148\n",
      "  0.00501401  0.0050241   0.00494277  0.00494444  0.00491762  0.00494094\n",
      "  0.0049121   0.00488664  0.00490306  0.00493097  0.00494321  0.00493691\n",
      "  0.00486822  0.00490517  0.00483984  0.00491567  0.00494779  0.00494927\n",
      "  0.00493902  0.00499188]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "36\n",
      "18\n",
      "Iter 4600, start_accuracy = 0.0000, train loss = 0.070532\n",
      "[ 0.00490267  0.00489492  0.00489821  0.00491588  0.00489385  0.00490404\n",
      "  0.00492048  0.00493216  0.00492806  0.0049544   0.0049938   0.00499468\n",
      "  0.00501486  0.00509702  0.00509916  0.00512009  0.00509475  0.00510765\n",
      "  0.00507081  0.00506272  0.00503454  0.00498106  0.00492811  0.00495137\n",
      "  0.00497259  0.00495407  0.00497108  0.00497217  0.00494925  0.00496734\n",
      "  0.00492299  0.00495876  0.00495712  0.00497491  0.00499055  0.00497547\n",
      "  0.00499543  0.00498897  0.00495656  0.00491883  0.00492923  0.0049777\n",
      "  0.00498626  0.0050122   0.00503127  0.00503036  0.0050086   0.00502439\n",
      "  0.00502112  0.00498279  0.0049758   0.00496564  0.00498575  0.00499853\n",
      "  0.00500612  0.00502456  0.00499418  0.00499509  0.00502293  0.00500807\n",
      "  0.00502287  0.00501289  0.00498136  0.00495619  0.00494916  0.00495719\n",
      "  0.00499701  0.00498321  0.00497788  0.00500834  0.0050188   0.00498298\n",
      "  0.00497271  0.00497886  0.004993    0.00499216  0.00499475  0.00499749\n",
      "  0.00501818  0.00500282  0.00500364  0.00504164  0.00506407  0.00507243\n",
      "  0.00505485  0.00506443  0.00503141  0.00503037  0.0050599   0.00503297\n",
      "  0.00502172  0.00506319  0.00505719  0.00508121  0.00505404  0.00505668\n",
      "  0.00509343  0.00509421  0.00509247  0.0051028   0.00508777  0.00507692\n",
      "  0.00507543  0.00507091  0.00503418  0.00505515  0.00506283  0.0050464\n",
      "  0.00501185  0.0049808   0.0050136   0.00500703  0.00498273  0.00499208\n",
      "  0.00501461  0.00501393  0.00501538  0.00501801  0.00501315  0.00496108\n",
      "  0.00495036  0.00494007  0.00499127  0.00499527  0.0049911   0.0050391\n",
      "  0.00501353  0.00506574  0.00506614  0.00505614  0.00509068  0.00500888\n",
      "  0.00502334  0.0050224   0.00500596  0.00501045  0.00501111  0.00500953\n",
      "  0.00500061  0.00499027  0.00504819  0.00509529  0.00512357  0.00512906\n",
      "  0.0051261   0.00510566  0.00508362  0.00507053  0.00503217  0.0050092\n",
      "  0.00501239  0.00500381  0.00497039  0.00495839  0.00498794  0.00503248\n",
      "  0.00506515  0.00504165  0.00503175  0.00504746  0.00498253  0.00496789\n",
      "  0.00494076  0.00500806  0.00499345  0.00498116  0.00497771  0.00494367\n",
      "  0.00495152  0.00499229  0.0049939   0.00502573  0.00500679  0.00503096\n",
      "  0.00500922  0.00500642  0.00498418  0.00494731  0.00495851  0.00493937\n",
      "  0.00496879  0.00494336  0.00495481  0.00499707  0.00495942  0.00491876\n",
      "  0.00493064  0.00492975  0.00493657  0.00497064  0.0049172   0.00494251\n",
      "  0.00493359  0.00489921  0.00490975  0.0048797   0.00487782  0.00488577\n",
      "  0.00486302  0.00487719]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "143\n",
      "76\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-eba3e6a27eee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m                                    \u001b[0mquestion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_question\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                                    \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_answer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                                    dropout_keep_prob: keep_prob})\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrain_iter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         disp_feed_dict = {passage: train_context, \n",
      "\u001b[0;32m/home/ai2-rey/anaconda3/envs/tensorflow_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ai2-rey/anaconda3/envs/tensorflow_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ai2-rey/anaconda3/envs/tensorflow_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/ai2-rey/anaconda3/envs/tensorflow_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ai2-rey/anaconda3/envs/tensorflow_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for train_iter in range(train_iters):\n",
    "\n",
    "    train_iter +=1\n",
    "    train_batch, _ = train_data_iter.next_batch()\n",
    "    train_context, train_question, train_answer = zip(*train_batch)\n",
    "    train_answer = np.array(train_answer).transpose(1,0,2)\n",
    "    if train_iter == 1:\n",
    "        train_feed_dict = {passage: train_context,\n",
    "                           question: train_question,\n",
    "                           y: train_answer,\n",
    "                           dropout_keep_prob: 1.0}\n",
    "        training_loss, start_accuracy = tuple(sess.run([loss, start_accuracy_sq],\n",
    "                                                       feed_dict=train_feed_dict))\n",
    "\n",
    "        print(\"Iter {}, start_accuracy = {:.4f}, train loss = {:.6f}\"\n",
    "              .format(train_iter, start_accuracy, training_loss))\n",
    "\n",
    "    sess.run(optimizer, feed_dict={passage: train_context,\n",
    "                                   question: train_question, \n",
    "                                   y: train_answer, \n",
    "                                   dropout_keep_prob: keep_prob})\n",
    "    if train_iter % display_step == 0:\n",
    "        disp_feed_dict = {passage: train_context, \n",
    "                          question: train_question, \n",
    "                          y: train_answer,\n",
    "                          dropout_keep_prob: keep_prob\n",
    "                         }\n",
    "        training_loss, start_accuracy = tuple(sess.run([loss, start_accuracy_sq],\n",
    "                                                       feed_dict={passage: train_context, \n",
    "                                                                  question: train_question, \n",
    "                                                                  y: train_answer,\n",
    "                                                                  dropout_keep_prob: keep_prob\n",
    "                                                                 }))\n",
    "        print(\"Iter {}, start_accuracy = {:.4f}, train loss = {:.6f}\"\n",
    "              .format(train_iter, start_accuracy, training_loss))\n",
    "        t = sess.run(idx_distributions[0], feed_dict=disp_feed_dict)\n",
    "        l = sess.run(start_label_sq, feed_dict=disp_feed_dict)\n",
    "        print(t[0])\n",
    "        print(l[0])\n",
    "        print(np.argmax(t[0]))\n",
    "        print(np.argmax(l[0]))\n",
    "    \n",
    "    if val_data_iter !=None and train_iter%val_interval ==0:\n",
    "        val_batch, _ = val_data_iter.next_batch()\n",
    "        val_context, val_question, val_answer = zip(*val_batch)\n",
    "        val_answer = np.array(val_answer).transpose(1,0,2)\n",
    "        val_loss, val_start_accuracy = tuple(sess.run([loss, start_accuracy_sq],\n",
    "                                                      feed_dict={passage: val_context, \n",
    "                                                                 question: val_question, \n",
    "                                                                 y: val_answer, \n",
    "                                                                 dropout_keep_prob: 1.0 \n",
    "                                                                }))\n",
    "        print(\"Iter {}, val_start_accuracy = {:.4f}, val loss = {:.6f}\"\n",
    "              .format(train_iter, val_start_accuracy, val_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "128, 766, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_env]",
   "language": "python",
   "name": "conda-env-tensorflow_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
