{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import shuffle\n",
    "import os\n",
    "import gensim\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import operator\n",
    "import math\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import string\n",
    "import fileinput\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import nltk\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import split, explode\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors, DenseVector, VectorUDT\n",
    "\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.models.rnn.translate import seq2seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_PYTHON\"] = 'python2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext('local[4]')\n",
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/media/ai2-rey/data_disk/data_sets/SQuAD/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_dir, prefix):\n",
    "    filenames = [data_dir + prefix + tag for tag in ['.context', '.question', '.answer', '.span']]\n",
    "\n",
    "    data = []\n",
    "    column = []\n",
    "    line_iter = fileinput.input(filenames)\n",
    "    for line in line_iter:\n",
    "        if line_iter.isfirstline():\n",
    "            data.append(column)\n",
    "            column = []\n",
    "        column.append(line.rstrip())\n",
    "    data.append(column)\n",
    "    line_iter.close()\n",
    "    data = data[1:]\n",
    "    return list(zip(data[0], data[1], data[2], data[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = load_data(data_dir, 'train')\n",
    "train_cont, train_ques, train_ans, train_span = list(zip(*train_data))\n",
    "\n",
    "val_data = load_data(data_dir, 'val')\n",
    "val_cont, val_ques, val_ans, val_span = list(zip(*val_data))\n",
    "\n",
    "test_data = load_data(data_dir, 'dev')\n",
    "test_cont, test_ques, test_ans, test_span = list(zip(*test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(val_cont[120])\n",
    "print(val_ques[120])\n",
    "print(val_ans[120])\n",
    "print(val_span[120].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QA_Dataset:\n",
    "  \n",
    "    def __init__(self, context, question, answer, span,\n",
    "                 max_vocab_size=None, autopad_context=None, \n",
    "                 autopad_ques = None, autopad_answer=None, \n",
    "                 context_pad_len=None, ques_pad_len = None, answer_pad_len=None):\n",
    "\n",
    "        assert autopad_context in {'min', 'max', 'avg'}\n",
    "        assert autopad_answer in {'min', 'max', 'avg'}\n",
    "        assert autopad_ques in {'min', 'max', 'avg'}\n",
    "        \n",
    "        self.context = context\n",
    "        self.question = question\n",
    "        self.answer = answer\n",
    "        self.span = span\n",
    "#         span_ids = [i.split() for i in span]\n",
    "#         t_span = []\n",
    "#         for s in span_ids:\n",
    "#             s = [int(c) for c in s]\n",
    "#             t_span.append(s)\n",
    "#         self.span = t_span\n",
    "        \n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.context_pad_len = context_pad_len\n",
    "        self.ques_pad_len = ques_pad_len\n",
    "        self.answer_pad_len = answer_pad_len\n",
    "        self.__autopad_context = autopad_context\n",
    "        self.__autopad_ques = autopad_ques\n",
    "        self.__autopad_answer = autopad_answer\n",
    "        \n",
    "        self.vocab = set()\n",
    "        self.word_counter = dict()\n",
    "        self.word2id_dict = dict()\n",
    "        self.id2word_dict = dict()\n",
    "        \n",
    "        self.context_ids = None\n",
    "        self.context_sentences_ids = [] \n",
    "        self.question_ids = None\n",
    "#        self.answer_ids = None\n",
    "#         self.answer_masks = []\n",
    "        self.answer_one_hot_labels = []\n",
    "\n",
    "#         self.context_text = None\n",
    "#         self.question_text = None\n",
    "#         self.answer_text = None\n",
    "        \n",
    "#         self.num_tokens = None\n",
    "#         self.max_sent_len_context = 0\n",
    "#         self.max_num_sentences = 0\n",
    "#         self.max_sent_len_question = 0\n",
    "        \n",
    "        self.__parse_data()\n",
    "        self.__create_word2id_dict()\n",
    "        self.__numericize_data()\n",
    "\n",
    "    def __update_word_counter(self, sequence):\n",
    "        \"\"\" Update word_counter with counts for words in a sentence\n",
    "        \n",
    "        Args:\n",
    "            sequence (list<str>) : list of words in a sequence\n",
    "        \n",
    "        \"\"\"\n",
    "        for word in sequence:\n",
    "            self.word_counter[word] = self.word_counter.get(word, 0) + 1\n",
    "            \n",
    "    def __create_vocab(self):\n",
    "        \"\"\" Create set of most frequent unique words found in the training data \"\"\"\n",
    "        \n",
    "        if self.max_vocab_size == None:\n",
    "            self.vocab = set(self.word_counter.keys())\n",
    "        else:\n",
    "            self.vocab = set(sorted(self.word_counter, key=self.word_counter.get, reverse=True)[:self.max_vocab_size])\n",
    "        \n",
    "    def __shuffle_data(self, data):\n",
    "        random.shuffle(data)\n",
    "        return list(zip(*data))\n",
    "    \n",
    "    def __parse_data(self):\n",
    "        \n",
    "        for idx in range(len(self.context)):\n",
    "            self.__update_word_counter(self.context[idx].split())\n",
    "            self.__update_word_counter(self.question[idx].split())\n",
    "            self.__update_word_counter(self.answer[idx].split())\n",
    "        \n",
    "        self.__create_vocab()\n",
    "        \n",
    "        shuffle = list(zip(self.context, self.question, self.answer, self.span))        \n",
    "        self.context, self.question, self.answer, self.span = self.__shuffle_data(shuffle)\n",
    "    \n",
    "    def __create_word2id_dict(self):\n",
    "        \n",
    "        misc_tokens = ['PAD', 'UNK']\n",
    "            \n",
    "        for i, token in enumerate(misc_tokens):\n",
    "            self.word2id_dict[token] = i\n",
    "            \n",
    "        for word in self.vocab:\n",
    "            self.word2id_dict[word] = len(self.word2id_dict)\n",
    "\n",
    "        self.vocab |= set(misc_tokens)\n",
    "        self.id2word_dict = dict(zip(self.word2id_dict.values(), self.word2id_dict.keys()))\n",
    "        self.num_tokens = len(self.word2id_dict)\n",
    "    \n",
    "    def __convert_word2id(self, word):\n",
    "        try:\n",
    "            word_id = self.word2id_dict[word]\n",
    "        except:\n",
    "            word_id = self.word2id_dict['UNK']\n",
    "        return word_id\n",
    "    \n",
    "    def __apply_padding(self, s, pad_len):\n",
    "        sequence = s[:]\n",
    "        if len(sequence) < pad_len:\n",
    "            sequence += [self.word2id_dict['PAD'] for i in range(pad_len - len(sequence))]\n",
    "        elif len(sequence) > pad_len:\n",
    "            sequence = sequence[:pad_len]\n",
    "        else:\n",
    "            pass\n",
    "        return sequence\n",
    "        \n",
    "    def __get_seq_length_stats(self, sequences):\n",
    "        max_len = 0\n",
    "        min_len = 100000\n",
    "        avg_len = 0\n",
    "        for sequence in sequences:\n",
    "            max_len = max(max_len, len(sequence))\n",
    "            min_len = min(min_len, len(sequence))\n",
    "            avg_len += len(sequence)\n",
    "        avg_len = int(float(avg_len) / len(sequences))\n",
    "        return min_len, max_len, avg_len\n",
    "\n",
    "    def __get_max_sequence_lengths(self, context_ids, question_ids, answer_ids):\n",
    "        min_context_len, max_context_len, avg_context_len = self.__get_seq_length_stats(context_ids)\n",
    "        min_ques_len, max_ques_len, avg_ques_len = self.__get_seq_length_stats(question_ids)\n",
    "        min_answer_len, max_answer_len, avg_answer_len = self.__get_seq_length_stats(answer_ids)\n",
    "\n",
    "        if self.context_pad_len == None:\n",
    "            if self.__autopad_context != None:\n",
    "                if self.__autopad_context == 'min':\n",
    "                    self.context_pad_len = min_context_len\n",
    "                elif self.__autopad_context == 'max':\n",
    "                    self.context_pad_len = max_context_len\n",
    "                elif self.__autopad_context == 'avg':\n",
    "                    self.context_pad_len = avg_context_len\n",
    "            else:\n",
    "                self.context_pad_len = avg_context_len\n",
    "                \n",
    "        if self.ques_pad_len == None:\n",
    "            if self.__autopad_ques != None:\n",
    "                if self.__autopad_ques == 'min':\n",
    "                    self.ques_pad_len = min_ques_len\n",
    "                elif self.__autopad_ques == 'max':\n",
    "                    self.ques_pad_len = max_ques_len\n",
    "                elif self.__autopad_ques == 'avg':\n",
    "                    self.ques_pad_len = avg_ques_len\n",
    "            else:\n",
    "                self.ques_pad_len = avg_ques_len\n",
    "                \n",
    "        if self.answer_pad_len == None:\n",
    "            if self.__autopad_answer != None:\n",
    "                if self.__autopad_answer == 'min':\n",
    "                    self.answer_pad_len = min_answer_len\n",
    "                elif self.__autopad_answer == 'max':\n",
    "                    self.answer_pad_len = max_answer_len\n",
    "                elif self.__autopad_answer == 'avg':\n",
    "                    self.answer_pad_len = avg_answer_len\n",
    "            else:\n",
    "                self.answer_pad_len = avg_answer_len \n",
    "                \n",
    "        #print('Context length stats: min = {}, max = {}, avg = {}'.format(min_context_len, max_context_len, avg_context_len))\n",
    "        #print('Context pad length set to: {}'.format(self.context_pad_len))\n",
    "        \n",
    "        #print('Question length stats: min = {}, max = {}, avg = {}'.format(min_ques_len, max_ques_len, avg_ques_len))\n",
    "        #print('Question pad length set to: {}'.format(self.ques_pad_len))        \n",
    "        \n",
    "        #print('Answer length stats: min = {}, max = {}, avg = {}'.format(min_answer_len, max_answer_len, avg_answer_len))\n",
    "        #print('Answer pad length set to: {}'.format(self.answer_pad_len))        \n",
    "    def __convert_to_one_hot(self, span_ids, num_classes):\n",
    "        label = []\n",
    "        for idx in span_ids:\n",
    "            one_hot = [0 for i in range(num_classes)]\n",
    "            if idx >= num_classes - 1:\n",
    "                one_hot[-1] = 1\n",
    "            else:\n",
    "                one_hot[idx] = 1\n",
    "            label.append(one_hot)\n",
    "        return label\n",
    "    \n",
    "    def __convert_to_mask(self, i, a):\n",
    "        mask = (np.in1d(np.array(self.context_ids[i]), a[i]))*1\n",
    "        if len(a[i])>1:\n",
    "            count = 0\n",
    "            while count < len(mask):\n",
    "                if mask[count:count+len(a[i])].all():\n",
    "                    count+=len(a[i])\n",
    "                else:\n",
    "                    mask[count] = False\n",
    "                    count +=1\n",
    "        return mask\n",
    "        \n",
    "    def __tokenize_sentences(self, context_ids, question_ids, answer_ids):\n",
    "        \"\"\" Tokenizes sentences.\n",
    "        :param raw: dict returned from load_babi\n",
    "        :param word_table: WordTable\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        context_tokens = []\n",
    "        question_tokens = []\n",
    "        #answer_tokens = []\n",
    "        \n",
    "        for i in range(len(context_ids)):\n",
    "            c_tkn_ids = self.__apply_padding(context_ids[i], self.context_pad_len)\n",
    "            context_tokens.append(c_tkn_ids)\n",
    "            q_tkn_ids = self.__apply_padding(question_ids[i], self.ques_pad_len)\n",
    "            question_tokens.append(q_tkn_ids)\n",
    "            \n",
    "            span_ids = [int(x) for x in self.span[i].split()]\n",
    "            label = self.__convert_to_one_hot(span_ids, self.context_pad_len)\n",
    "            self.answer_one_hot_labels.append(label)\n",
    "            \n",
    "            if i%100 == 0:\n",
    "                print(i)\n",
    "            #a_tkn_ids = self.__apply_padding(answer_ids[i], self.answer_pad_len)\n",
    "            #answer_tokens.append(a_tkn_ids)\n",
    "        return context_tokens, question_tokens#, answer_tokens\n",
    "        \n",
    "    def __convert_text2ids(self, context, question, answer):\n",
    "        \"\"\" Tokenizes sentences.\n",
    "        :param raw: dict returned from load_babi\n",
    "        :param word_table: WordTable\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        context_ids = []\n",
    "        question_ids = []\n",
    "        answer_ids = []\n",
    "        for i in range(len(context)):\n",
    "            c_ids = [self.__convert_word2id(word) for word in context[i].split()]\n",
    "            context_ids.append(c_ids)\n",
    "            q_ids = [self.__convert_word2id(word) for word in question[i].split()]\n",
    "            question_ids.append(q_ids)\n",
    "            a_ids = [self.__convert_word2id(word) for word in answer[i].split()]\n",
    "            answer_ids.append(a_ids)            \n",
    "        return context_ids, question_ids, answer_ids\n",
    "          \n",
    "    def __convert_text2words(self, context, question, answer):\n",
    "        context_words = []\n",
    "        question_words = []\n",
    "        answer_words = []\n",
    "        for i in range(len(context)):\n",
    "            c_words = context[i].split()\n",
    "            context_words.append(c_words)\n",
    "            q_words = question[i].split()\n",
    "            question_words.append(q_words)\n",
    "            a_words = answer[i].split()\n",
    "            answer_words.append(a_words)\n",
    "        return context_words, question_words, answer_words    \n",
    "\n",
    "    def __get_max_sentence_length(self, stories):\n",
    "        num_sentences = 0\n",
    "        sen_len = 0\n",
    "        for story in stories: \n",
    "            sentences = sent_tokenize(story)\n",
    "            num_sentences = max(num_sentences, len(sentences))\n",
    "            word_count = lambda sentence: len(word_tokenize(sentence))\n",
    "            sen_len = max(sen_len, len((max(sentences, key=word_count)).split()))\n",
    "        return sen_len, num_sentences\n",
    "        \n",
    "    def __context2sentences(self, story):\n",
    "        story_sentences = []\n",
    "        curr_sentence = []\n",
    "        for word_id in story:\n",
    "            if self.id2word_dict[word_id] == '.':\n",
    "                if len(curr_sentence) < self.max_sent_len_context:\n",
    "                    curr_sentence += [0 for i in range(self.max_sent_len_context - len(curr_sentence))]\n",
    "                elif len(curr_sentence) > self.max_sent_len_context:\n",
    "                    curr_sentence = curr_sentence[:self.max_sent_len_context]\n",
    "                story_sentences.append(curr_sentence)\n",
    "                curr_sentence = []\n",
    "            else:\n",
    "                curr_sentence.append(word_id)\n",
    "            \n",
    "        return story_sentences\n",
    "    \n",
    "    def __numericize_data(self):\n",
    "        c, q, a = self.__convert_text2ids(self.context, self.question, self.answer)\n",
    "        \n",
    "        self.__get_max_sequence_lengths(c, q, a)\n",
    "        \n",
    "#         self.context_ids, self.question_ids, self.answer_ids = self.__tokenize_sentences(c, q, a)\n",
    "        self.context_ids, self.question_ids = self.__tokenize_sentences(c, q, a)\n",
    "            \n",
    "#         self.context_text, self.question_text, self.answer_text = self.__convert_text2words(self.context,\n",
    "#                                                                                             self.question,\n",
    "#                                                                                             self.answer)\n",
    "        #self.max_sent_len_context, self.max_num_sentences = self.__get_max_sentence_length(self.context)\n",
    "        #print(\"max_sent_len_context: {} max_num_sentences: {}\".format(self.max_sent_len_context, self.max_num_sentences))\n",
    "        #self.max_sent_len_question, ques_num_sentences = self.__get_max_sentence_length(self.question)\n",
    "        #print(\"max_sent_len_ques: {} ques_num_sentences: {}\".format(self.max_sent_len_question, ques_num_sentences))\n",
    "        #for c_ids in c:\n",
    "            #self.context_sentences_ids.append(self.__context2sentences(c_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "UNKNOWN = \"*UNKNOWN*\"\n",
    "\n",
    "def generate_embedding_files(filename):\n",
    "    embeddings = {}\n",
    "    for line in open(filename):\n",
    "        parts = line.split()\n",
    "        embeddings[parts[0]] = list(map(float, parts[1:]))\n",
    "    embedding_size = len(list(embeddings.values())[0])\n",
    "    embeddings[UNKNOWN] = [0.0 for _ in range(embedding_size)]\n",
    "    \n",
    "    words = embeddings.keys()\n",
    "    embedding_matrix = np.array([embeddings[word] for word in list(embeddings.keys())])\n",
    "    return words, embedding_matrix\n",
    "\n",
    "glove_words, glove_embedding_matrix = generate_embedding_files('/media/ai2-rey/data_disk/data_sets/glove.6B/glove.6B.50d.txt')\n",
    "glove_vocab_lookup = {word: i for i, word  in enumerate(glove_words)}\n",
    "glove_vocab_size, glove_embedding_size= glove_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataIterator:  \n",
    "    def __init__(self, data, batch_size):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.data_iterator = self.make_random_iter()\n",
    "        \n",
    "    def next_batch(self):\n",
    "        try:\n",
    "            idxs = next(self.data_iterator)\n",
    "        except StopIteration:\n",
    "            self.data_iterator = self.make_random_iter()\n",
    "            idxs = next(self.data_iterator)\n",
    "            \n",
    "        batch = [self.data[i] for i in idxs]\n",
    "        batch_idxs = [idx for idx in idxs]\n",
    "        return batch, batch_idxs\n",
    "\n",
    "    def make_random_iter(self):\n",
    "        splits = np.arange(self.batch_size, len(self.data), self.batch_size)\n",
    "        it = np.split(np.random.permutation(range(len(self.data))), splits)[:-1]\n",
    "        return iter(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n",
      "26500\n",
      "26600\n",
      "26700\n",
      "26800\n",
      "26900\n",
      "27000\n",
      "27100\n",
      "27200\n",
      "27300\n",
      "27400\n",
      "27500\n",
      "27600\n",
      "27700\n",
      "27800\n",
      "27900\n",
      "28000\n",
      "28100\n",
      "28200\n",
      "28300\n",
      "28400\n",
      "28500\n",
      "28600\n",
      "28700\n",
      "28800\n",
      "28900\n",
      "29000\n",
      "29100\n",
      "29200\n",
      "29300\n",
      "29400\n",
      "29500\n",
      "29600\n",
      "29700\n",
      "29800\n",
      "29900\n",
      "30000\n",
      "30100\n",
      "30200\n",
      "30300\n",
      "30400\n",
      "30500\n",
      "30600\n",
      "30700\n",
      "30800\n",
      "30900\n",
      "31000\n",
      "31100\n",
      "31200\n",
      "31300\n",
      "31400\n",
      "31500\n",
      "31600\n",
      "31700\n",
      "31800\n",
      "31900\n",
      "32000\n",
      "32100\n",
      "32200\n",
      "32300\n",
      "32400\n",
      "32500\n",
      "32600\n",
      "32700\n",
      "32800\n",
      "32900\n",
      "33000\n",
      "33100\n",
      "33200\n",
      "33300\n",
      "33400\n",
      "33500\n",
      "33600\n",
      "33700\n",
      "33800\n",
      "33900\n",
      "34000\n",
      "34100\n",
      "34200\n",
      "34300\n",
      "34400\n",
      "34500\n",
      "34600\n",
      "34700\n",
      "34800\n",
      "34900\n",
      "35000\n",
      "35100\n",
      "35200\n",
      "35300\n",
      "35400\n",
      "35500\n",
      "35600\n",
      "35700\n",
      "35800\n",
      "35900\n",
      "36000\n",
      "36100\n",
      "36200\n",
      "36300\n",
      "36400\n",
      "36500\n",
      "36600\n",
      "36700\n",
      "36800\n",
      "36900\n",
      "37000\n",
      "37100\n",
      "37200\n",
      "37300\n",
      "37400\n",
      "37500\n",
      "37600\n",
      "37700\n",
      "37800\n",
      "37900\n",
      "38000\n",
      "38100\n",
      "38200\n",
      "38300\n",
      "38400\n",
      "38500\n",
      "38600\n",
      "38700\n",
      "38800\n",
      "38900\n",
      "39000\n",
      "39100\n",
      "39200\n",
      "39300\n",
      "39400\n",
      "39500\n",
      "39600\n",
      "39700\n",
      "39800\n",
      "39900\n",
      "40000\n",
      "40100\n",
      "40200\n",
      "40300\n",
      "40400\n",
      "40500\n",
      "40600\n",
      "40700\n",
      "40800\n",
      "40900\n",
      "41000\n",
      "41100\n",
      "41200\n",
      "41300\n",
      "41400\n",
      "41500\n",
      "41600\n",
      "41700\n",
      "41800\n",
      "41900\n",
      "42000\n",
      "42100\n",
      "42200\n",
      "42300\n",
      "42400\n",
      "42500\n",
      "42600\n",
      "42700\n",
      "42800\n",
      "42900\n",
      "43000\n",
      "43100\n",
      "43200\n",
      "43300\n",
      "43400\n",
      "43500\n",
      "43600\n",
      "43700\n",
      "43800\n",
      "43900\n",
      "44000\n",
      "44100\n",
      "44200\n",
      "44300\n",
      "44400\n",
      "44500\n",
      "44600\n",
      "44700\n",
      "44800\n",
      "44900\n",
      "45000\n",
      "45100\n",
      "45200\n",
      "45300\n",
      "45400\n",
      "45500\n",
      "45600\n",
      "45700\n",
      "45800\n",
      "45900\n",
      "46000\n",
      "46100\n",
      "46200\n",
      "46300\n",
      "46400\n",
      "46500\n",
      "46600\n",
      "46700\n",
      "46800\n",
      "46900\n",
      "47000\n",
      "47100\n",
      "47200\n",
      "47300\n",
      "47400\n",
      "47500\n",
      "47600\n",
      "47700\n",
      "47800\n",
      "47900\n",
      "48000\n",
      "48100\n",
      "48200\n",
      "48300\n",
      "48400\n",
      "48500\n",
      "48600\n",
      "48700\n",
      "48800\n",
      "48900\n",
      "49000\n",
      "49100\n",
      "49200\n",
      "49300\n",
      "49400\n",
      "49500\n",
      "49600\n",
      "49700\n",
      "49800\n",
      "49900\n",
      "50000\n",
      "50100\n",
      "50200\n",
      "50300\n",
      "50400\n",
      "50500\n",
      "50600\n",
      "50700\n",
      "50800\n",
      "50900\n",
      "51000\n",
      "51100\n",
      "51200\n",
      "51300\n",
      "51400\n",
      "51500\n",
      "51600\n",
      "51700\n",
      "51800\n",
      "51900\n",
      "52000\n",
      "52100\n",
      "52200\n",
      "52300\n",
      "52400\n",
      "52500\n",
      "52600\n",
      "52700\n",
      "52800\n",
      "52900\n",
      "53000\n",
      "53100\n",
      "53200\n",
      "53300\n",
      "53400\n",
      "53500\n",
      "53600\n",
      "53700\n",
      "53800\n",
      "53900\n",
      "54000\n",
      "54100\n",
      "54200\n",
      "54300\n",
      "54400\n",
      "54500\n",
      "54600\n",
      "54700\n",
      "54800\n",
      "54900\n",
      "55000\n",
      "55100\n",
      "55200\n",
      "55300\n",
      "55400\n",
      "55500\n",
      "55600\n",
      "55700\n",
      "55800\n",
      "55900\n",
      "56000\n",
      "56100\n",
      "56200\n",
      "56300\n",
      "56400\n",
      "56500\n",
      "56600\n",
      "56700\n",
      "56800\n",
      "56900\n",
      "57000\n",
      "57100\n",
      "57200\n",
      "57300\n",
      "57400\n",
      "57500\n",
      "57600\n",
      "57700\n",
      "57800\n",
      "57900\n",
      "58000\n",
      "58100\n",
      "58200\n",
      "58300\n",
      "58400\n",
      "58500\n",
      "58600\n",
      "58700\n",
      "58800\n",
      "58900\n",
      "59000\n",
      "59100\n",
      "59200\n",
      "59300\n",
      "59400\n",
      "59500\n",
      "59600\n",
      "59700\n",
      "59800\n",
      "59900\n",
      "60000\n",
      "60100\n",
      "60200\n",
      "60300\n",
      "60400\n",
      "60500\n",
      "60600\n",
      "60700\n",
      "60800\n",
      "60900\n",
      "61000\n",
      "61100\n",
      "61200\n",
      "61300\n",
      "61400\n",
      "61500\n",
      "61600\n",
      "61700\n",
      "61800\n",
      "61900\n",
      "62000\n",
      "62100\n",
      "62200\n",
      "62300\n",
      "62400\n",
      "62500\n",
      "62600\n",
      "62700\n",
      "62800\n",
      "62900\n",
      "63000\n",
      "63100\n",
      "63200\n",
      "63300\n",
      "63400\n",
      "63500\n",
      "63600\n",
      "63700\n",
      "63800\n",
      "63900\n",
      "64000\n",
      "64100\n",
      "64200\n",
      "64300\n",
      "64400\n",
      "64500\n",
      "64600\n",
      "64700\n",
      "64800\n",
      "64900\n",
      "65000\n",
      "65100\n",
      "65200\n",
      "65300\n",
      "65400\n",
      "65500\n",
      "65600\n",
      "65700\n",
      "65800\n",
      "65900\n",
      "66000\n",
      "66100\n",
      "66200\n",
      "66300\n",
      "66400\n",
      "66500\n",
      "66600\n",
      "66700\n",
      "66800\n",
      "66900\n",
      "67000\n",
      "67100\n",
      "67200\n",
      "67300\n",
      "67400\n",
      "67500\n",
      "67600\n",
      "67700\n",
      "67800\n",
      "67900\n",
      "68000\n",
      "68100\n",
      "68200\n",
      "68300\n",
      "68400\n",
      "68500\n",
      "68600\n",
      "68700\n",
      "68800\n",
      "68900\n",
      "69000\n",
      "69100\n",
      "69200\n",
      "69300\n",
      "69400\n",
      "69500\n",
      "69600\n",
      "69700\n",
      "69800\n",
      "69900\n",
      "70000\n",
      "70100\n",
      "70200\n",
      "70300\n",
      "70400\n",
      "70500\n",
      "70600\n",
      "70700\n",
      "70800\n",
      "70900\n",
      "71000\n",
      "71100\n",
      "71200\n",
      "71300\n",
      "71400\n",
      "71500\n",
      "71600\n",
      "71700\n",
      "71800\n",
      "71900\n",
      "72000\n",
      "72100\n",
      "72200\n",
      "72300\n",
      "72400\n",
      "72500\n",
      "72600\n",
      "72700\n",
      "72800\n",
      "72900\n",
      "73000\n",
      "73100\n",
      "73200\n",
      "73300\n",
      "73400\n",
      "73500\n",
      "73600\n",
      "73700\n",
      "73800\n",
      "73900\n",
      "74000\n",
      "74100\n",
      "74200\n",
      "74300\n",
      "74400\n",
      "74500\n",
      "74600\n",
      "74700\n",
      "74800\n",
      "74900\n",
      "75000\n",
      "75100\n",
      "75200\n",
      "75300\n",
      "75400\n",
      "75500\n",
      "75600\n",
      "75700\n",
      "75800\n",
      "75900\n",
      "76000\n",
      "76100\n",
      "76200\n",
      "76300\n",
      "76400\n",
      "76500\n",
      "76600\n",
      "76700\n",
      "76800\n",
      "76900\n",
      "77000\n",
      "77100\n",
      "77200\n",
      "77300\n",
      "77400\n",
      "77500\n",
      "77600\n",
      "77700\n",
      "77800\n",
      "77900\n",
      "78000\n",
      "78100\n",
      "78200\n",
      "78300\n",
      "78400\n",
      "78500\n",
      "78600\n",
      "78700\n",
      "78800\n",
      "78900\n",
      "79000\n",
      "79100\n",
      "79200\n",
      "79300\n",
      "79400\n",
      "79500\n",
      "79600\n",
      "79700\n",
      "79800\n",
      "79900\n",
      "80000\n",
      "80100\n",
      "80200\n",
      "80300\n",
      "80400\n",
      "80500\n",
      "80600\n",
      "80700\n",
      "80800\n",
      "80900\n",
      "81000\n",
      "81100\n",
      "81200\n",
      "81300\n"
     ]
    }
   ],
   "source": [
    "train = QA_Dataset(train_cont, train_ques, train_ans, train_span,\n",
    "                   context_pad_len = 200, ques_pad_len = 20,\n",
    "                   autopad_context='max', autopad_ques = 'max', autopad_answer='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n"
     ]
    }
   ],
   "source": [
    "val = QA_Dataset(val_cont, val_ques, val_ans, val_span,\n",
    "                 context_pad_len=train.context_pad_len, \n",
    "                 ques_pad_len = train.ques_pad_len, \n",
    "                 answer_pad_len=train.answer_pad_len,\n",
    "                 autopad_context='max', autopad_ques='max', autopad_answer='max')\n",
    "\n",
    "test = QA_Dataset(test_cont, test_ques, test_ans, test_span,\n",
    "                  context_pad_len=train.context_pad_len, \n",
    "                  ques_pad_len = train.ques_pad_len,\n",
    "                  answer_pad_len=train.answer_pad_len,\n",
    "                  autopad_context='max', autopad_ques='max', autopad_answer='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_cont, train_ques, train_ans, train_span\n",
    "del val_cont, val_ques, val_ans, val_span\n",
    "del test_cont, test_ques, test_ans, test_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ids = list(zip(train.context_ids, train.question_ids, train.answer_one_hot_labels))\n",
    "val_ids = list(zip(val.context_ids,val.question_ids,val.answer_one_hot_labels))\n",
    "test_ids = list(zip(test.context_ids,test.question_ids, test.answer_one_hot_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_iter = DataIterator(train_ids,128)\n",
    "val_data_iter = DataIterator(val_ids,999)\n",
    "test_data_iter = DataIterator(test_ids,999)\n",
    "deploy_data_iter = DataIterator(train_ids,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.ques_pad_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.context_pad_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "passage_max_len = train.context_pad_len\n",
    "question_max_len = train.ques_pad_len\n",
    "#output of lstm is n_steps or passage length by hidden size \n",
    "n_hidden = 50 \n",
    "num_indices = 2\n",
    "batch_size = 128\n",
    "n_hidden_layers = 2\n",
    "learning_rate = 0.001\n",
    "blend_units = 6\n",
    "train_iters = 100000\n",
    "keep_prob = 0.5\n",
    "display_step = 200\n",
    "val_interval = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "100\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "passage = tf.placeholder(tf.int32, [None, passage_max_len], name='passage')\n",
    "question = tf.placeholder(tf.int32,[None, question_max_len], name='question')\n",
    "#desired_output = tf.placeholder(tf.int32, [num_indices, batch_size, passage_max_len], name='desired_output')\n",
    "y = tf.placeholder(tf.float32,[None,num_indices,passage_max_len])\n",
    "\n",
    "dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "batch_size = tf.placeholder(tf.int32)\n",
    "\n",
    "embeddings = tf.constant(glove_embedding_matrix, name='embeddings', dtype=tf.float32)\n",
    "\n",
    "#preprocessing layer \n",
    "passage_embedded = tf.nn.embedding_lookup(embeddings, passage)\n",
    "question_embedded = tf.nn.embedding_lookup(embeddings, question)\n",
    "\n",
    "with tf.variable_scope('passage_lstm'):\n",
    "    passage_cell = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
    "    passage_cell = tf.nn.rnn_cell.DropoutWrapper(passage_cell, output_keep_prob=dropout_keep_prob)\n",
    "    passage_cell = tf.nn.rnn_cell.MultiRNNCell([passage_cell]*n_hidden_layers)\n",
    "    #H_p shape = batch_size, max_passage_len, n_hidden\n",
    "    H_p, _ = tf.nn.dynamic_rnn(passage_cell, passage_embedded, dtype=tf.float32) \n",
    "    \n",
    "with tf.variable_scope('question_lstm'):\n",
    "    question_cell = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
    "    question_cell = tf.nn.rnn_cell.DropoutWrapper(question_cell, output_keep_prob=dropout_keep_prob)\n",
    "    question_cell = tf.nn.rnn_cell.MultiRNNCell([question_cell]*n_hidden_layers)\n",
    "    H_q, _ = tf.nn.dynamic_rnn(question_cell, question_embedded, dtype=tf.float32)\n",
    "\n",
    "#Match LSTM layer \n",
    "#Weights and bias to compute G\n",
    "W_q = tf.Variable(tf.truncated_normal(shape=[n_hidden, n_hidden], stddev=0.1))\n",
    "W_p = tf.Variable(tf.truncated_normal(shape=[n_hidden, n_hidden], stddev=0.1))\n",
    "W_r = tf.Variable(tf.truncated_normal(shape=[n_hidden, n_hidden], stddev=0.1))\n",
    "b_p = tf.Variable(tf.truncated_normal(shape=[n_hidden], stddev=0.1))\n",
    "\n",
    "#Weight and bias to compute a\n",
    "w= tf.Variable(tf.truncated_normal(shape=[1, n_hidden], stddev=0.1))\n",
    "#b_alpha shape = 1,none\n",
    "b_alpha = tf.Variable(tf.constant(0.1, shape = [1]))\n",
    "\n",
    "#only calculate WH_q once \n",
    "#H_q shape = (batch_size&max_passage_len merge, n_hidden)\n",
    "H_q = tf.reshape(H_q, [-1, H_q.get_shape().as_list()[-1]])\n",
    "#H_q shape = (n_hidden, batch_size&max_passage_len merge)\n",
    "H_q = tf.transpose(H_q, [1,0])\n",
    "WH_q = tf.batch_matmul(W_q, H_q)\n",
    "WH_q = tf.reshape(WH_q, [n_hidden, question_max_len, -1])\n",
    "#WH_q shape =(batch_size, n_hidden, max_ques_len)\n",
    "WH_q = tf.transpose(WH_q, [2,0,1])\n",
    "\n",
    "H_r_forward = []\n",
    "H_r_backward = []\n",
    "\n",
    "with tf.variable_scope('forward_match_lstm'):\n",
    "    forward_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(n_hidden), output_keep_prob=dropout_keep_prob)\n",
    "    forward_state = forward_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    h = forward_state.h #h shape =(batch_size, n_hidden)\n",
    "\n",
    "    for i in range(passage_max_len):\n",
    "        H_p_i = tf.slice(H_p, [0,i,0], [-1,1,-1])\n",
    "        H_p_i = tf.transpose(tf.squeeze(H_p_i,1),[1,0])\n",
    "        WH_p_i = tf.transpose(tf.matmul(W_p, H_p_i), [1,0])\n",
    "        h = tf.transpose(h,[1,0])\n",
    "        W_rh = tf.transpose(tf.matmul(W_r, h),[1,0])\n",
    "        G_tile = tf.tile((WH_p_i + W_rh + b_p), [question_max_len, 1])\n",
    "        G_tile = tf.reshape(G_tile, [-1, n_hidden, question_max_len])\n",
    "        G_forward = tf.tanh(WH_q + G_tile)\n",
    "        G_forward = tf.reshape(G_forward, [n_hidden, -1])\n",
    "        wG_forward = tf.matmul(w,G_forward)\n",
    "        a_tile = tf.tile(b_alpha, [question_max_len])\n",
    "        wG_forward = tf.reshape(tf.squeeze(wG_forward,0),[-1, question_max_len])\n",
    "        alpha_forward = tf.nn.softmax(wG_forward + a_tile)\n",
    "\n",
    "        H_q = tf.reshape(H_q,[-1, n_hidden, question_max_len])\n",
    "        alpha_forward = tf.expand_dims(alpha_forward, 2)\n",
    "        H_q_alpha_forward = tf.batch_matmul(H_q, alpha_forward)\n",
    "        H_q_alpha_forward = tf.reshape(H_q_alpha_forward,[-1, n_hidden])\n",
    "        H_p_i = tf.transpose(H_p_i,[1,0])\n",
    "        z_forward = tf.concat(1, [H_p_i, H_q_alpha_forward])\n",
    "\n",
    "        h,forward_state = forward_cell(z_forward, forward_state)\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        H_r_forward.append(h)\n",
    "        if i%100 == 0:\n",
    "            print(i)\n",
    "\n",
    "    H_r_forward = tf.transpose(tf.stack(H_r_forward),[1,0,2])\n",
    "    \n",
    "with tf.variable_scope('backward_match_lstm'):\n",
    "    backward_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(n_hidden), output_keep_prob=dropout_keep_prob)\n",
    "    backward_state = backward_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    h = backward_state.h #h shape =(bathc_size, n_hidden)\n",
    "\n",
    "    for i in reversed(range(passage_max_len)):\n",
    "        H_p_i = tf.slice(H_p, [0,i,0], [-1,1,-1])\n",
    "        H_p_i = tf.transpose(tf.squeeze(H_p_i,1),[1,0])\n",
    "        WH_p_i = tf.transpose(tf.matmul(W_p, H_p_i), [1,0])\n",
    "        h = tf.transpose(h,[1,0])\n",
    "        W_rh = tf.transpose(tf.matmul(W_r, h),[1,0])\n",
    "        G_tile = tf.tile((WH_p_i + W_rh + b_p), [question_max_len, 1])\n",
    "        G_tile = tf.reshape(G_tile, [-1, n_hidden, question_max_len])\n",
    "        G_backward = tf.tanh(WH_q + G_tile)\n",
    "        G_backward = tf.reshape(G_backward, [n_hidden, -1])\n",
    "        wG_backward = tf.matmul(w,G_backward)\n",
    "        a_tile = tf.tile(b_alpha, [question_max_len])\n",
    "        wG_backward = tf.reshape(tf.squeeze(wG_backward,0),[-1, question_max_len])\n",
    "        alpha_backward = tf.nn.softmax(wG_backward + a_tile)\n",
    "\n",
    "#         H_q = tf.reshape(H_q, [self.n_hidden, self.n_steps_question, -1])\n",
    "#         H_q = tf.reshape(H_q, [-1, self.n_steps_question])\n",
    "#         alpha_backward = tf.transpose(alpha_backward, [1, 0])\n",
    "#         H_qalpha_backward = tf.matmul(H_q, alpha_backward)\n",
    "#         H_qalpha_backward = tf.reshape(H_qalpha_backward, [self.n_hidden, -1])\n",
    "        \n",
    "        H_q = tf.reshape(H_q,[-1, n_hidden, question_max_len])\n",
    "        alpha_backward = tf.expand_dims(alpha_backward, 2)\n",
    "        H_q_alpha_backward = tf.batch_matmul(H_q, alpha_backward)\n",
    "        H_q_alpha_backward = tf.reshape(H_q_alpha_backward,[-1, n_hidden])\n",
    "        H_p_i = tf.transpose(H_p_i,[1,0])\n",
    "        z_backward = tf.concat(1, [H_p_i, H_q_alpha_backward])\n",
    "\n",
    "        h,backward_state = backward_cell(z_backward, backward_state)\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        H_r_backward.append(h)\n",
    "        if i%100==0:\n",
    "            print(i)\n",
    "    H_r_backward = list(reversed(H_r_backward))\n",
    "    H_r_backward = tf.transpose(tf.stack(H_r_backward),[1,0,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concat:0\", shape=(?, 200, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "H_r = tf.concat(2, [H_r_forward, H_r_backward])\n",
    "print(H_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice:0' shape=(200, 100) dtype=float32>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_r[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Variable_6/read:0\", shape=(50, 100), dtype=float32)\n",
      "Tensor(\"Reshape_2:0\", shape=(100, ?), dtype=float32)\n",
      "0\n",
      "1\n",
      "(<tf.Tensor 'Softmax:0' shape=(?, 200) dtype=float32>, <tf.Tensor 'Softmax_1:0' shape=(?, 200) dtype=float32>)\n",
      "Tensor(\"Squeeze:0\", shape=(?, 200), dtype=float32)\n",
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "Tensor(\"global_norm/global_norm:0\", shape=(), dtype=float32)\n",
      "name: \"Adam\"\n",
      "op: \"NoOp\"\n",
      "input: \"^Adam/update_passage_lstm/RNN/MultiRNNCell/Cell0/LSTMCell/W_0/ApplyAdam\"\n",
      "input: \"^Adam/update_passage_lstm/RNN/MultiRNNCell/Cell0/LSTMCell/B/ApplyAdam\"\n",
      "input: \"^Adam/update_passage_lstm/RNN/MultiRNNCell/Cell1/LSTMCell/W_0/ApplyAdam\"\n",
      "input: \"^Adam/update_passage_lstm/RNN/MultiRNNCell/Cell1/LSTMCell/B/ApplyAdam\"\n",
      "input: \"^Adam/update_question_lstm/RNN/MultiRNNCell/Cell0/LSTMCell/W_0/ApplyAdam\"\n",
      "input: \"^Adam/update_question_lstm/RNN/MultiRNNCell/Cell0/LSTMCell/B/ApplyAdam\"\n",
      "input: \"^Adam/update_question_lstm/RNN/MultiRNNCell/Cell1/LSTMCell/W_0/ApplyAdam\"\n",
      "input: \"^Adam/update_question_lstm/RNN/MultiRNNCell/Cell1/LSTMCell/B/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_1/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_2/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_3/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_4/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_5/ApplyAdam\"\n",
      "input: \"^Adam/update_forward_match_lstm/LSTMCell/W_0/ApplyAdam\"\n",
      "input: \"^Adam/update_forward_match_lstm/LSTMCell/B/ApplyAdam\"\n",
      "input: \"^Adam/update_backward_match_lstm/LSTMCell/W_0/ApplyAdam\"\n",
      "input: \"^Adam/update_backward_match_lstm/LSTMCell/B/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_6/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_7/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_8/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_9/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_10/ApplyAdam\"\n",
      "input: \"^Adam/update_answer_pointer_lstm/LSTMCell/W_0/ApplyAdam\"\n",
      "input: \"^Adam/update_answer_pointer_lstm/LSTMCell/B/ApplyAdam\"\n",
      "input: \"^Adam/Assign\"\n",
      "input: \"^Adam/Assign_1\"\n",
      "\n",
      "Tensor(\"Equal:0\", shape=(2, ?), dtype=bool)\n",
      "Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Answer Pointer Layer \n",
    "\n",
    "#Weights and bias to compute 'F'\n",
    "V = tf.Variable(tf.truncated_normal(shape=[n_hidden, 2*n_hidden], stddev=0.1))\n",
    "print(V)\n",
    "W_a = tf.Variable(tf.truncated_normal(shape=[n_hidden, n_hidden], stddev=0.1))\n",
    "b_a = tf.Variable(tf.constant(0.1, shape=[n_hidden]))\n",
    "\n",
    "#weight and bias to computer 'beta'\n",
    "v = tf.Variable(tf.truncated_normal(shape=[1,n_hidden], stddev=0.1))\n",
    "b_beta = tf.Variable(tf.constant(0.1, shape=[1]))\n",
    "\n",
    "#Only calculate 'VH' once \n",
    "H_r = tf.reshape(H_r, [H_r.get_shape().as_list()[2],-1]) #shape = (2*n_hidden, batch&passage_max_len)\n",
    "print(H_r)\n",
    "VH = tf.matmul(V,H_r) #shape (hidden_size, passage_max_len)\n",
    "VH = tf.reshape(VH,[-1, passage_max_len, n_hidden])\n",
    "\n",
    "output_logits = []\n",
    "output_probs = []\n",
    "\n",
    "with tf.variable_scope('answer_pointer_lstm'):\n",
    "    pointer_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(n_hidden), output_keep_prob=dropout_keep_prob)\n",
    "    pointer_state = pointer_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    h = pointer_state.h\n",
    "    \n",
    "    for k in range(2):\n",
    "        h = tf.transpose(h,[1,0])\n",
    "        W_ah = tf.transpose(tf.matmul(W_a, h),[1,0])\n",
    "        F_tile = tf.tile(tf.expand_dims(tf.add(W_ah,b_a),1),[1,passage_max_len,1])\n",
    "        F_tile = tf.reshape(F_tile,[-1, passage_max_len, n_hidden])\n",
    "        F = tf.tanh(tf.add(VH, F_tile))\n",
    "        F = tf.reshape(F, [n_hidden, -1])   \n",
    "        vF = tf.matmul(v, F)\n",
    "        beta_tile = tf.tile(b_beta, [passage_max_len])\n",
    "        vF = tf.reshape(tf.squeeze(vF, 0), [-1,passage_max_len])\n",
    "        beta_logits = tf.add(vF,beta_tile)\n",
    "        beta = tf.nn.softmax(beta_logits)\n",
    "        output_logits.append(beta_logits)\n",
    "        output_probs.append(beta)\n",
    "\n",
    "\n",
    "    #    beta = tf.transpose(beta,[1,0])\n",
    "        beta = tf.expand_dims(beta,2) #unsure and changed \n",
    "        H_r = tf.reshape(H_r, [-1, n_hidden*2, passage_max_len])\n",
    "        H_r_beta = tf.batch_matmul(H_r, beta)\n",
    "        H_r_beta = tf.squeeze(H_r_beta,2)\n",
    "        h,pointer_state = pointer_cell(H_r_beta, pointer_state)\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        print(k)\n",
    "#                         H_rbeta = tf.matmul(H_r, beta)\n",
    "#                         H_rbeta = tf.reshape(H_rbeta, [-1, 2*self.n_hidden])\n",
    "#                         h, pointer_state = pointer_cell(H_rbeta, pointer_state)\n",
    "#                         tf.get_variable_scope().reuse_variables()\n",
    "#                         H_a.append(h)\n",
    "\n",
    "probs_start = tf.nn.softmax(output_logits[0])\n",
    "probs_end = tf.nn.softmax(output_logits[1])\n",
    "probs = (probs_start,probs_end)\n",
    "print(probs)\n",
    "\n",
    "start_label, end_label = tf.split(1,2,y)\n",
    "start_label = tf.squeeze(start_label,1)\n",
    "end_label = tf.squeeze(end_label,1)\n",
    "print(start_label)\n",
    "\n",
    "loss_start = tf.nn.softmax_cross_entropy_with_logits(output_logits[0], start_label)\n",
    "loss_end = tf.nn.softmax_cross_entropy_with_logits(output_logits[1], end_label)\n",
    "loss = tf.reduce_mean(tf.add(loss_start, loss_end))\n",
    "# loss = tf.reduce_mean(tf.reduce_sum(tf.pow(desired_output - output, 2), reduction_indices=[1]))\n",
    "print(loss)\n",
    "\n",
    "params = tf.trainable_variables()\n",
    "gradients = tf.gradients(loss, params)\n",
    "gradient_norm = tf.global_norm(gradients)\n",
    "print(gradient_norm)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "print(optimizer)\n",
    "\n",
    "correct_prediction = tf.equal([tf.argmax(probs_start,1), tf.argmax(probs_end,1)],\n",
    "                             [tf.argmax(start_label,1), tf.argmax(end_label,1)])\n",
    "print(correct_prediction)\n",
    "\n",
    "mean_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(mean_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a,b = val_data_iter.next_batch()\n",
    "c,d,e,f = zip(*a)\n",
    "t = list(zip(e,f))\n",
    "np.array(t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1, grad_norm = 0.1863, train loss = 10.599065, acc = 0.0078\n"
     ]
    }
   ],
   "source": [
    "for train_iter in range(train_iters):\n",
    "\n",
    "    train_iter +=1\n",
    "    train_batch, _ = train_data_iter.next_batch()\n",
    "    train_context, train_question, train_answer = zip(*train_batch)\n",
    "\n",
    "    if train_iter == 1:\n",
    "        train_feed_dict = {passage: train_context,\n",
    "                           question: train_question,\n",
    "                           y: train_answer,\n",
    "                           dropout_keep_prob: 1.0,\n",
    "                           batch_size: train_data_iter.batch_size}\n",
    "        training_loss, training_accuracy, grad_norm = tuple(sess.run([loss,\n",
    "                                                                      mean_accuracy,\n",
    "                                                                      gradient_norm],\n",
    "                                                                     feed_dict=train_feed_dict))\n",
    "\n",
    "        print(\"Iter {}, grad_norm = {:.4f}, train loss = {:.6f}, acc = {:.4f}\"\n",
    "              .format(train_iter, grad_norm, training_loss, training_accuracy))\n",
    "\n",
    "    sess.run(optimizer, feed_dict={passage: train_context,\n",
    "                                   question: train_question, \n",
    "                                   y: train_answer, \n",
    "                                   dropout_keep_prob: keep_prob, \n",
    "                                   batch_size: train_data_iter.batch_size})\n",
    "    if train_iter % display_step == 0:\n",
    "        training_loss, training_accuracy, grad_norm = tuple(sess.run([loss, \n",
    "                                                                      mean_accuracy, \n",
    "                                                                      gradient_norm],\n",
    "                                                                    feed_dict={passage: train_context, \n",
    "                                                                               question: train_question, \n",
    "                                                                               y: train_answer,\n",
    "                                                                               dropout_keep_prob: keep_prob, \n",
    "                                                                               batch_size: train_data_iter.batch_size}))\n",
    "        print(\"Iter {}, grad_norm = {:.4f}, train loss = {:.6f}, acc = {:.4f}\"\n",
    "              .format(train_iter, grad_norm, training_loss, training_accuracy))\n",
    "    \n",
    "    if val_data_iter !=None and train_iter%val_interval ==0:\n",
    "        val_batch, _ = val_data_iter.next_batch()\n",
    "        val_context, val_question, val_answer = zip(*val_batch)\n",
    "        \n",
    "        val_loss, val_accuracy = tuple(sess.run([loss, \n",
    "                                                 mean_accuracy], \n",
    "                                                feed_dict={passage: val_context, \n",
    "                                                           question: val_question, \n",
    "                                                           y: val_answer, \n",
    "                                                           dropout_keep_prob: 1.0, \n",
    "                                                           batch_size: val_data_iter.batch_size}))\n",
    "        print(\"Iter {}, val loss = {:.6f}, val acc = {:.4f}\"\n",
    "              .format(train_iter, val_loss, val_accuracy))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a =(128, 766, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_env]",
   "language": "python",
   "name": "conda-env-tensorflow_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
