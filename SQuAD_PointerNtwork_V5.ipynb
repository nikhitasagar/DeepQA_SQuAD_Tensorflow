{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import shuffle\n",
    "import os\n",
    "import gensim\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import operator\n",
    "import math\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import string\n",
    "import fileinput\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import nltk\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import split, explode\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors, DenseVector, VectorUDT\n",
    "\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.models.rnn.translate import seq2seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_PYTHON\"] = 'python2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext('local[4]')\n",
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/media/ai2-rey/data_disk/data_sets/SQuAD/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_dir, prefix):\n",
    "    filenames = [data_dir + prefix + tag for tag in ['.context', '.question', '.answer', '.span']]\n",
    "\n",
    "    data = []\n",
    "    column = []\n",
    "    line_iter = fileinput.input(filenames)\n",
    "    for line in line_iter:\n",
    "        if line_iter.isfirstline():\n",
    "            data.append(column)\n",
    "            column = []\n",
    "        column.append(line.rstrip())\n",
    "    data.append(column)\n",
    "    line_iter.close()\n",
    "    data = data[1:]\n",
    "    return list(zip(data[0], data[1], data[2], data[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = load_data(data_dir, 'train')\n",
    "train_cont, train_ques, train_ans, train_span = list(zip(*train_data))\n",
    "\n",
    "val_data = load_data(data_dir, 'val')\n",
    "val_cont, val_ques, val_ans, val_span = list(zip(*val_data))\n",
    "\n",
    "test_data = load_data(data_dir, 'dev')\n",
    "test_cont, test_ques, test_ans, test_span = list(zip(*test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(val_cont[120])\n",
    "print(val_ques[120])\n",
    "print(val_ans[120])\n",
    "print(val_span[120].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QA_Dataset:\n",
    "  \n",
    "    def __init__(self, context, question, answer, span,\n",
    "                 max_vocab_size=None, autopad_context=None, \n",
    "                 autopad_ques = None, autopad_answer=None, \n",
    "                 context_pad_len=None, ques_pad_len = None, answer_pad_len=None):\n",
    "\n",
    "        assert autopad_context in {'min', 'max', 'avg'}\n",
    "        assert autopad_answer in {'min', 'max', 'avg'}\n",
    "        assert autopad_ques in {'min', 'max', 'avg'}\n",
    "        \n",
    "        self.context = context\n",
    "        self.question = question\n",
    "        self.answer = answer\n",
    "        self.span = span\n",
    "#         span_ids = [i.split() for i in span]\n",
    "#         t_span = []\n",
    "#         for s in span_ids:\n",
    "#             s = [int(c) for c in s]\n",
    "#             t_span.append(s)\n",
    "#         self.span = t_span\n",
    "        \n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.context_pad_len = context_pad_len\n",
    "        self.ques_pad_len = ques_pad_len\n",
    "        self.answer_pad_len = answer_pad_len\n",
    "        self.__autopad_context = autopad_context\n",
    "        self.__autopad_ques = autopad_ques\n",
    "        self.__autopad_answer = autopad_answer\n",
    "        \n",
    "        self.vocab = set()\n",
    "        self.word_counter = dict()\n",
    "        self.word2id_dict = dict()\n",
    "        self.id2word_dict = dict()\n",
    "        \n",
    "        self.context_ids = None\n",
    "        self.context_sentences_ids = [] \n",
    "        self.question_ids = None\n",
    "#        self.answer_ids = None\n",
    "#         self.answer_masks = []\n",
    "        self.answer_one_hot_labels = []\n",
    "\n",
    "#         self.context_text = None\n",
    "#         self.question_text = None\n",
    "#         self.answer_text = None\n",
    "        \n",
    "#         self.num_tokens = None\n",
    "#         self.max_sent_len_context = 0\n",
    "#         self.max_num_sentences = 0\n",
    "#         self.max_sent_len_question = 0\n",
    "        \n",
    "        self.__lower_words()\n",
    "        self.__parse_data()\n",
    "        self.__create_word2id_dict()\n",
    "        self.__numericize_data()\n",
    "\n",
    "    def __lower_words(self):\n",
    "        self.context = [t.lower() for t in self.context]\n",
    "        self.question = [t.lower() for t in self.question]\n",
    "        self.answer = [t.lower() for t in self.answer]\n",
    "    \n",
    "    def __update_word_counter(self, sequence):\n",
    "        \"\"\" Update word_counter with counts for words in a sentence\n",
    "        \n",
    "        Args:\n",
    "            sequence (list<str>) : list of words in a sequence\n",
    "        \n",
    "        \"\"\"\n",
    "        for word in sequence:\n",
    "            self.word_counter[word] = self.word_counter.get(word, 0) + 1\n",
    "            \n",
    "    def __create_vocab(self):\n",
    "        \"\"\" Create set of most frequent unique words found in the training data \"\"\"\n",
    "        \n",
    "        if self.max_vocab_size == None:\n",
    "            self.vocab = set(self.word_counter.keys())\n",
    "        else:\n",
    "            self.vocab = set(sorted(self.word_counter, key=self.word_counter.get, reverse=True)[:self.max_vocab_size])\n",
    "        \n",
    "    def __shuffle_data(self, data):\n",
    "        random.shuffle(data)\n",
    "        return list(zip(*data))\n",
    "    \n",
    "    def __parse_data(self):\n",
    "        \n",
    "        for idx in range(len(self.context)):\n",
    "            self.__update_word_counter(self.context[idx].split())\n",
    "            self.__update_word_counter(self.question[idx].split())\n",
    "            self.__update_word_counter(self.answer[idx].split())\n",
    "        \n",
    "        self.__create_vocab()\n",
    "        \n",
    "        shuffle = list(zip(self.context, self.question, self.answer, self.span))        \n",
    "        self.context, self.question, self.answer, self.span = self.__shuffle_data(shuffle)\n",
    "    \n",
    "    def __create_word2id_dict(self):\n",
    "        \n",
    "        misc_tokens = ['PAD', 'UNK']\n",
    "            \n",
    "        for i, token in enumerate(misc_tokens):\n",
    "            self.word2id_dict[token] = i\n",
    "            \n",
    "        for word in self.vocab:\n",
    "            self.word2id_dict[word] = len(self.word2id_dict)\n",
    "\n",
    "        self.vocab |= set(misc_tokens)\n",
    "        self.id2word_dict = dict(zip(self.word2id_dict.values(), self.word2id_dict.keys()))\n",
    "        self.num_tokens = len(self.word2id_dict)\n",
    "    \n",
    "    def __convert_word2id(self, word):\n",
    "        try:\n",
    "            word_id = self.word2id_dict[word]\n",
    "        except:\n",
    "            word_id = self.word2id_dict['UNK']\n",
    "        return word_id\n",
    "    \n",
    "    def __apply_padding(self, s, pad_len):\n",
    "        sequence = s[:]\n",
    "        if len(sequence) < pad_len:\n",
    "            sequence += [self.word2id_dict['PAD'] for i in range(pad_len - len(sequence))]\n",
    "        elif len(sequence) > pad_len:\n",
    "            sequence = sequence[:pad_len]\n",
    "        else:\n",
    "            pass\n",
    "        return sequence\n",
    "        \n",
    "    def __get_seq_length_stats(self, sequences):\n",
    "        max_len = 0\n",
    "        min_len = 100000\n",
    "        avg_len = 0\n",
    "        for sequence in sequences:\n",
    "            max_len = max(max_len, len(sequence))\n",
    "            min_len = min(min_len, len(sequence))\n",
    "            avg_len += len(sequence)\n",
    "        avg_len = int(float(avg_len) / len(sequences))\n",
    "        return min_len, max_len, avg_len\n",
    "\n",
    "    def __get_max_sequence_lengths(self, context_ids, question_ids, answer_ids):\n",
    "        min_context_len, max_context_len, avg_context_len = self.__get_seq_length_stats(context_ids)\n",
    "        min_ques_len, max_ques_len, avg_ques_len = self.__get_seq_length_stats(question_ids)\n",
    "        min_answer_len, max_answer_len, avg_answer_len = self.__get_seq_length_stats(answer_ids)\n",
    "\n",
    "        if self.context_pad_len == None:\n",
    "            if self.__autopad_context != None:\n",
    "                if self.__autopad_context == 'min':\n",
    "                    self.context_pad_len = min_context_len\n",
    "                elif self.__autopad_context == 'max':\n",
    "                    self.context_pad_len = max_context_len\n",
    "                elif self.__autopad_context == 'avg':\n",
    "                    self.context_pad_len = avg_context_len\n",
    "            else:\n",
    "                self.context_pad_len = avg_context_len\n",
    "                \n",
    "        if self.ques_pad_len == None:\n",
    "            if self.__autopad_ques != None:\n",
    "                if self.__autopad_ques == 'min':\n",
    "                    self.ques_pad_len = min_ques_len\n",
    "                elif self.__autopad_ques == 'max':\n",
    "                    self.ques_pad_len = max_ques_len\n",
    "                elif self.__autopad_ques == 'avg':\n",
    "                    self.ques_pad_len = avg_ques_len\n",
    "            else:\n",
    "                self.ques_pad_len = avg_ques_len\n",
    "                \n",
    "        if self.answer_pad_len == None:\n",
    "            if self.__autopad_answer != None:\n",
    "                if self.__autopad_answer == 'min':\n",
    "                    self.answer_pad_len = min_answer_len\n",
    "                elif self.__autopad_answer == 'max':\n",
    "                    self.answer_pad_len = max_answer_len\n",
    "                elif self.__autopad_answer == 'avg':\n",
    "                    self.answer_pad_len = avg_answer_len\n",
    "            else:\n",
    "                self.answer_pad_len = avg_answer_len \n",
    "                \n",
    "        #print('Context length stats: min = {}, max = {}, avg = {}'.format(min_context_len, max_context_len, avg_context_len))\n",
    "        #print('Context pad length set to: {}'.format(self.context_pad_len))\n",
    "        \n",
    "        #print('Question length stats: min = {}, max = {}, avg = {}'.format(min_ques_len, max_ques_len, avg_ques_len))\n",
    "        #print('Question pad length set to: {}'.format(self.ques_pad_len))        \n",
    "        \n",
    "        #print('Answer length stats: min = {}, max = {}, avg = {}'.format(min_answer_len, max_answer_len, avg_answer_len))\n",
    "        #print('Answer pad length set to: {}'.format(self.answer_pad_len))        \n",
    "    def __convert_to_one_hot(self, span_ids, num_classes):\n",
    "        label = []\n",
    "        for idx in span_ids:\n",
    "            one_hot = [0 for i in range(num_classes)]\n",
    "            if idx >= num_classes - 1:\n",
    "                one_hot[-1] = 1\n",
    "            else:\n",
    "                one_hot[idx] = 1\n",
    "            label.append(one_hot)\n",
    "        return label\n",
    "    \n",
    "    def __convert_to_mask(self, i, a):\n",
    "        mask = (np.in1d(np.array(self.context_ids[i]), a[i]))*1\n",
    "        if len(a[i])>1:\n",
    "            count = 0\n",
    "            while count < len(mask):\n",
    "                if mask[count:count+len(a[i])].all():\n",
    "                    count+=len(a[i])\n",
    "                else:\n",
    "                    mask[count] = False\n",
    "                    count +=1\n",
    "        return mask\n",
    "        \n",
    "    def __tokenize_sentences(self, context_ids, question_ids, answer_ids):\n",
    "        \"\"\" Tokenizes sentences.\n",
    "        :param raw: dict returned from load_babi\n",
    "        :param word_table: WordTable\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        context_tokens = []\n",
    "        question_tokens = []\n",
    "        #answer_tokens = []\n",
    "        \n",
    "        for i in range(len(context_ids)):\n",
    "            c_tkn_ids = self.__apply_padding(context_ids[i], self.context_pad_len)\n",
    "            context_tokens.append(c_tkn_ids)\n",
    "            q_tkn_ids = self.__apply_padding(question_ids[i], self.ques_pad_len)\n",
    "            question_tokens.append(q_tkn_ids)\n",
    "            \n",
    "            span_ids = [int(x) for x in self.span[i].split()]\n",
    "            label = self.__convert_to_one_hot(span_ids, self.context_pad_len)\n",
    "            self.answer_one_hot_labels.append(label)\n",
    "            \n",
    "#             if i%100 == 0:\n",
    "#                 print(i)\n",
    "            #a_tkn_ids = self.__apply_padding(answer_ids[i], self.answer_pad_len)\n",
    "            #answer_tokens.append(a_tkn_ids)\n",
    "        return context_tokens, question_tokens#, answer_tokens\n",
    "        \n",
    "    def __convert_text2ids(self, context, question, answer):\n",
    "        \"\"\" Tokenizes sentences.\n",
    "        :param raw: dict returned from load_babi\n",
    "        :param word_table: WordTable\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        context_ids = []\n",
    "        question_ids = []\n",
    "        answer_ids = []\n",
    "        for i in range(len(context)):\n",
    "            c_ids = [self.__convert_word2id(word) for word in context[i].split()]\n",
    "            context_ids.append(c_ids)\n",
    "            q_ids = [self.__convert_word2id(word) for word in question[i].split()]\n",
    "            question_ids.append(q_ids)\n",
    "            a_ids = [self.__convert_word2id(word) for word in answer[i].split()]\n",
    "            answer_ids.append(a_ids)            \n",
    "        return context_ids, question_ids, answer_ids\n",
    "          \n",
    "    def __convert_text2words(self, context, question, answer):\n",
    "        context_words = []\n",
    "        question_words = []\n",
    "        answer_words = []\n",
    "        for i in range(len(context)):\n",
    "            c_words = context[i].split()\n",
    "            context_words.append(c_words)\n",
    "            q_words = question[i].split()\n",
    "            question_words.append(q_words)\n",
    "            a_words = answer[i].split()\n",
    "            answer_words.append(a_words)\n",
    "        return context_words, question_words, answer_words    \n",
    "\n",
    "    def __get_max_sentence_length(self, stories):\n",
    "        num_sentences = 0\n",
    "        sen_len = 0\n",
    "        for story in stories: \n",
    "            sentences = sent_tokenize(story)\n",
    "            num_sentences = max(num_sentences, len(sentences))\n",
    "            word_count = lambda sentence: len(word_tokenize(sentence))\n",
    "            sen_len = max(sen_len, len((max(sentences, key=word_count)).split()))\n",
    "        return sen_len, num_sentences\n",
    "        \n",
    "    def __context2sentences(self, story):\n",
    "        story_sentences = []\n",
    "        curr_sentence = []\n",
    "        for word_id in story:\n",
    "            if self.id2word_dict[word_id] == '.':\n",
    "                if len(curr_sentence) < self.max_sent_len_context:\n",
    "                    curr_sentence += [0 for i in range(self.max_sent_len_context - len(curr_sentence))]\n",
    "                elif len(curr_sentence) > self.max_sent_len_context:\n",
    "                    curr_sentence = curr_sentence[:self.max_sent_len_context]\n",
    "                story_sentences.append(curr_sentence)\n",
    "                curr_sentence = []\n",
    "            else:\n",
    "                curr_sentence.append(word_id)\n",
    "            \n",
    "        return story_sentences\n",
    "    \n",
    "    def __numericize_data(self):\n",
    "        c, q, a = self.__convert_text2ids(self.context, self.question, self.answer)\n",
    "        \n",
    "        self.__get_max_sequence_lengths(c, q, a)\n",
    "        \n",
    "#         self.context_ids, self.question_ids, self.answer_ids = self.__tokenize_sentences(c, q, a)\n",
    "        self.context_ids, self.question_ids = self.__tokenize_sentences(c, q, a)\n",
    "            \n",
    "#         self.context_text, self.question_text, self.answer_text = self.__convert_text2words(self.context,\n",
    "#                                                                                             self.question,\n",
    "#                                                                                             self.answer)\n",
    "        #self.max_sent_len_context, self.max_num_sentences = self.__get_max_sentence_length(self.context)\n",
    "        #print(\"max_sent_len_context: {} max_num_sentences: {}\".format(self.max_sent_len_context, self.max_num_sentences))\n",
    "        #self.max_sent_len_question, ques_num_sentences = self.__get_max_sentence_length(self.question)\n",
    "        #print(\"max_sent_len_ques: {} ques_num_sentences: {}\".format(self.max_sent_len_question, ques_num_sentences))\n",
    "        #for c_ids in c:\n",
    "            #self.context_sentences_ids.append(self.__context2sentences(c_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "UNKNOWN = \"*UNKNOWN*\"\n",
    "\n",
    "def generate_embedding_files(filename):\n",
    "    embeddings = {}\n",
    "    for line in open(filename):\n",
    "        parts = line.split()\n",
    "        embeddings[parts[0]] = list(map(float, parts[1:]))\n",
    "    embedding_size = len(list(embeddings.values())[0])\n",
    "    embeddings[UNKNOWN] = [0.0 for _ in range(embedding_size)]\n",
    "    \n",
    "    words = embeddings.keys()\n",
    "    embedding_matrix = np.array([embeddings[word] for word in list(embeddings.keys())])\n",
    "    return words, embedding_matrix\n",
    "\n",
    "glove_words, glove_embedding_matrix = generate_embedding_files('/media/ai2-rey/data_disk/data_sets/glove.6B/glove.6B.50d.txt')\n",
    "glove_vocab_lookup = {word: i for i, word  in enumerate(glove_words)}\n",
    "glove_vocab_size, glove_embedding_size= glove_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataIterator:  \n",
    "    def __init__(self, data, batch_size):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.data_iterator = self.make_random_iter()\n",
    "        \n",
    "    def next_batch(self):\n",
    "        try:\n",
    "            idxs = next(self.data_iterator)\n",
    "        except StopIteration:\n",
    "            self.data_iterator = self.make_random_iter()\n",
    "            idxs = next(self.data_iterator)\n",
    "            \n",
    "        batch = [self.data[i] for i in idxs]\n",
    "        batch_idxs = [idx for idx in idxs]\n",
    "        return batch, batch_idxs\n",
    "\n",
    "    def make_random_iter(self):\n",
    "        splits = np.arange(self.batch_size, len(self.data), self.batch_size)\n",
    "        it = np.split(np.random.permutation(range(len(self.data))), splits)[:-1]\n",
    "        return iter(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = QA_Dataset(train_cont, train_ques, train_ans, train_span,\n",
    "                   context_pad_len = 200, ques_pad_len = 20,\n",
    "                   autopad_context='max', autopad_ques = 'max', autopad_answer='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val = QA_Dataset(val_cont, val_ques, val_ans, val_span,\n",
    "                 context_pad_len=train.context_pad_len, \n",
    "                 ques_pad_len = train.ques_pad_len, \n",
    "                 answer_pad_len=train.answer_pad_len,\n",
    "                 autopad_context='max', autopad_ques='max', autopad_answer='max')\n",
    "\n",
    "test = QA_Dataset(test_cont, test_ques, test_ans, test_span,\n",
    "                  context_pad_len=train.context_pad_len, \n",
    "                  ques_pad_len = train.ques_pad_len,\n",
    "                  answer_pad_len=train.answer_pad_len,\n",
    "                  autopad_context='max', autopad_ques='max', autopad_answer='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_cont, train_ques, train_ans, train_span\n",
    "del val_cont, val_ques, val_ans, val_span\n",
    "del test_cont, test_ques, test_ans, test_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ids = list(zip(train.context_ids, train.question_ids, train.answer_one_hot_labels))\n",
    "val_ids = list(zip(val.context_ids,val.question_ids,val.answer_one_hot_labels))\n",
    "test_ids = list(zip(test.context_ids,test.question_ids, test.answer_one_hot_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_iter = DataIterator(train_ids,128)\n",
    "val_data_iter = DataIterator(val_ids,128)\n",
    "test_data_iter = DataIterator(test_ids,128)\n",
    "#deploy_data_iter = DataIterator(train_ids,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.ques_pad_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.context_pad_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_model_dir():\n",
    "    model_dir = os.getcwd() + '/'\n",
    "    if not os.path.exists(model_dir + 'weights'):\n",
    "        os.makedirs(model_dir + 'weights')\n",
    "    return model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_log(string,filename):\n",
    "    print(string)\n",
    "    with open(filename,'a') as write_file:\n",
    "        write_file.write(string + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_bounds_to_words(bounds, passage):\n",
    "    id_span = list(range(bounds[0], bounds[1]+1))\n",
    "    return ' '.join([train.id2word_dict[passage[idx]] for idx in id_span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    \n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exact_match_score(prediction, ground_truth):\n",
    "    return int(normalize_answer(prediction) == normalize_answer(ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(sess, feed_dict, passage, labels):\n",
    "    model_pred = sess.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "passage_max_len = train.context_pad_len\n",
    "question_max_len = train.ques_pad_len\n",
    "#output of lstm is n_steps or passage length by hidden size \n",
    "n_hidden = 150 \n",
    "num_indices = 2\n",
    "#batch_size = 128\n",
    "n_hidden_layers = 2\n",
    "learning_rate = 0.001\n",
    "blend_units = 6\n",
    "train_iters = 100000\n",
    "keep_prob = 0.5\n",
    "display_step = 200\n",
    "val_interval = 1000\n",
    "save_weights_interval = 1000\n",
    "word_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dir = prepare_model_dir()\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "passage = tf.placeholder(tf.int32, [None, passage_max_len], name='passage')\n",
    "question = tf.placeholder(tf.int32,[None, question_max_len], name='question')\n",
    "y = tf.placeholder(tf.float32,[None,2,passage_max_len])\n",
    "\n",
    "dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "batch_size = tf.placeholder(tf.int32)\n",
    "\n",
    "embeddings = tf.constant(glove_embedding_matrix, name='embeddings', dtype=tf.float32)\n",
    "\n",
    "#preprocessing layer \n",
    "passage_embedded = tf.nn.embedding_lookup(embeddings, passage) #P = batch_size(b), word_dims(d), max_passage_len(P)\n",
    "question_embedded = tf.nn.embedding_lookup(embeddings, question)\n",
    "print(passage_embedded[0])\n",
    "print(passage_embedded)\n",
    "\n",
    "with tf.variable_scope('passage_lstm'):\n",
    "    passage_cell = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
    "    passage_cell = tf.nn.rnn_cell.DropoutWrapper(passage_cell, output_keep_prob=dropout_keep_prob)\n",
    "    passage_cell = tf.nn.rnn_cell.MultiRNNCell([passage_cell]*n_hidden_layers)\n",
    "    #H_p shape = batch_size, max_passage_len, n_hidden\n",
    "    H_p, _ = tf.nn.dynamic_rnn(passage_cell, passage_embedded, dtype=tf.float32)\n",
    "#     H_p = tf.concat(2,H_p)\n",
    "    print(H_p)\n",
    "    \n",
    "with tf.variable_scope('question_lstm'):\n",
    "    question_cell = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
    "    question_cell = tf.nn.rnn_cell.DropoutWrapper(question_cell, output_keep_prob=dropout_keep_prob)\n",
    "    question_cell = tf.nn.rnn_cell.MultiRNNCell([question_cell]*n_hidden_layers)\n",
    "    H_q, _ = tf.nn.dynamic_rnn(question_cell, question_embedded, dtype=tf.float32)\n",
    "#     H_q = tf.concat(2,H_q)\n",
    "    print(H_q)\n",
    "\n",
    "#Match LSTM layer \n",
    "#Weights and bias to compute G\n",
    "W_q = tf.Variable(tf.truncated_normal(shape=[n_hidden, n_hidden], stddev=0.1))\n",
    "W_p = tf.Variable(tf.truncated_normal(shape=[n_hidden, n_hidden], stddev=0.1))\n",
    "W_r = tf.Variable(tf.truncated_normal(shape=[n_hidden, n_hidden], stddev=0.1))\n",
    "b_p = tf.Variable(tf.constant(0.1, shape=[n_hidden]))\n",
    "\n",
    "#Weight and bias to compute a\n",
    "w= tf.Variable(tf.truncated_normal(shape=[n_hidden,1], stddev=0.1))#unsure \n",
    "#b_alpha shape = 1,none\n",
    "b_alpha = tf.Variable(tf.constant(0.1, shape = [1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "H_q = tf.reshape(H_q, [-1, n_hidden])\n",
    "WH_q = tf.matmul(H_q, W_q)\n",
    "print(WH_q)\n",
    "WH_q = tf.reshape(WH_q,[-1, question_max_len, n_hidden])\n",
    "print(WH_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H_r_forward = []\n",
    "H_r_backward = []\n",
    "\n",
    "with tf.variable_scope('forward_match_lstm'):\n",
    "    forward_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(n_hidden), output_keep_prob=dropout_keep_prob)\n",
    "    forward_state = forward_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    h = forward_state.h #h shape =(batch_size, n_hidden)\n",
    "\n",
    "    for i in range(passage_max_len):\n",
    "        H_p_i =tf.squeeze(tf.slice(H_p, [0,i,0], [-1,1,-1]),1)\n",
    "        WH_p_i = tf.matmul(H_p_i, W_p)\n",
    "        \n",
    "        W_rh = tf.matmul(h,W_r)\n",
    "        \n",
    "        G_tile = tf.tile((WH_p_i + W_rh + b_p), [question_max_len, 1])\n",
    "        G_tile = tf.reshape(G_tile, [-1, question_max_len, n_hidden])\n",
    "        G_forward = tf.tanh(WH_q + G_tile)\n",
    "        \n",
    "        G_forward = tf.reshape(G_forward, [-1,n_hidden])\n",
    "        wG_forward = tf.matmul(G_forward,w)\n",
    "        a_tile = tf.tile(b_alpha, [question_max_len])\n",
    "        wG_forward = tf.reshape(tf.squeeze(wG_forward,1),[-1, question_max_len])\n",
    "        alpha_forward = tf.nn.softmax(wG_forward + a_tile)\n",
    "\n",
    "        H_q = tf.reshape(H_q,[-1, n_hidden, question_max_len])\n",
    "        alpha_forward = tf.expand_dims(alpha_forward, 2)\n",
    "        H_q_alpha_forward = tf.batch_matmul(H_q, alpha_forward)\n",
    "        H_q_alpha_forward = tf.squeeze(H_q_alpha_forward, 2)\n",
    "        \n",
    "        z_forward = tf.concat(1, [H_p_i, H_q_alpha_forward])\n",
    "\n",
    "        h,forward_state = forward_cell(z_forward, forward_state)\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        H_r_forward.append(h)\n",
    "        if i%100 == 0:\n",
    "            print(i)\n",
    "\n",
    "    H_r_forward = tf.transpose(tf.stack(H_r_forward),[1,0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('backward_match_lstm'):\n",
    "    backward_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(n_hidden), output_keep_prob=dropout_keep_prob)\n",
    "    backward_state = backward_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    h = backward_state.h #h shape =(bathc_size, n_hidden)\n",
    "\n",
    "    for i in reversed(range(passage_max_len)):\n",
    "        H_p_i =tf.squeeze(tf.slice(H_p, [0,i,0], [-1,1,-1]),1)\n",
    "        WH_p_i = tf.matmul(H_p_i, W_p)\n",
    "        \n",
    "        W_rh = tf.matmul(h,W_r)\n",
    "        \n",
    "        G_tile = tf.tile((WH_p_i + W_rh + b_p), [question_max_len, 1])\n",
    "        G_tile = tf.reshape(G_tile, [-1, question_max_len, n_hidden])\n",
    "        G_backward = tf.tanh(WH_q + G_tile)\n",
    "        \n",
    "        G_backward = tf.reshape(G_backward, [-1,n_hidden])\n",
    "        wG_backward = tf.matmul(G_backward,w)\n",
    "        a_tile = tf.tile(b_alpha, [question_max_len])\n",
    "        wG_backward = tf.reshape(tf.squeeze(wG_backward,1),[-1, question_max_len])\n",
    "        alpha_backward = tf.nn.softmax(wG_backward + a_tile)\n",
    "\n",
    "        H_q = tf.reshape(H_q,[-1, n_hidden, question_max_len])\n",
    "        alpha_backward = tf.expand_dims(alpha_backward, 2)\n",
    "        H_q_alpha_backward = tf.batch_matmul(H_q, alpha_backward)\n",
    "        H_q_alpha_backward = tf.squeeze(H_q_alpha_backward, 2)\n",
    "        \n",
    "        z_backward = tf.concat(1, [H_p_i, H_q_alpha_backward])\n",
    "\n",
    "        h,backward_state = backward_cell(z_backward, backward_state)\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        H_r_backward.append(h)\n",
    "        if i%100 == 0:\n",
    "            print(i)\n",
    "            \n",
    "    H_r_backward = list(reversed(H_r_backward))\n",
    "    H_r_backward = tf.transpose(tf.stack(H_r_backward),[1,0,2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "H_r = tf.concat(2, [H_r_forward, H_r_backward])\n",
    "print(H_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# H_r = H_r_forward\n",
    "# H_r = tf.reshape(H_r,[-1,n_hidden])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Answer Pointer Layer \n",
    "\n",
    "#Weights and bias to compute 'F'\n",
    "V = tf.Variable(tf.truncated_normal(shape=[2*n_hidden, n_hidden], stddev=0.1))\n",
    "print(V)\n",
    "W_a = tf.Variable(tf.truncated_normal(shape=[n_hidden, n_hidden], stddev=0.1))\n",
    "b_a = tf.Variable(tf.constant(0.1, shape=[n_hidden]))\n",
    "\n",
    "#weight and bias to computer 'beta'\n",
    "v = tf.Variable(tf.truncated_normal(shape=[n_hidden,1], stddev=0.1))\n",
    "b_beta = tf.Variable(tf.constant(0.1, shape=[1]))\n",
    "\n",
    "#Only calculate 'VH' once \n",
    "H_r = tf.reshape(H_r, [-1,2*n_hidden]) #shape = (2*n_hidden, batch&passage_max_len)\n",
    "print(H_r)\n",
    "VH = tf.matmul(H_r,V) #shape (hidden_size, passage_max_len)\n",
    "VH = tf.reshape(VH,[-1, passage_max_len, n_hidden])\n",
    "\n",
    "output_logits = []\n",
    "output_probs = []\n",
    "\n",
    "with tf.variable_scope('answer_pointer_lstm'):\n",
    "    pointer_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(n_hidden), output_keep_prob=dropout_keep_prob)\n",
    "    pointer_state = pointer_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    h = pointer_state.h\n",
    "    \n",
    "    for k in range(2):\n",
    "        W_ah = tf.matmul(h, W_a)\n",
    "        \n",
    "        F_tile = tf.tile(tf.expand_dims(tf.add(W_ah,b_a),1),[1,passage_max_len,1])\n",
    "        F = tf.tanh(tf.add(VH, F_tile))\n",
    "        F = tf.reshape(F, [-1,n_hidden])   \n",
    "        vF = tf.matmul(F,v)\n",
    "        beta_tile = tf.tile(b_beta, [passage_max_len])\n",
    "        vF = tf.reshape(tf.squeeze(vF, 1), [-1,passage_max_len])\n",
    "        beta_logits = tf.add(vF,beta_tile)\n",
    "        beta = tf.nn.softmax(beta_logits)\n",
    "        output_logits.append(beta_logits)\n",
    "        output_probs.append(beta)\n",
    "\n",
    "\n",
    "    #    beta = tf.transpose(beta,[1,0])\n",
    "        beta = tf.expand_dims(beta,2) #unsure and changed \n",
    "        H_r = tf.reshape(H_r, [-1, 2*n_hidden, passage_max_len])\n",
    "        H_r_beta = tf.batch_matmul(H_r, beta)\n",
    "        H_r_beta = tf.squeeze(H_r_beta,2)\n",
    "        h,pointer_state = pointer_cell(H_r_beta, pointer_state)\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        print(k)\n",
    "\n",
    "probs_start = output_probs[0]\n",
    "probs_end = output_probs[1]\n",
    "probs = tf.concat(1,[tf.expand_dims(probs_start,1), tf.expand_dims(probs_end,1)])\n",
    "print(probs)\n",
    "\n",
    "start_label, end_label = tf.split(1,2,y)\n",
    "start_label = tf.squeeze(start_label,1)\n",
    "end_label = tf.squeeze(end_label,1)\n",
    "print(start_label)\n",
    "\n",
    "combined_prob = tf.batch_matmul(tf.batch_matmul(tf.expand_dims(probs_start, 1), tf.expand_dims(start_label, 2)), \n",
    "                                tf.batch_matmul(tf.expand_dims(probs_end, 1), tf.expand_dims(end_label, 2)))\n",
    "loss = -tf.reduce_mean(tf.log(combined_prob))\n",
    "print(loss)\n",
    "\n",
    "params = tf.trainable_variables()\n",
    "gradients = tf.gradients(loss, params)\n",
    "gradient_norm = tf.global_norm(gradients)\n",
    "print(gradient_norm)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "print(optimizer)\n",
    "\n",
    "correct_start = tf.equal(tf.argmax(probs_start, axis=1), tf.argmax(start_label, axis=1))\n",
    "correct_end = tf.equal(tf.argmax(probs_end, axis=1), tf.argmax(end_label, axis=1))\n",
    "equal_vals = tf.equal(tf.stack([tf.argmax(probs_start, axis=1), tf.argmax(probs_end, axis=1)], axis=1),\n",
    "                      tf.stack([tf.argmax(start_label, axis=1), tf.argmax(end_label, axis=1)], axis=1))\n",
    "correct_prediction = tf.reduce_all(equal_vals, axis=1)\n",
    "print(correct_prediction)\n",
    "\n",
    "start_accuracy = tf.reduce_mean(tf.cast(correct_start, tf.float32))\n",
    "end_accuracy = tf.reduce_mean(tf.cast(correct_end, tf.float32))\n",
    "mean_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(mean_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(sess, feed_dict, passage, labels):\n",
    "    model_pred = sess.run(probs, feed_dict= feed_dict)\n",
    "    em_total = 0\n",
    "    \n",
    "    for i in range(len(model_pred)):\n",
    "        label_ids = [np.argmax(x) for x in labels[i]]\n",
    "        pred_ids = [np.argmax(x) for x in model_pred[i]]\n",
    "        \n",
    "        label_string = convert_bounds_to_words(label_ids, passage[i])\n",
    "        pred_string = convert_bounds_to_words(pred_ids, passage[i])\n",
    "        em_total += exact_match_score(pred_string, label_string)\n",
    "        if i <5:\n",
    "            print(i)\n",
    "            print(label_string)\n",
    "            print(i)\n",
    "            print(pred_string)\n",
    "    return em_total / float(len(model_pred)), pred_string, label_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_file = model_dir + 'Pnter_ntwrk_log.txt'\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "log = open(log_file,'w')\n",
    "log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for train_iter in range(train_iters):\n",
    "    \n",
    "    train_iter +=1\n",
    "    train_batch, _ = train_data_iter.next_batch()\n",
    "    train_context, train_question, train_answer = zip(*train_batch)\n",
    "    if train_iter == 1:\n",
    "        train_feed_dict = {passage: train_context,\n",
    "                           question: train_question,\n",
    "                           y: train_answer,\n",
    "                           dropout_keep_prob: 1.0, \n",
    "                           batch_size: train_data_iter.batch_size}\n",
    "        \n",
    "        training_loss, start_acc, end_acc, acc, grad_norm = tuple(sess.run([loss, start_accuracy, end_accuracy,\n",
    "                                                                            mean_accuracy, gradient_norm],\n",
    "                                                                  feed_dict=train_feed_dict))\n",
    "\n",
    "        log_output= (\"Iter {}, start_accuracy = {:.4f}, end_accuracy = {:.4f}, accuracy = {:.4f}, train loss = {:.6f}\"\n",
    "                     .format(train_iter, start_acc, end_acc, acc, training_loss))\n",
    "        print(log_output)\n",
    "        em_score, _, _ =evaluate(sess, train_feed_dict, train_context, train_answer)\n",
    "        \n",
    "        write_to_log(log_output, log_file)\n",
    "        write_to_log('Exact Match Score: {:.4f}'.format(em_score), log_file)\n",
    "        \n",
    "    sess.run(optimizer, feed_dict={passage: train_context,\n",
    "                                   question: train_question, \n",
    "                                   y: train_answer, \n",
    "                                   dropout_keep_prob: keep_prob, \n",
    "                                   batch_size: train_data_iter.batch_size})\n",
    "    if train_iter % display_step == 0:\n",
    "        disp_feed_dict = {passage: train_context, \n",
    "                          question: train_question, \n",
    "                          y: train_answer,\n",
    "                          dropout_keep_prob: 1.0,\n",
    "                          batch_size: train_data_iter.batch_size\n",
    "                         }\n",
    "        training_loss, start_acc, end_acc, acc= tuple(sess.run([loss, start_accuracy, end_accuracy, \n",
    "                                                                mean_accuracy],\n",
    "                                                               feed_dict=disp_feed_dict))\n",
    "        log_output = (\"Iter {}, start_accuracy = {:.4f}, end_accuracy = {:.4f}, accuracy = {:.4f}, train loss = {:.6f}\"\n",
    "              .format(train_iter, start_acc, end_acc, acc, training_loss))\n",
    "        print(log_output)\n",
    "        em_score, _, _ = evaluate(sess, disp_feed_dict, train_context, train_answer)\n",
    "        write_to_log(log_output, log_file)\n",
    "        write_to_log('Exact Match Score: {:.4f}'.format(em_score), log_file)\n",
    "        \n",
    "    if val_data_iter !=None and train_iter%val_interval ==0:\n",
    "        val_batch, _ = val_data_iter.next_batch()\n",
    "        val_context, val_question, val_answer = zip(*val_batch)\n",
    "        val_feed_dict = {passage: val_context,\n",
    "                         question: val_question, \n",
    "                         y: val_answer,\n",
    "                         dropout_keep_prob: 1.0,\n",
    "                         batch_size: val_data_iter.batch_size}\n",
    "        \n",
    "        val_loss, val_start_acc, val_end_acc, val_acc = tuple(sess.run([loss, start_accuracy, end_accuracy, \n",
    "                                                       mean_accuracy],\n",
    "                                                      feed_dict=val_feed_dict))\n",
    "        \n",
    "        log_output = (\"Iter {}, val_start_accuracy = {:.4f}, val_end_acc = {:.4f}, val_acc = {:.4f}, val loss = {:.6f}\"\n",
    "                      .format(train_iter, val_start_acc, val_end_acc, val_acc, val_loss))\n",
    "        print(log_output)\n",
    "        em_score, _, _ = evaluate(sess, val_feed_dict, val_context, val_answer)\n",
    "        write_to_log(log_output, log_file)\n",
    "        write_to_log('Exact Match Score: {:.4f}'.format(em_score), log_file)\n",
    "    \n",
    "    if train_iter%save_weights_interval ==0:\n",
    "        weights_dir = model_dir  + \"weights/PntrNtwrk_weights_iter--{}.ckpt\".format(train_iter)\n",
    "        save_path = saver.save(sess, weights_dir)\n",
    "        save_string = \"Model saved in file: {}\".format(save_path)\n",
    "        print(save_string)\n",
    "        write_to_log(save_string, log_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "128, 766, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_env]",
   "language": "python",
   "name": "conda-env-tensorflow_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
