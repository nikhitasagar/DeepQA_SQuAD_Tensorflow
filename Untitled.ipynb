{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json \n",
    "import re\n",
    "import math\n",
    "import os \n",
    "import datetime \n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#util\n",
    "def shuffle_in_unison(a, b):\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(b)\n",
    "\n",
    "def one_hot(idx, size):\n",
    "    v = np.zeros(size)\n",
    "    v[idx] = 1\n",
    "    return v\n",
    "\n",
    "def rand_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def weight_var(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helpers\n",
    "def tokenize(text):\n",
    "    ignore_chars = r\"[.,'!?_-]\"\n",
    "    text = re.sub(ignore_chars, \" \", text)\n",
    "    words = text.lower().split()\n",
    "    return [w for w in words if w != '']\n",
    "\n",
    "def print_distribution(numbers):\n",
    "    n = sorted(numbers)\n",
    "    return (\"{} - {}, with median {} and 90th-percentile {}\".format(n[0], n[-1], n[int(len(n)/2)], n[int(len(n)*0.9)]))\n",
    "\n",
    "assert tokenize('abc def') == ['abc', 'def']\n",
    "assert tokenize('abc-def.ghi') == ['abc', 'def', 'ghi']\n",
    "assert tokenize('A  B  ') == ['a', 'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#net\n",
    "class Net(object):\n",
    "    def __init__(self, dir_path=None, **kwargs):\n",
    "        \n",
    "        for k, v in kwargs.iteritems():\n",
    "            setattr(self, k, v)\n",
    "        \n",
    "        self.setup()\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.initialize_all_variables())\n",
    "        \n",
    "        self.was_restored = False\n",
    "        if dir_path:\n",
    "            if dir_path[-1] != '/': dir_path += '/'\n",
    "            if not os.path.exists(dir_path):\n",
    "                os.mkdir(dir_path)\n",
    "            self.dir_path = dir_path\n",
    "            \n",
    "            self.saver = tf.train.Saver()\n",
    "            ckpt = tf.train.get_checkpoint_state(self.dir_path)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                self.saver.restore(self.session, ckpt.model_checkpoint_path)\n",
    "                self.was_restored = True\n",
    "                print(\"Restored model from checkpoint {0}\".format(ckpt.model_checkpoint_path))\n",
    "        else:\n",
    "            self.saver = None\n",
    "        \n",
    "    def setup(self):\n",
    "        raise NotImplementedError(\"Subclasses should implement setup()\")\n",
    "        \n",
    "    def train(self, inputs, outputs):\n",
    "        raise NotImplementedError(\"Subclasses should implement train()\")\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        raise NotImplementedError(\"Subclasses should implement predict()\")\n",
    "    \n",
    "    def save(self, step):\n",
    "        if self.saver:\n",
    "            self.saver.save(self.session, self.dir_path + 'model.ckpt', global_step=step)\n",
    "    \n",
    "    def evaluate(self, inputs, outputs):\n",
    "        raise NotImplementedError(\"Subclasses should implement evaluate()\")\n",
    "    \n",
    "    def training_loop(self, training_batch_generator, testing_batch_generator, evaluation_interval=10):\n",
    "        step = 0\n",
    "        for step, (inp, out) in enumerate(training_batch_generator):\n",
    "            self.train(inp, out)\n",
    "            if step % evaluation_interval == 0:\n",
    "                inp, out = next(testing_batch_generator)\n",
    "                print(self.evaluate(inp, out))\n",
    "                self.save(step)\n",
    "        print(self.evaluate(inp, out))\n",
    "        self.save(step)\n",
    "\n",
    "def random_batch(inputs, outputs, count=100):\n",
    "    indices = np.random.randint(0, len(inputs)-1, count)\n",
    "    return inputs.take(indices, axis=0), outputs.take(indices, axis=0)\n",
    "\n",
    "def batch_generator(inputs, outputs, size=100, batches=None, epochs=None, random=False, print_progress=False):\n",
    "    if epochs is not None:\n",
    "        batches = int(math.ceil(len(inputs) * 1.0 / size))\n",
    "    \n",
    "    last_printed = datetime.datetime.now()\n",
    "    \n",
    "    step = 0\n",
    "    while True:\n",
    "        if random:\n",
    "            indices = np.random.randint(0, len(inputs)-1, size)\n",
    "            yield inputs.take(indices, axis=0), outputs.take(indices, axis=0)\n",
    "        else:\n",
    "            start_index = step * size % len(inputs)\n",
    "            end_index = min(start_index + size, len(inputs))\n",
    "            yield inputs[start_index:end_index], outputs[start_index:end_index]\n",
    "        step += 1\n",
    "        if print_progress and batches and (datetime.datetime.now() - last_printed).total_seconds() > 4:\n",
    "            last_printed = datetime.datetime.now()\n",
    "            print(\"{0}%\".format(step * 100.0 / batches))\n",
    "        if batches is not None and step >= batches:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage length distribution (tokens): 20 - 678, with median 110 and 90th-percentile 184\n",
      "Question length distribution (tokens): 1 - 40, with median 10 and 90th-percentile 15\n"
     ]
    }
   ],
   "source": [
    "#dataset\n",
    "data_dir = '/media/ai2-rey/data_disk/data_sets/SQuAD/'\n",
    "class Dataset(object):\n",
    "    def __init__(self, path):\n",
    "        data = json.load(open(path))\n",
    "        self.paragraphs = [Paragraph(p) for article in data['data'] for p in article['paragraphs']]\n",
    "\n",
    "class Paragraph(object):\n",
    "    def __init__(self, data):\n",
    "        self.passage = data['context']\n",
    "        self.qas = [QA(qa) for qa in data['qas']]\n",
    "\n",
    "class QA(object):\n",
    "    def __init__(self, data):\n",
    "        self.question = data['question']\n",
    "        self.answers = data['answers'] # array of dictionaries, with keys `answer_start` (a character index) and `text`\n",
    "\n",
    "train = Dataset(data_dir+'train-v1.1.json')\n",
    "test = Dataset(data_dir+'dev-v1.1.json')\n",
    "passage_length_distribution = [len(tokenize(p.passage)) for p in train.paragraphs]\n",
    "question_length_distribution = [len(tokenize(q.question)) for para in train.paragraphs for q in para.qas]\n",
    "print( \"Passage length distribution (tokens):\", print_distribution(passage_length_distribution))\n",
    "print( \"Question length distribution (tokens):\", print_distribution(question_length_distribution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n"
     ]
    }
   ],
   "source": [
    "print(train.paragraphs[0].passage)\n",
    "print(train.paragraphs[0].qas[0].question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#embedding \n",
    "UNKNOWN = \"*UNKNOWN*\"\n",
    "\n",
    "def generate_embedding_files(filename):\n",
    "    embeddings = {}\n",
    "    for line in open(filename):\n",
    "        parts = line.split()\n",
    "        embeddings[parts[0]] = list(map(float, parts[1:]))\n",
    "    embedding_size = len(list(embeddings.values())[0])\n",
    "    embeddings[UNKNOWN] = [0.0 for _ in range(embedding_size)]\n",
    "    \n",
    "    words = embeddings.keys()\n",
    "    embedding_matrix = np.array([embeddings[word] for word in list(embeddings.keys())])\n",
    "    return words, embedding_matrix\n",
    "\n",
    "def embeddings():\n",
    "    #words = json.load(open('cache/vocab.json'))\n",
    "    #matrix = np.load(open('cache/embedding.npy'))\n",
    "    return words, embedding_matrix\n",
    "\n",
    "words, embedding_matrix = generate_embedding_files('/media/ai2-rey/data_disk/data_sets/glove.6B/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab, embedding_matrix = embeddings()\n",
    "vocab_lookup = {word: i for i, word in enumerate(vocab)}\n",
    "vocab_size, embedding_size = embedding_matrix.shape\n",
    "\n",
    "passage_max_length = 200\n",
    "question_max_length = 20\n",
    "\n",
    "hidden_size = 50\n",
    "\n",
    "def vectorize(text, fixed_length=None):\n",
    "    vocab_size = len(vocab_lookup)\n",
    "    tokens = tokenize(text)\n",
    "    if fixed_length is not None:\n",
    "        tokens = (tokens + [0] * max(0, fixed_length - len(tokens)))[:fixed_length]\n",
    "    return np.array([vocab_lookup.get(token, vocab_lookup[UNKNOWN]) for token in tokens])\n",
    "\n",
    "def output_mask(passage, answers):\n",
    "    # returns a vector -- same length as the passage -- with 1 if the token is part of an answer, otherwise 0\n",
    "    answer_marker = \"$$answer$\" # hack !! !!! !  !  yikes ! ! ! ! !\n",
    "    for answer in answers:\n",
    "        replacement = \" \".join([answer_marker + w for w in tokenize(answer)])\n",
    "        passage = passage.replace(answer, replacement)\n",
    "    return np.array([(1 if token.startswith(answer_marker) else 0) for token in tokenize(passage)])\n",
    "\n",
    "def make_fixed_length(a, length):\n",
    "    return np.concatenate([a, [0] * (length - len(a))]) if len(a) < length else a[:length]\n",
    "\n",
    "def vectors_from_question(para, qa):\n",
    "    # return an (input, output) tuple, where input is a tuple (passage, question) and output is an output mask\n",
    "    print(type(para.passage))\n",
    "    print(qa.answers[0]['text'])\n",
    "    passage = vectorize(para.passage, fixed_length=passage_max_length)\n",
    "    # print para.passage\n",
    "    question = vectorize(qa.question, fixed_length=question_max_length)\n",
    "    # print qa.question\n",
    "    # print [answer['text'] for answer in qa.answers]\n",
    "    mask = output_mask(para.passage, [answer['text'] for answer in qa.answers])\n",
    "    mask = make_fixed_length(mask, passage_max_length)\n",
    "    return (passage, question), mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saint Bernadette Soubirous\n",
      "September 1876\n",
      "Rome\n",
      "eight\n",
      "Learning Resource Center\n",
      "1924\n",
      "Joan B. Kroc Institute for International Peace Studies\n",
      "14\n",
      "3,577\n",
      "18th overall\n",
      "Father Joseph Carrier, C.S.C.\n",
      "1882\n",
      "The Lobund Institute\n",
      "The Review of Politics\n",
      "John Jenkins\n",
      "8,448\n",
      "80%\n",
      "Congregatio a Sancta Cruce\n",
      "Washington Hall\n",
      "scholastic and classical\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):    \n",
    "    print(train.paragraphs[i].qas[0].answers[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "the 1940s\n",
      "[ 92178 231801 271140 320117 194553  48384 251511 309502 246252 331526\n",
      " 123312 349485  69676  64890  88846  41309  19814 185030  38690 331526\n",
      "  41309  94653 167381 185710 108035 123312  35809  85041 139361  11166\n",
      "  85041  41546 172559  85041 333475 164460  92178 123312  82442 286349\n",
      " 239654 126155 379080 173234  19814 168850 330996 213770 161473 121309\n",
      " 209216  85041   2971 297314 331247  19814 336198 193380 111238 299432\n",
      " 126155  19814 388236 157727  19814 320117 330996  19814 360138 213770\n",
      "  57092 121309 123312  44753  85041 193243 133170  38690 154637  85041\n",
      "     99  19814 116177 175473 213770  51506  83922  19814 185710 288040\n",
      " 352366 299432  23255 188624  19814 145139  51506 385142  85041 290277\n",
      "  76140 366442  85041 320117  64890  85041 337697 294263 246252 123789\n",
      " 108035  48384 251511  85041 202358  30049 309502 341100 297521 168850\n",
      "  85041  19814 116177  19814 145139 213770 323215 152219 299432  85041\n",
      "  18142 394130 288040 123312 120788 344390 210773 122009 282803 223925\n",
      " 123789  19814 366442 126155 384897 219959 281301 246252 230079 154488\n",
      "  19814 145139 318885 297314 130886 123312 196903 384379 123312 102219\n",
      "  83347  75408 283084 316840  51506  91775 126155 160765 219959 320117\n",
      " 246252 230079 154488  19814 320029 362199 123312 102219 384379  19814\n",
      " 196903 320029 385024   7352 198597  31612  34892 228062 320029 213770\n",
      "  51506  92178 307900  92178  19814 102470 242792 258206 167381 385906]\n",
      "[ 30247 245083 103902 399374 102470  69676  48384 251511  16630 152219\n",
      " 102470 102470 102470 102470 102470 102470 102470 102470 102470 102470]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "(a,b), c = vectors_from_question(train.paragraphs[1], train.paragraphs[12].qas[1])\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SQuAD\n",
    "\n",
    "def questions_from_dataset(ds):\n",
    "    for para in ds.paragraphs:\n",
    "        for qa in para.qas:\n",
    "            yield (para, qa)\n",
    "\n",
    "class Squad(Net):\n",
    "    def setup(self):\n",
    "        passage = tf.placeholder(tf.int32, [None, passage_max_length], name='passage')  # shape (batch_size, passage_max_length)\n",
    "        question = tf.placeholder(tf.int32, [None, question_max_length], name='question')  # shape (batch_size, question_max_length)\n",
    "        desired_output = tf.placeholder(tf.float32, [None, passage_max_length], name='desired_output')  # shape (batch_size, passage_max_length)\n",
    "        \n",
    "        embedding = tf.constant(embedding_matrix, name='embeddintf.nn\n",
    "\n",
    "        #######################\n",
    "        # Preprocessing layer #\n",
    "        ####################### \n",
    "\n",
    "        passage_embedded = tf.nn.embedding_lookup(embedding, passage)  # shape (batch_size, passage_max_length, embedding_size)\n",
    "        question_embedded = tf.nn.embedding_lookup(embedding, question)  # shape (batch_size, question_max_length, embedding_size)\n",
    "\n",
    "        dropout = tf.placeholder(tf.float32)\n",
    "\n",
    "        with tf.variable_scope('passage_lstm'):\n",
    "            passage_cell = tf.nn.rnn_cell.LSTMCell(hidden_size)\n",
    "            passage_cell = tf.nn.rnn_cell.DropoutWrapper(passage_cell, output_keep_prob=dropout)\n",
    "            passage_cell = tf.nn.rnn_cell.MultiRNNCell([passage_cell] * 2)\n",
    "            H_p, _ = tf.nn.dynamic_rnn(passage_cell, passage_embedded, dtype=tf.float32)  # shape (batch_size, passage_max_length, hidden_size)\n",
    "        \n",
    "        with tf.variable_scope('question_lstm'):\n",
    "            question_cell = tf.nn.rnn_cell.LSTMCell(hidden_size)\n",
    "            question_cell = tf.nn.rnn_cell.DropoutWrapper(question_cell, output_keep_prob=dropout)\n",
    "            question_cell = tf.nn.rnn_cell.MultiRNNCell([question_cell] * 2)\n",
    "            H_q, _ = tf.nn.dynamic_rnn(question_cell, question_embedded, dtype=tf.float32)  # shape (batch_size, question_max_length, hidden_size)\n",
    "\n",
    "\n",
    "        ####################\n",
    "        # Match-LSTM layer #\n",
    "        ####################\n",
    "\n",
    "        # Weights and bias to compute `G`\n",
    "        W_q = self.weight_variable(shape=[hidden_size, hidden_size])\n",
    "        W_p = self.weight_variable(shape=[hidden_size, hidden_size])\n",
    "        W_r = self.weight_variable(shape=[hidden_size, hidden_size])\n",
    "        b_p = self.bias_variable(shape=[hidden_size])\n",
    "\n",
    "        # Weight and bias to compute `a`\n",
    "        w = self.weight_variable(shape=[hidden_size])\n",
    "        b_alpha = self.bias_variable(shape=[])   # In the paper, this is `b`\n",
    "\n",
    "        # Only calculate `WH_q` once\n",
    "        WH_q = tf.matmul(W_q, H_q)\n",
    "\n",
    "        # Results for forward and backward LSTMs\n",
    "        H_r_forward = []\n",
    "        H_r_backward = []\n",
    "\n",
    "        with tf.variable_scope('forward_match_lstm'):\n",
    "            forward_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(hidden_size), output_keep_prob=dropout)\n",
    "            forward_state = forward_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "            h = forward_state.h\n",
    "            for i in range(len(H_p)):\n",
    "                G_forward = tf.tanh(WH_q + tf.tile((tf.matmul(W_p, H_p[i]) + tf.matmul(W_r, h) + b_p), [question_max_length, 1]))\n",
    "                alpha_forward = tf.nn.softmax(w * G_forward + tf.tile(b_alpha, [question_max_length, 1]))\n",
    "\n",
    "                z_forward = tf.concatenate(H_p[i], H_q * alpha_forward[i])\n",
    "                h, forward_state = forward_cell(z_forward, forward_state)\n",
    "                H_r_forward.append(h)\n",
    "\n",
    "        with tf.variable_scope('backward_match_lstm'):\n",
    "            backward_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(hidden_size), output_keep_prob=dropout)\n",
    "            backward_state = backward_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "            h = backward_state.h\n",
    "            for i in reversed(range(len(H_p))):\n",
    "                G_backward = tf.tanh(WH_q + tf.tile((tf.matmul(W_p, H_p[i]) + tf.matmul(W_r, h) + b_p), [question_max_length, 1]))\n",
    "                alpha_backward = tf.nn.softmax(w * G_backward + tf.tile(b_alpha, [question_max_length, 1]))\n",
    "\n",
    "                z_backward = tf.concatenate(H_p[i], H_q * alpha_backward[i])\n",
    "                h, backward_state = backward_cell(z_backward, backward_state)\n",
    "                H_r_backward.append(h)\n",
    "\n",
    "        # After finding forward and backward `H_r[i]` for all `i`, concatenate `H_r_forward` and `H_r_backward`\n",
    "        H_r = tf.concatenate(H_r_forward, H_r_backward)\n",
    "\n",
    "        # TODO: Assert that the shape of `H_r` is (2 * hidden_size, passage_max_length)\n",
    "\n",
    "\n",
    "        ########################\n",
    "        # Answer-Pointer layer #\n",
    "        ########################\n",
    "\n",
    "        # TODO: Switch this over to boundary model or add zero vector padding at end of H_r\n",
    "        #       ^ Might not be necessary ??\n",
    "\n",
    "        # Weights and bias to compute `F`\n",
    "        V = self.weight_variable(shape=[hidden_size, 2 * hidden_size])\n",
    "        W_a = self.weight_variable(shape=[hidden_size, hidden_size])\n",
    "        b_a = self.bias_variable(shape=[hidden_size])   # In the paper, this is `c`\n",
    "\n",
    "        # Weight and bias to compute `beta`\n",
    "        v = self.weight_variable(shape=[hidden_size])\n",
    "        b_beta = self.bias_variable(shape=[])\n",
    "\n",
    "        # Only calculate `VH` once\n",
    "        VH = tf.matmul(V, H_r)        # shape (hidden_size, passage_max_length)\n",
    "\n",
    "        H_a = []\n",
    "\n",
    "        with tf.variable_scope('answer_pointer_lstm'):\n",
    "            pointer_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(hidden_size), output_keep_prob=dropout)\n",
    "            pointer_state = pointer_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "            h = pointer_state.h\n",
    "            for k in range(len(H_p)):\n",
    "                F = tf.tanh(VH + tf.tile((tf.matmul(W_a, H_a[k]) + b_a), [passage_max_length, 1]))\n",
    "                beta = tf.nn.softmax(v * F + tf.tile(b_beta, [passage_max_length, 1]))\n",
    "\n",
    "                h, pointer_state = pointer_cell(tf.matmul(H_r, beta), pointer_state)\n",
    "                H_a.append(h)\n",
    "        \n",
    "\n",
    "        # TODO: Replace the loss function below with the loss function from the paper\n",
    "        loss = tf.reduce_mean(tf.reduce_sum(tf.pow(desired_output - output, 2), reduction_indices=[1]))\n",
    "        train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "        \n",
    "        self.passage = passage\n",
    "        self.question = question\n",
    "        self.output = output\n",
    "        self.desired_output = desired_output\n",
    "        self.train_step = train_step\n",
    "        self.loss = loss\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def train(self, paragraph_question_pairs):\n",
    "        vectors = [vectors_from_question(p, q) for p, q in paragraph_question_pairs]\n",
    "        # print vectors[0]\n",
    "        questions = np.array([question for ((passage, question), mask) in vectors])\n",
    "        passages = np.array([passage for ((passage, question), mask) in vectors])\n",
    "        masks = np.array([mask for ((passage, question), mask) in vectors])\n",
    "        \n",
    "        feed = {self.passage: passages, self.question: questions, self.desired_output: masks, self.dropout: 0.5}\n",
    "        _, loss = self.session.run([self.train_step, self.loss], feed_dict=feed)\n",
    "        print loss\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_variable(shape, name=None):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial, name=name)\n",
    "\n",
    "    @staticmethod\n",
    "    def bias_variable(shape, name=None):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial, name=name)\n",
    "\n",
    "def iterate_batches(list, size=10):\n",
    "    i = 0\n",
    "    while True:\n",
    "        yield [list[i+j] for j in range(size)]\n",
    "        i += size\n",
    "\n",
    "n = Squad(dir_path='save/squad1')\n",
    "\n",
    "def train():\n",
    "    train_questions = list(questions_from_dataset(train))\n",
    "    random.shuffle(train_questions)\n",
    "    test_questions = list(questions_from_dataset(test))\n",
    "    random.shuffle(test_questions)\n",
    "    \n",
    "    i = 0\n",
    "    for i, batch in enumerate(iterate_batches(train_questions, size=20)):\n",
    "        n.train(batch)\n",
    "        if i % 10 == 0:\n",
    "            n.save(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_env]",
   "language": "python",
   "name": "conda-env-tensorflow_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
