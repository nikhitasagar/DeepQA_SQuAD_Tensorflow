{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import shuffle\n",
    "import os\n",
    "import gensim\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import operator\n",
    "import math\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.models.rnn.translate import seq2seq_model\n",
    "import string\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import split, explode\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext('local[4]')\n",
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_punct(line):    \n",
    "    punctuation = [x for x in list(string.punctuation)]\n",
    "    space_punct = [' {0}'.format(elem) for elem in punctuation]\n",
    "    replace_punctuation = str.maketrans(dict(zip(punctuation, space_punct)))\n",
    "    line = line.translate(replace_punctuation)\n",
    "    return line.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preproccessData(filename):\n",
    "    \n",
    "    data_dir = '/media/ai2-rey/data_disk/data_sets/SQuAD/'\n",
    "    df = sqlcontext.read.json(data_dir+filename)\n",
    "    df = df.withColumn('data', explode('data'))\n",
    "    df = df.select('data.paragraphs')\n",
    "    df = df.withColumn('paragraphs', explode('paragraphs'))\n",
    "    df = df.select(['paragraphs.context','paragraphs.qas'])\n",
    "    df = df.withColumn('qas', explode('qas'))\n",
    "    df = df.selectExpr(['context as context','qas.question as question','qas.answers.text as answer'])\n",
    "    df = df.withColumn('answer', explode('answer'))\n",
    "    \n",
    "    lowercase_UDF = func.UserDefinedFunction(replace_punct, StringType())\n",
    "\n",
    "    df = (df.withColumn('context_lower', lowercase_UDF(df.context))\n",
    "            .withColumn('question_lower', lowercase_UDF(df.question))\n",
    "            .withColumn('answer_lower', lowercase_UDF(df.answer))\n",
    "            .drop('context')\n",
    "            .drop('question')\n",
    "            .drop('answer')\n",
    "            .withColumnRenamed('context_lower', 'context')\n",
    "            .withColumnRenamed('question_lower', 'question')\n",
    "            .withColumnRenamed('answer_lower', 'answer'))\n",
    "    \n",
    "    print(df.printSchema())\n",
    "    pandas_df = df.toPandas()\n",
    "    context = df.select(\"context\").rdd.flatMap(lambda x: x).collect()\n",
    "    question = df.select(\"question\").rdd.flatMap(lambda x: x).collect()\n",
    "    answer = df.select(\"answer\").rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    return pandas_df, context, question, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- context: string (nullable = true)\n",
      " |-- question: string (nullable = true)\n",
      " |-- answer: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trian_pandas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-341e68a4016b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_pandas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_question\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreproccessData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train-v1.1.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrian_pandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trian_pandas' is not defined"
     ]
    }
   ],
   "source": [
    "train_pandas, train_context, train_question, train_answer = preproccessData('train-v1.1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>architecturally , the school has a catholic ch...</td>\n",
       "      <td>to whom did the virgin mary allegedly appear i...</td>\n",
       "      <td>saint bernadette soubirous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>architecturally , the school has a catholic ch...</td>\n",
       "      <td>what is in front of the notre dame main buildi...</td>\n",
       "      <td>a copper statue of christ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>architecturally , the school has a catholic ch...</td>\n",
       "      <td>the basilica of the sacred heart at notre dame...</td>\n",
       "      <td>the main building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>architecturally , the school has a catholic ch...</td>\n",
       "      <td>what is the grotto at notre dame ?</td>\n",
       "      <td>a marian place of prayer and reflection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>architecturally , the school has a catholic ch...</td>\n",
       "      <td>what sits on top of the main building at notre...</td>\n",
       "      <td>a golden statue of the virgin mary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  architecturally , the school has a catholic ch...   \n",
       "1  architecturally , the school has a catholic ch...   \n",
       "2  architecturally , the school has a catholic ch...   \n",
       "3  architecturally , the school has a catholic ch...   \n",
       "4  architecturally , the school has a catholic ch...   \n",
       "\n",
       "                                            question  \\\n",
       "0  to whom did the virgin mary allegedly appear i...   \n",
       "1  what is in front of the notre dame main buildi...   \n",
       "2  the basilica of the sacred heart at notre dame...   \n",
       "3                 what is the grotto at notre dame ?   \n",
       "4  what sits on top of the main building at notre...   \n",
       "\n",
       "                                    answer  \n",
       "0               saint bernadette soubirous  \n",
       "1                a copper statue of christ  \n",
       "2                        the main building  \n",
       "3  a marian place of prayer and reflection  \n",
       "4       a golden statue of the virgin mary  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_context, test_question, test_answer = preproccessData('dev-v1.1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/media/ai2-rey/data_disk/data_sets/SQuAD/'\n",
    "df_train = sqlcontext.read.json(data_dir+'train-v1.1.json')\n",
    "df_test = sqlcontext.read.json(data_dir+'dev-v1.1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = df.withColumn('data', explode('data'))\n",
    "df_test = df.withColumn('data', explode('data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.select('data.paragraphs')\n",
    "df = df.withColumn('paragraphs', explode('paragraphs'))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.select(['paragraphs.context','paragraphs.qas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('qas', explode('qas'))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.selectExpr(['context as context','qas.question as question','qas.answers.text as answer'])\n",
    "df = df.withColumn('answer', explode('answer'))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.take(7)[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def replace_punct(line):    \n",
    "    punctuation = [x for x in list(string.punctuation)]\n",
    "    space_punct = [' {0}'.format(elem) for elem in punctuation]\n",
    "    replace_punctuation = str.maketrans(dict(zip(punctuation, space_punct)))\n",
    "    line = line.translate(replace_punctuation)\n",
    "    return line.lower()\n",
    "\n",
    "lowercase_UDF = func.UserDefinedFunction(replace_punct, StringType())\n",
    "\n",
    "df = (df.withColumn('context_lower', lowercase_UDF(df.context))\n",
    "                .withColumn('question_lower', lowercase_UDF(df.question))\n",
    "                .withColumn('answer_lower', lowercase_UDF(df.answer))\n",
    "                .drop('context')\n",
    "                .drop('question')\n",
    "                .drop('answer')\n",
    "                .withColumnRenamed('context_lower', 'context')\n",
    "                .withColumnRenamed('question_lower', 'question')\n",
    "                .withColumnRenamed('answer_lower', 'answer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pandas_df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context = df.select(\"context\").rdd.flatMap(lambda x: x).collect()\n",
    "question = df.select(\"question\").rdd.flatMap(lambda x: x).collect()\n",
    "answer = df.select(\"answer\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counter = {}\n",
    "\n",
    "def count_words(text):\n",
    "    for line in text:\n",
    "        for word in line.split():\n",
    "            word_counter[word] = word_counter.get(word, 0) + 1\n",
    "\n",
    "count_words(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_counter['school']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QA_Dataset:\n",
    "    \"\"\" This class parses and preprocesses context-utterance datasets for Information Retrieval models\n",
    "    \n",
    "    Attributes: \n",
    "        train_file (str)        : directory location where the train dataset is located\n",
    "        val_file (str)          : directory location where the validation dataset is located\n",
    "        test_file (str)         : directory location where the test dataset is located\n",
    "        model_type (str)        : type of model that data will be used for (e.g. TF-IDF, LSTM, etc.) \n",
    "        context_pad_len (int)   : length that context sequences will be truncated to\n",
    "        utterance_pad_len (int) : length that utterance sequences will be truncated to\n",
    "        max_vocab_size (int)    : number of words that the model will recognize\n",
    "        max_data_size (int)     : number of data samples to load\n",
    "        word_counter (dict)     : dictionary to count the instances of each word in the train data\n",
    "        vocab (set)             : set of unique words in the train data\n",
    "        train_text_data (list)  : list containing all samples of training data in text sequence form. \n",
    "        val_text_data (list)    : list containing all samples of validation data in text sequence form\n",
    "        test_text_data (list)   : list containing all samples of test data in text sequence form\n",
    "        num_tokens (int)        : number of unique tokens that will be embedded\n",
    "        word2id_dict (dict)     : dictionary that converts words into unique ids\n",
    "        id2word_dict (dict)     : dictionary that converts ids into unique words\n",
    "        train_id_data (list)    : list containing all samples of training data in token sequence form\n",
    "        val_id_data (list)      : list containing all samples of validation data in token sequence form\n",
    "        test_id_data (list)     : list containing al samples of test data in token sequence form\n",
    "            \n",
    "    \"\"\"  \n",
    "    def __init__(self, train_file, val_file, test_file, model_type, context_pad_len=None, utterance_pad_len=None, \n",
    "                 max_vocab_size=None, max_data_size=None):\n",
    "        \"\"\" Load data and apply preprocessing based on model type\n",
    "        \n",
    "        Args: \n",
    "            train_file (str)        : directory location where the train dataset is located\n",
    "            val_file (str)          : directory location where the validation dataset is located\n",
    "            test_file (str)         : directory location where the test dataset is located\n",
    "            model_type (str)        : type of model that data will be used for (e.g. TF-IDF, LSTM, etc.) \n",
    "            context_pad_len (int)   : length that context sequences will be truncated to\n",
    "            utterance_pad_len (int) : length that utterance sequences will be truncated to\n",
    "            max_vocab_size (int)    : number of words that the model will recognize\n",
    "            max_data_size (int)     : number of data samples to load\n",
    "            \n",
    "        \"\"\"\n",
    "        assert model_type in {'dual_LSTM'}\n",
    "        \n",
    "        self.train_file = train_file\n",
    "        self.test_file = test_file\n",
    "        self.model_type = model_type\n",
    "        self.context_pad_len = context_pad_len\n",
    "        self.utterance_pad_len = utterance_pad_len\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_data_size = max_data_size\n",
    "        self.word_counter = dict()\n",
    "        self.vocab = set()\n",
    "        self.train_text_data = None\n",
    "        self.val_text_data = None\n",
    "        self.test_text_data = None\n",
    "        self.num_tokens = None \n",
    "        \n",
    "        self.__load_data()\n",
    "        self.word2id_dict, self.id2word_dict = self.__create_word2id_dict()\n",
    "        self.train_id_data = self.__tokenize_sentences(self.train_text_data, 'train')\n",
    "        self.val_id_data = self.__tokenize_sentences(self.val_text_data, 'val')\n",
    "        self.test_id_data = self.__tokenize_sentences(self.test_text_data, 'test')\n",
    "        \n",
    "        if self.context_pad_len == None:\n",
    "            self.context_pad_len, min_context_len, avg_context_len = self.__get_max_sequence_length(zip(*self.train_id_data)[0])\n",
    "            print 'Context Lengths: max = {}, min = {}, avg = {}'.format(self.context_pad_len, min_context_len, avg_context_len)\n",
    "        if self.utterance_pad_len == None:\n",
    "            self.utterance_pad_len, min_utterance_len, avg_utterance_len = self.__get_max_sequence_length(zip(*self.train_id_data)[1])\n",
    "            print 'Utterance Lengths: max = {}, min = {}, avg = {}'.format(self.utterance_pad_len, min_utterance_len, avg_utterance_len)\n",
    "            \n",
    "        self.train_id_data = self.__pad_sequences(self.train_id_data, 'train')\n",
    "        self.val_id_data = self.__pad_sequences(self.val_id_data, 'val')\n",
    "        self.test_id_data = self.__pad_sequences(self.test_id_data, 'test')\n",
    "        \n",
    "    def __update_word_counter(self, sequence):\n",
    "        \"\"\" Update word_counter with counts for words in a sentence\n",
    "        \n",
    "        Args:\n",
    "            sequence (list<str>) : list of words in a sequence\n",
    "        \n",
    "        \"\"\"\n",
    "        for word in sequence:\n",
    "            self.word_counter[word] = self.word_counter.get(word, 0) + 1\n",
    "            \n",
    "    def __create_vocab(self):\n",
    "        \"\"\" Create set of most frequent unique words found in the training data \"\"\"\n",
    "        \n",
    "        if self.max_vocab_size == None:\n",
    "            self.vocab == set(self.word_counter.keys())\n",
    "        else:\n",
    "            self.vocab = set(sorted(self.word_counter, key=self.word_counter.get, reverse=True)[:self.max_vocab_size])\n",
    "        \n",
    "    def __parse_data(self, df, phase):\n",
    "        \"\"\" Convert raw data into sets of word sequences\n",
    "        \n",
    "        Args:\n",
    "            df (pandas.dataframe) : dataframe containing raw text samples loaded from file\n",
    "            phase (str)           : defines format in which to parse the data (train, val, test)\n",
    "            \n",
    "        Returns:\n",
    "            list<tuple> : returns a list of tuples where each tuple is a sample of context sequences,\n",
    "                          utterance sequences, and labels (for train phase) or distractor sequences (for val and test phases)\n",
    "                          \n",
    "        \"\"\"\n",
    "        assert phase in {'train', 'val', 'test'} \n",
    "        parsed_text_data = []\n",
    "        \n",
    "        if phase == 'train':\n",
    "            for index, row in df.iterrows():\n",
    "                context = row['Context'].split(' ')\n",
    "                utterance = row['Utterance'].split(' ')\n",
    "                label = row['Label']\n",
    "                \n",
    "                self.__update_word_counter(context)\n",
    "                self.__update_word_counter(utterance)\n",
    "                \n",
    "                parsed_text_data.append((context, utterance, label))\n",
    "            self.__create_vocab()\n",
    "        elif phase == 'test' or phase == 'val':\n",
    "            for index, row in df.iterrows():\n",
    "                context = row['Context'].split(' ')\n",
    "                gtu = row['Ground Truth Utterance'].split(' ')\n",
    "                num_distractors = len(row) - 2\n",
    "                distractors_list = []\n",
    "                for i in range(num_distractors):\n",
    "                    d_i = row[i+2].split(' ')\n",
    "                    distractors_list.append(d_i)\n",
    "                \n",
    "                parsed_text_data.append((context, gtu) + tuple(distractors_list))\n",
    "        return parsed_text_data\n",
    "                \n",
    "    def __load_data(self):\n",
    "        \"\"\" Loads raw train, validation, and test data from file into pandas dataframes \"\"\"\n",
    "        \n",
    "        train_df = pd.read_csv(self.train_file)\n",
    "        val_df = pd.read_csv(self.val_file)\n",
    "        test_df = pd.read_csv(self.test_file)\n",
    "        \n",
    "        if self.max_data_size != None:\n",
    "            train_df = train_df.sample(self.max_data_size)\n",
    "\n",
    "        self.train_text_data = self.__parse_data(train_df, 'train')\n",
    "        self.val_text_data = self.__parse_data(val_df, 'val')\n",
    "        self.test_text_data = self.__parse_data(test_df, 'test')\n",
    "        \n",
    "        random.shuffle(self.train_text_data)\n",
    "        random.shuffle(self.val_text_data)\n",
    "        random.shuffle(self.test_text_data)\n",
    "        \n",
    "        del train_df, val_df, test_df\n",
    "        \n",
    "    def __create_word2id_dict(self):\n",
    "        \"\"\" Create a dictionary to map each word in the vocab set to a unique id \n",
    "        \n",
    "        Returns:\n",
    "            dict, dict : A dictionary mapping words to ids and a dictionary mapping ids to words\n",
    "            \n",
    "        \"\"\"\n",
    "        word2id_dict = dict()\n",
    "        \n",
    "        if self.model_type == 'dual_LSTM':\n",
    "            word2id_dict['PAD'] = 0\n",
    "            word2id_dict['UNK'] = 1\n",
    "            \n",
    "        for word in self.vocab:\n",
    "            word2id_dict[word] = len(word2id_dict)\n",
    "            \n",
    "        self.vocab.add('PAD')\n",
    "        self.vocab.add('UNK')\n",
    "        id2word_dict = dict(zip(word2id_dict.values(), word2id_dict.keys()))\n",
    "        self.num_tokens = len(word2id_dict)\n",
    "        return word2id_dict, id2word_dict\n",
    "    \n",
    "    def __convert_seq2id(self, sequence):\n",
    "        \"\"\" Convert a sequence of text into a sequence of ids \n",
    "        \n",
    "        Returns:\n",
    "            list<int> : a list of integers where each integer is a word id\n",
    "            \n",
    "        \"\"\"\n",
    "        sequence_ids = []\n",
    "        for word in sequence:\n",
    "            try:\n",
    "                word_id = self.word2id_dict[word]\n",
    "            except:\n",
    "                word_id = self.word2id_dict['UNK']\n",
    "            sequence_ids.append(word_id)\n",
    "        return sequence_ids\n",
    "    \n",
    "    def __tokenize_sentences(self, text_data, phase):\n",
    "        \"\"\" Convert text data into tokenized id data \n",
    "        \n",
    "        Args:\n",
    "            text_data (list<tuple>) : list of sample tuples with text sequence data\n",
    "            phase (str)             : defines format in which to process the data (train, val, test)\n",
    "            \n",
    "        Returns:\n",
    "            list<tuple> : list of sample tuples with tokenized id sequence data\n",
    "        \n",
    "        \"\"\"\n",
    "        assert phase in {'train', 'val', 'test'} \n",
    "        id_data = []\n",
    "        \n",
    "        if phase == 'train':\n",
    "            for sample in text_data:\n",
    "                context_ids, utterance_ids = (self.__convert_seq2id(seq) for seq in sample[:2])\n",
    "                id_data.append((context_ids, utterance_ids, sample[2]))\n",
    "        elif phase == 'val' or phase == 'test':\n",
    "            for sample in text_data:\n",
    "                id_data.append(tuple([self.__convert_seq2id(seq) for seq in sample]))\n",
    "        return id_data\n",
    "    \n",
    "    def __get_max_sequence_length(self, sequences):\n",
    "        \"\"\" Finds the maximum, minimum, and average length of a set of sequences \n",
    "        \n",
    "        Args:\n",
    "            sequences (list<list>) : list of sequences \n",
    "        \n",
    "        Returns:\n",
    "            int : max sequence length\n",
    "            int : minimum sequence length\n",
    "            int : average sequence length\n",
    "            \n",
    "        \"\"\"\n",
    "        max_len = 0\n",
    "        min_len = 10000\n",
    "        avg_len = 0\n",
    "        for sequence in sequences:\n",
    "            max_len = max(max_len, len(sequence))\n",
    "            min_len = min(min_len, len(sequence))\n",
    "            avg_len += len(sequence)\n",
    "        avg_len = int(float(avg_len) / len(sequences))\n",
    "        return max_len, min_len, avg_len\n",
    "    \n",
    "    def __apply_padding(self, sequence, pad_len):\n",
    "        \"\"\" Pads a set of sequences to equal length\n",
    "        \n",
    "        Args:\n",
    "            sequence (list<int>) : list of id tokens\n",
    "            pad_len (int)        : desired length to pad/truncate all sequences to\n",
    "        \n",
    "        Returns:\n",
    "            list<int> : list of id tokens representing padded sequence\n",
    "        \n",
    "        \"\"\"\n",
    "        padded_sequence = sequence[:]\n",
    "        \n",
    "        if len(sequence) < pad_len:\n",
    "            for i in range(pad_len - len(sequence)):\n",
    "                padded_sequence.append(0)\n",
    "        elif len(sequence) > pad_len:\n",
    "            padded_sequence = sequence[:pad_len]\n",
    "        return padded_sequence\n",
    "    \n",
    "    def __pad_sequences(self, id_data, phase):\n",
    "        \"\"\" Adds padding to all data and pairs each sequence to its original length \n",
    "        \n",
    "        Args: \n",
    "            id_data (list<tuple>) : list of sample tuples with id sequence data\n",
    "            phase (str)           : defines format in which to process the data (train, val, test)\n",
    "            \n",
    "        Returns:\n",
    "            list<tuple> : list of sample tuples with padded id sequences pair with original lengths\n",
    "            \n",
    "        \"\"\"\n",
    "        assert phase in {'train', 'val', 'test'} \n",
    "        \n",
    "        if self.model_type == 'dual_LSTM':\n",
    "            padded_id_data = []\n",
    "            \n",
    "            if phase == 'train':\n",
    "                for sample in id_data:\n",
    "                    padded_id_data.append(((self.__apply_padding(sample[0], self.context_pad_len), \n",
    "                                           min(len(sample[0]), self.context_pad_len)),\n",
    "                                          (self.__apply_padding(sample[1], self.utterance_pad_len), \n",
    "                                           min(len(sample[1]), self.utterance_pad_len)),\n",
    "                                          sample[2]))\n",
    "            elif phase == 'val' or phase =='test':\n",
    "                for sample in id_data:\n",
    "                    padded_id_data.append(((self.__apply_padding(sample[0], self.context_pad_len), \n",
    "                                           min(len(sample[0]), self.context_pad_len)),) + \n",
    "                                          tuple([(self.__apply_padding(seq, self.utterance_pad_len), \n",
    "                                            min(len(seq), self.utterance_pad_len)) for seq in sample[1:]]))\n",
    "            return padded_id_data"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_env]",
   "language": "python",
   "name": "conda-env-tensorflow_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
