{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import shuffle\n",
    "import os\n",
    "import gensim\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import operator\n",
    "import math\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import string\n",
    "import fileinput\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import nltk\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import split, explode\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors, DenseVector, VectorUDT\n",
    "\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.models.rnn.translate import seq2seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_PYTHON\"] = 'python2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext('local[4]')\n",
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/media/ai2-rey/data_disk/data_sets/SQuAD/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_dir, prefix):\n",
    "    filenames = [data_dir + prefix + tag for tag in ['.context', '.question', '.answer', '.span']]\n",
    "\n",
    "    data = []\n",
    "    column = []\n",
    "    line_iter = fileinput.input(filenames)\n",
    "    for line in line_iter:\n",
    "        if line_iter.isfirstline():\n",
    "            data.append(column)\n",
    "            column = []\n",
    "        column.append(line.rstrip())\n",
    "    data.append(column)\n",
    "    line_iter.close()\n",
    "    data = data[1:]\n",
    "    return list(zip(data[0], data[1], data[2], data[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = load_data(data_dir, 'train')\n",
    "train_cont, train_ques, train_ans, train_span = list(zip(*train_data))\n",
    "\n",
    "val_data = load_data(data_dir, 'val')\n",
    "val_cont, val_ques, val_ans, val_span = list(zip(*val_data))\n",
    "\n",
    "test_data = load_data(data_dir, 'dev')\n",
    "test_cont, test_ques, test_ans, test_span = list(zip(*test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Martorana church ( decorated around 1143 ) looked originally even more Byzantine although important parts were later demolished . The dome mosaic is similar to that of the Cappella Palatina , with Christ enthroned in the middle and four bowed , elongated angels . The Greek inscriptions , decorative patterns , and evangelists in the squinches are obviously executed by the same Greek masters who worked on the Cappella Palatina . The mosaic depicting Roger II of Sicily , dressed in Byzantine imperial robes and receiving the crown by Christ , was originally in the demolished narthex together with another panel , the Theotokos with Georgios of Antiochia , the founder of the church .\n",
      "The Martorana church was likely decorated by the same greek masters who decorated which church ?\n",
      "the Cappella Palatina\n",
      "['68', '70']\n"
     ]
    }
   ],
   "source": [
    "print(val_cont[120])\n",
    "print(val_ques[120])\n",
    "print(val_ans[120])\n",
    "print(val_span[120].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QA_Dataset:\n",
    "  \n",
    "    def __init__(self, context, question, answer, span, word2id_dict=None, \n",
    "                 id2word_dict=None, max_vocab_size=None, autopad_context=None, \n",
    "                 autopad_ques = None, autopad_answer=None, \n",
    "                 context_pad_len=None, ques_pad_len = None, answer_pad_len=None):\n",
    "\n",
    "        assert autopad_context in {'min', 'max', 'avg'}\n",
    "        assert autopad_answer in {'min', 'max', 'avg'}\n",
    "        assert autopad_ques in {'min', 'max', 'avg'}\n",
    "        \n",
    "        self.context = context\n",
    "        self.question = question\n",
    "        self.answer = answer\n",
    "        self.span = span\n",
    "#         span_ids = [i.split() for i in span]\n",
    "#         t_span = []\n",
    "#         for s in span_ids:\n",
    "#             s = [int(c) for c in s]\n",
    "#             t_span.append(s)\n",
    "#         self.span = t_span\n",
    "        \n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.context_pad_len = context_pad_len\n",
    "        self.ques_pad_len = ques_pad_len\n",
    "        self.answer_pad_len = answer_pad_len\n",
    "        self.__autopad_context = autopad_context\n",
    "        self.__autopad_ques = autopad_ques\n",
    "        self.__autopad_answer = autopad_answer\n",
    "        \n",
    "        self.vocab = set()\n",
    "        self.word_counter = dict()\n",
    "        self.word2id_dict = word2id_dict\n",
    "        self.id2word_dict = id2word_dict\n",
    "        \n",
    "        self.context_ids = None\n",
    "        self.context_sentences_ids = [] \n",
    "        self.question_ids = None\n",
    "#        self.answer_ids = None\n",
    "#         self.answer_masks = []\n",
    "        self.answer_one_hot_labels = []\n",
    "\n",
    "#         self.context_text = None\n",
    "#         self.question_text = None\n",
    "#         self.answer_text = None\n",
    "        \n",
    "#         self.num_tokens = None\n",
    "#         self.max_sent_len_context = 0\n",
    "#         self.max_num_sentences = 0\n",
    "#         self.max_sent_len_question = 0\n",
    "        \n",
    "        self.__lower_words()\n",
    "        self.__parse_data()\n",
    "        self.__create_word2id_dict()\n",
    "        self.__numericize_data()\n",
    "\n",
    "    def __lower_words(self):\n",
    "        self.context = [t.lower() for t in self.context]\n",
    "        self.question = [t.lower() for t in self.question]\n",
    "        self.answer = [t.lower() for t in self.answer]\n",
    "    \n",
    "    def __update_word_counter(self, sequence):\n",
    "        \"\"\" Update word_counter with counts for words in a sentence\n",
    "        \n",
    "        Args:\n",
    "            sequence (list<str>) : list of words in a sequence\n",
    "        \n",
    "        \"\"\"\n",
    "        for word in sequence:\n",
    "            self.word_counter[word] = self.word_counter.get(word, 0) + 1\n",
    "            \n",
    "    def __create_vocab(self):\n",
    "        \"\"\" Create set of most frequent unique words found in the training data \"\"\"\n",
    "        \n",
    "        if self.max_vocab_size == None:\n",
    "            self.vocab = set(self.word_counter.keys())\n",
    "        else:\n",
    "            self.vocab = set(sorted(self.word_counter, key=self.word_counter.get, reverse=True)[:self.max_vocab_size])\n",
    "        \n",
    "    def __shuffle_data(self, data):\n",
    "        random.shuffle(data)\n",
    "        return list(zip(*data))\n",
    "    \n",
    "    def __parse_data(self):\n",
    "        \n",
    "        for idx in range(len(self.context)):\n",
    "            self.__update_word_counter(self.context[idx].split())\n",
    "            self.__update_word_counter(self.question[idx].split())\n",
    "            self.__update_word_counter(self.answer[idx].split())\n",
    "        \n",
    "        self.__create_vocab()\n",
    "        \n",
    "        shuffle = list(zip(self.context, self.question, self.answer, self.span))        \n",
    "        self.context, self.question, self.answer, self.span = self.__shuffle_data(shuffle)\n",
    "    \n",
    "    def __create_word2id_dict(self):\n",
    "        \n",
    "        if self.word2id_dict == None:\n",
    "            self.word2id_dict = dict()\n",
    "            self.id2word_dict = dict()\n",
    "            misc_tokens = ['PAD', 'UNK']\n",
    "\n",
    "            for i, token in enumerate(misc_tokens):\n",
    "                self.word2id_dict[token] = i\n",
    "\n",
    "            for word in self.vocab:\n",
    "                self.word2id_dict[word] = len(self.word2id_dict)\n",
    "\n",
    "            self.vocab |= set(misc_tokens)\n",
    "            self.id2word_dict = dict(zip(self.word2id_dict.values(), self.word2id_dict.keys()))\n",
    "            self.num_tokens = len(self.word2id_dict)\n",
    "    \n",
    "    def __convert_word2id(self, word):\n",
    "        try:\n",
    "            word_id = self.word2id_dict[word]\n",
    "        except:\n",
    "            word_id = self.word2id_dict['UNK']\n",
    "        return word_id\n",
    "    \n",
    "    def __apply_padding(self, s, pad_len):\n",
    "        sequence = s[:]\n",
    "        if len(sequence) < pad_len:\n",
    "            sequence += [self.word2id_dict['PAD'] for i in range(pad_len - len(sequence))]\n",
    "        elif len(sequence) > pad_len:\n",
    "            sequence = sequence[:pad_len]\n",
    "        else:\n",
    "            pass\n",
    "        return sequence\n",
    "        \n",
    "    def __get_seq_length_stats(self, sequences):\n",
    "        max_len = 0\n",
    "        min_len = 100000\n",
    "        avg_len = 0\n",
    "        for sequence in sequences:\n",
    "            max_len = max(max_len, len(sequence))\n",
    "            min_len = min(min_len, len(sequence))\n",
    "            avg_len += len(sequence)\n",
    "        avg_len = int(float(avg_len) / len(sequences))\n",
    "        return min_len, max_len, avg_len\n",
    "\n",
    "    def __get_max_sequence_lengths(self, context_ids, question_ids, answer_ids):\n",
    "        min_context_len, max_context_len, avg_context_len = self.__get_seq_length_stats(context_ids)\n",
    "        min_ques_len, max_ques_len, avg_ques_len = self.__get_seq_length_stats(question_ids)\n",
    "        min_answer_len, max_answer_len, avg_answer_len = self.__get_seq_length_stats(answer_ids)\n",
    "\n",
    "        if self.context_pad_len == None:\n",
    "            if self.__autopad_context != None:\n",
    "                if self.__autopad_context == 'min':\n",
    "                    self.context_pad_len = min_context_len\n",
    "                elif self.__autopad_context == 'max':\n",
    "                    self.context_pad_len = max_context_len\n",
    "                elif self.__autopad_context == 'avg':\n",
    "                    self.context_pad_len = avg_context_len\n",
    "            else:\n",
    "                self.context_pad_len = avg_context_len\n",
    "                \n",
    "        if self.ques_pad_len == None:\n",
    "            if self.__autopad_ques != None:\n",
    "                if self.__autopad_ques == 'min':\n",
    "                    self.ques_pad_len = min_ques_len\n",
    "                elif self.__autopad_ques == 'max':\n",
    "                    self.ques_pad_len = max_ques_len\n",
    "                elif self.__autopad_ques == 'avg':\n",
    "                    self.ques_pad_len = avg_ques_len\n",
    "            else:\n",
    "                self.ques_pad_len = avg_ques_len\n",
    "                \n",
    "        if self.answer_pad_len == None:\n",
    "            if self.__autopad_answer != None:\n",
    "                if self.__autopad_answer == 'min':\n",
    "                    self.answer_pad_len = min_answer_len\n",
    "                elif self.__autopad_answer == 'max':\n",
    "                    self.answer_pad_len = max_answer_len\n",
    "                elif self.__autopad_answer == 'avg':\n",
    "                    self.answer_pad_len = avg_answer_len\n",
    "            else:\n",
    "                self.answer_pad_len = avg_answer_len \n",
    "                \n",
    "        #print('Context length stats: min = {}, max = {}, avg = {}'.format(min_context_len, max_context_len, avg_context_len))\n",
    "        #print('Context pad length set to: {}'.format(self.context_pad_len))\n",
    "        \n",
    "        #print('Question length stats: min = {}, max = {}, avg = {}'.format(min_ques_len, max_ques_len, avg_ques_len))\n",
    "        #print('Question pad length set to: {}'.format(self.ques_pad_len))        \n",
    "        \n",
    "        #print('Answer length stats: min = {}, max = {}, avg = {}'.format(min_answer_len, max_answer_len, avg_answer_len))\n",
    "        #print('Answer pad length set to: {}'.format(self.answer_pad_len))        \n",
    "    def __convert_to_one_hot(self, span_ids, num_classes):\n",
    "        label = []\n",
    "        for idx in span_ids:\n",
    "            one_hot = [0 for i in range(num_classes)]\n",
    "            if idx >= num_classes - 1:\n",
    "                one_hot[-1] = 1\n",
    "            else:\n",
    "                one_hot[idx] = 1\n",
    "            label.append(one_hot)\n",
    "        return label\n",
    "    \n",
    "    def __convert_to_mask(self, i, a):\n",
    "        mask = (np.in1d(np.array(self.context_ids[i]), a[i]))*1\n",
    "        if len(a[i])>1:\n",
    "            count = 0\n",
    "            while count < len(mask):\n",
    "                if mask[count:count+len(a[i])].all():\n",
    "                    count+=len(a[i])\n",
    "                else:\n",
    "                    mask[count] = False\n",
    "                    count +=1\n",
    "        return mask\n",
    "        \n",
    "    def __tokenize_sentences(self, context_ids, question_ids, answer_ids):\n",
    "        \"\"\" Tokenizes sentences.\n",
    "        :param raw: dict returned from load_babi\n",
    "        :param word_table: WordTable\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        context_tokens = []\n",
    "        question_tokens = []\n",
    "        #answer_tokens = []\n",
    "        \n",
    "        for i in range(len(context_ids)):\n",
    "            c_tkn_ids = self.__apply_padding(context_ids[i], self.context_pad_len)\n",
    "            context_tokens.append(c_tkn_ids)\n",
    "            q_tkn_ids = self.__apply_padding(question_ids[i], self.ques_pad_len)\n",
    "            question_tokens.append(q_tkn_ids)\n",
    "            \n",
    "            span_ids = [int(x) for x in self.span[i].split()]\n",
    "            label = self.__convert_to_one_hot(span_ids, self.context_pad_len)\n",
    "            self.answer_one_hot_labels.append(label)\n",
    "            \n",
    "#             if i%100 == 0:\n",
    "#                 print(i)\n",
    "            #a_tkn_ids = self.__apply_padding(answer_ids[i], self.answer_pad_len)\n",
    "            #answer_tokens.append(a_tkn_ids)\n",
    "        return context_tokens, question_tokens#, answer_tokens\n",
    "        \n",
    "    def __convert_text2ids(self, context, question, answer):\n",
    "        \"\"\" Tokenizes sentences.\n",
    "        :param raw: dict returned from load_babi\n",
    "        :param word_table: WordTable\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        context_ids = []\n",
    "        question_ids = []\n",
    "        answer_ids = []\n",
    "        for i in range(len(context)):\n",
    "            c_ids = [self.__convert_word2id(word) for word in context[i].split()]\n",
    "            context_ids.append(c_ids)\n",
    "            q_ids = [self.__convert_word2id(word) for word in question[i].split()]\n",
    "            question_ids.append(q_ids)\n",
    "            a_ids = [self.__convert_word2id(word) for word in answer[i].split()]\n",
    "            answer_ids.append(a_ids)            \n",
    "        return context_ids, question_ids, answer_ids\n",
    "          \n",
    "    def __convert_text2words(self, context, question, answer):\n",
    "        context_words = []\n",
    "        question_words = []\n",
    "        answer_words = []\n",
    "        for i in range(len(context)):\n",
    "            c_words = context[i].split()\n",
    "            context_words.append(c_words)\n",
    "            q_words = question[i].split()\n",
    "            question_words.append(q_words)\n",
    "            a_words = answer[i].split()\n",
    "            answer_words.append(a_words)\n",
    "        return context_words, question_words, answer_words    \n",
    "\n",
    "    def __get_max_sentence_length(self, stories):\n",
    "        num_sentences = 0\n",
    "        sen_len = 0\n",
    "        for story in stories: \n",
    "            sentences = sent_tokenize(story)\n",
    "            num_sentences = max(num_sentences, len(sentences))\n",
    "            word_count = lambda sentence: len(word_tokenize(sentence))\n",
    "            sen_len = max(sen_len, len((max(sentences, key=word_count)).split()))\n",
    "        return sen_len, num_sentences\n",
    "        \n",
    "    def __context2sentences(self, story):\n",
    "        story_sentences = []\n",
    "        curr_sentence = []\n",
    "        for word_id in story:\n",
    "            if self.id2word_dict[word_id] == '.':\n",
    "                if len(curr_sentence) < self.max_sent_len_context:\n",
    "                    curr_sentence += [0 for i in range(self.max_sent_len_context - len(curr_sentence))]\n",
    "                elif len(curr_sentence) > self.max_sent_len_context:\n",
    "                    curr_sentence = curr_sentence[:self.max_sent_len_context]\n",
    "                story_sentences.append(curr_sentence)\n",
    "                curr_sentence = []\n",
    "            else:\n",
    "                curr_sentence.append(word_id)\n",
    "            \n",
    "        return story_sentences\n",
    "    \n",
    "    def __numericize_data(self):\n",
    "        c, q, a = self.__convert_text2ids(self.context, self.question, self.answer)\n",
    "        \n",
    "        self.__get_max_sequence_lengths(c, q, a)\n",
    "        \n",
    "#         self.context_ids, self.question_ids, self.answer_ids = self.__tokenize_sentences(c, q, a)\n",
    "        self.context_ids, self.question_ids = self.__tokenize_sentences(c, q, a)\n",
    "            \n",
    "#         self.context_text, self.question_text, self.answer_text = self.__convert_text2words(self.context,\n",
    "#                                                                                             self.question,\n",
    "#                                                                                             self.answer)\n",
    "        #self.max_sent_len_context, self.max_num_sentences = self.__get_max_sentence_length(self.context)\n",
    "        #print(\"max_sent_len_context: {} max_num_sentences: {}\".format(self.max_sent_len_context, self.max_num_sentences))\n",
    "        #self.max_sent_len_question, ques_num_sentences = self.__get_max_sentence_length(self.question)\n",
    "        #print(\"max_sent_len_ques: {} ques_num_sentences: {}\".format(self.max_sent_len_question, ques_num_sentences))\n",
    "        #for c_ids in c:\n",
    "            #self.context_sentences_ids.append(self.__context2sentences(c_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "UNKNOWN = \"*UNKNOWN*\"\n",
    "\n",
    "def generate_embedding_files(filename):\n",
    "    embeddings = {}\n",
    "    for line in open(filename):\n",
    "        parts = line.split()\n",
    "        embeddings[parts[0]] = list(map(float, parts[1:]))\n",
    "    embedding_size = len(list(embeddings.values())[0])\n",
    "    embeddings[UNKNOWN] = [0.0 for _ in range(embedding_size)]\n",
    "    \n",
    "    words = embeddings.keys()\n",
    "    embedding_matrix = np.array([embeddings[word] for word in list(embeddings.keys())])\n",
    "    return words, embedding_matrix\n",
    "\n",
    "glove_words, glove_embedding_matrix = generate_embedding_files('/media/ai2-rey/data_disk/data_sets/glove.6B/glove.6B.50d.txt')\n",
    "glove_vocab_lookup = {word: i for i, word  in enumerate(glove_words)}\n",
    "glove_vocab_size, glove_embedding_size= glove_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataIterator:  \n",
    "    def __init__(self, data, batch_size):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.data_iterator = self.make_random_iter()\n",
    "        \n",
    "    def next_batch(self):\n",
    "        try:\n",
    "            idxs = next(self.data_iterator)\n",
    "        except StopIteration:\n",
    "            self.data_iterator = self.make_random_iter()\n",
    "            idxs = next(self.data_iterator)\n",
    "            \n",
    "        batch = [self.data[i] for i in idxs]\n",
    "        batch_idxs = [idx for idx in idxs]\n",
    "        return batch, batch_idxs\n",
    "\n",
    "    def make_random_iter(self):\n",
    "        splits = np.arange(self.batch_size, len(self.data), self.batch_size)\n",
    "        it = np.split(np.random.permutation(range(len(self.data))), splits)[:-1]\n",
    "        return iter(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = QA_Dataset(train_cont, train_ques, train_ans, train_span,\n",
    "                   context_pad_len = 200, ques_pad_len = 20,\n",
    "                   autopad_context='max', autopad_ques = 'max', autopad_answer='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val = QA_Dataset(val_cont, val_ques, val_ans, val_span,\n",
    "                 train.word2id_dict, train.id2word_dict,\n",
    "                 context_pad_len=train.context_pad_len, \n",
    "                 ques_pad_len = train.ques_pad_len, \n",
    "                 answer_pad_len=train.answer_pad_len,\n",
    "                 autopad_context='max', autopad_ques='max', autopad_answer='max')\n",
    "\n",
    "test = QA_Dataset(test_cont, test_ques, test_ans, test_span,\n",
    "                  train.word2id_dict, train.id2word_dict,\n",
    "                  context_pad_len=train.context_pad_len, \n",
    "                  ques_pad_len = train.ques_pad_len,\n",
    "                  answer_pad_len=train.answer_pad_len,\n",
    "                  autopad_context='max', autopad_ques='max', autopad_answer='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_cont, train_ques, train_ans, train_span\n",
    "del val_cont, val_ques, val_ans, val_span\n",
    "del test_cont, test_ques, test_ans, test_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ids = list(zip(train.context_ids, train.question_ids, train.answer_one_hot_labels))\n",
    "val_ids = list(zip(val.context_ids,val.question_ids,val.answer_one_hot_labels))\n",
    "test_ids = list(zip(test.context_ids,test.question_ids, test.answer_one_hot_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_iter = DataIterator(train_ids,128)\n",
    "val_data_iter = DataIterator(val_ids,128)\n",
    "test_data_iter = DataIterator(test_ids,128)\n",
    "#deploy_data_iter = DataIterator(train_ids,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.ques_pad_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.context_pad_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_model_dir():\n",
    "    model_dir = os.getcwd() + '/'\n",
    "    if not os.path.exists(model_dir + 'weights'):\n",
    "        os.makedirs(model_dir + 'weights')\n",
    "    return model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_log(string,filename):\n",
    "    print(string)\n",
    "    with open(filename,'a') as write_file:\n",
    "        write_file.write(string + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_bounds_to_words(bounds, passage):\n",
    "    id_span = list(range(bounds[0], bounds[1]+1))\n",
    "    return ' '.join([train.id2word_dict[passage[idx]] for idx in id_span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    \n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exact_match_score(prediction, ground_truth):\n",
    "    return int(normalize_answer(prediction) == normalize_answer(ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "passage_max_len = train.context_pad_len\n",
    "question_max_len = train.ques_pad_len\n",
    "#output of lstm is n_steps or passage length by hidden size \n",
    "n_hidden = 150 \n",
    "num_indices = 2\n",
    "#batch_size = 128\n",
    "n_hidden_layers = 1\n",
    "learning_rate = 0.001\n",
    "blend_units = 6\n",
    "train_iters = 100000\n",
    "keep_prob = 0.5\n",
    "display_step = 200\n",
    "val_interval = 1000\n",
    "save_weights_interval = 1000\n",
    "word_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dir = prepare_model_dir()\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice:0\", shape=(200, 50), dtype=float32)\n",
      "Tensor(\"embedding_lookup:0\", shape=(?, 200, 50), dtype=float32)\n",
      "(<tf.Tensor 'passage_lstm/BiRNN/FW/FW/transpose:0' shape=(?, 200, 150) dtype=float32>, <tf.Tensor 'passage_lstm/ReverseSequence:0' shape=(?, 200, 150) dtype=float32>)\n",
      "Tensor(\"passage_lstm/concat:0\", shape=(?, 200, 300), dtype=float32)\n",
      "(<tf.Tensor 'question_lstm/BiRNN/FW/FW/transpose:0' shape=(?, 20, 150) dtype=float32>, <tf.Tensor 'question_lstm/ReverseSequence:0' shape=(?, 20, 150) dtype=float32>)\n",
      "Tensor(\"question_lstm/concat:0\", shape=(?, 20, 300), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "passage = tf.placeholder(tf.int32, [None, passage_max_len], name='passage')\n",
    "question = tf.placeholder(tf.int32,[None, question_max_len], name='question')\n",
    "y = tf.placeholder(tf.float32,[None,2,passage_max_len])\n",
    "\n",
    "dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "batch_size = tf.placeholder(tf.int32)\n",
    "p_seq_len = tf.placeholder(tf.int32)\n",
    "q_seq_len = tf.placeholder(tf.int32)\n",
    "\n",
    "embeddings = tf.constant(glove_embedding_matrix, name='embeddings', dtype=tf.float32)\n",
    "\n",
    "#preprocessing layer \n",
    "passage_embedded = tf.nn.embedding_lookup(embeddings, passage) #P = batch_size(b), word_dims(d), max_passage_len(P)\n",
    "question_embedded = tf.nn.embedding_lookup(embeddings, question)\n",
    "print(passage_embedded[0])\n",
    "print(passage_embedded)\n",
    "\n",
    "with tf.variable_scope('passage_lstm'):\n",
    "    passage_cell_fwd = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
    "    passage_cell_fwd = tf.nn.rnn_cell.DropoutWrapper(passage_cell_fwd, output_keep_prob=dropout_keep_prob)\n",
    "    passage_cell_fwd = tf.nn.rnn_cell.MultiRNNCell([passage_cell_fwd]*n_hidden_layers)\n",
    "    \n",
    "    passage_cell_bwd = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
    "    passage_cell_bwd = tf.nn.rnn_cell.DropoutWrapper(passage_cell_bwd, output_keep_prob=dropout_keep_prob)\n",
    "    passage_cell_bwd = tf.nn.rnn_cell.MultiRNNCell([passage_cell_bwd]*n_hidden_layers)\n",
    "    #H_p shape = batch_size, max_passage_len, n_hidden\n",
    "    \n",
    "    H_p, _ = tf.nn.bidirectional_dynamic_rnn(passage_cell_fwd, passage_cell_bwd, passage_embedded, p_seq_len,dtype=tf.float32)\n",
    "    print(H_p)\n",
    "    H_p = tf.concat(2,H_p) #(batch_size,passage_len,n_hidden)*2 in a tuple\n",
    "    print(H_p)\n",
    "    \n",
    "with tf.variable_scope('question_lstm'):\n",
    "    question_cell_fwd = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
    "    question_cell_fwd = tf.nn.rnn_cell.DropoutWrapper(question_cell_fwd, output_keep_prob=dropout_keep_prob)\n",
    "    question_cell_fwd = tf.nn.rnn_cell.MultiRNNCell([question_cell_fwd]*n_hidden_layers)\n",
    "    \n",
    "    question_cell_bwd = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
    "    question_cell_bwd = tf.nn.rnn_cell.DropoutWrapper(question_cell_bwd, output_keep_prob=dropout_keep_prob)\n",
    "    question_cell_bwd = tf.nn.rnn_cell.MultiRNNCell([question_cell_bwd]*n_hidden_layers)\n",
    "    \n",
    "    H_q, _ = tf.nn.bidirectional_dynamic_rnn(question_cell_fwd, question_cell_bwd, question_embedded, q_seq_len, dtype=tf.float32)\n",
    "    print(H_q)\n",
    "    H_q = tf.concat(2,H_q)\n",
    "    print(H_q)\n",
    "\n",
    "#Match LSTM layer \n",
    "#Weights and bias to compute G\n",
    "W_q = tf.Variable(tf.truncated_normal(shape=[2*n_hidden, n_hidden], stddev=0.1))\n",
    "W_p = tf.Variable(tf.truncated_normal(shape=[2*n_hidden, n_hidden], stddev=0.1))\n",
    "W_r = tf.Variable(tf.truncated_normal(shape=[n_hidden, n_hidden], stddev=0.1))\n",
    "b_p = tf.Variable(tf.constant(0.1, shape=[n_hidden]))\n",
    "\n",
    "#Weight and bias to compute a\n",
    "w= tf.Variable(tf.truncated_normal(shape=[n_hidden,1], stddev=0.1))#unsure \n",
    "#b_alpha shape = 1,none\n",
    "b_alpha = tf.Variable(tf.constant(0.1, shape = [1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul:0\", shape=(?, 150), dtype=float32)\n",
      "Tensor(\"Reshape_1:0\", shape=(?, 20, 150), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "H_q = tf.reshape(H_q, [-1, 2*n_hidden])\n",
    "WH_q = tf.matmul(H_q, W_q)\n",
    "print(WH_q)\n",
    "WH_q = tf.reshape(WH_q,[-1, question_max_len, n_hidden])\n",
    "print(WH_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "H_r_forward = []\n",
    "H_r_backward = []\n",
    "\n",
    "with tf.variable_scope('forward_match_lstm'):\n",
    "    forward_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(n_hidden), output_keep_prob=dropout_keep_prob)\n",
    "    forward_state = forward_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    h = forward_state.h #h shape =(batch_size, n_hidden)\n",
    "\n",
    "    for i in range(passage_max_len):\n",
    "        H_p_i =tf.squeeze(tf.slice(H_p, [0,i,0], [-1,1,-1]),1)\n",
    "        WH_p_i = tf.matmul(H_p_i, W_p)\n",
    "        \n",
    "        W_rh = tf.matmul(h,W_r)\n",
    "        \n",
    "        G_tile = tf.tile((WH_p_i + W_rh + b_p), [question_max_len, 1])\n",
    "        G_tile = tf.reshape(G_tile, [-1, question_max_len, n_hidden])\n",
    "        G_forward = tf.tanh(WH_q + G_tile)\n",
    "        \n",
    "        G_forward = tf.reshape(G_forward, [-1,n_hidden])\n",
    "        wG_forward = tf.matmul(G_forward,w)\n",
    "        a_tile = tf.tile(b_alpha, [question_max_len])\n",
    "        wG_forward = tf.reshape(tf.squeeze(wG_forward,1),[-1, question_max_len])\n",
    "        alpha_forward = tf.nn.softmax(wG_forward + a_tile)\n",
    "\n",
    "        H_q = tf.reshape(H_q,[-1, 2*n_hidden, question_max_len])\n",
    "        alpha_forward = tf.expand_dims(alpha_forward, 2)\n",
    "        H_q_alpha_forward = tf.batch_matmul(H_q, alpha_forward)\n",
    "        H_q_alpha_forward = tf.squeeze(H_q_alpha_forward, 2)\n",
    "        \n",
    "        z_forward = tf.concat(1, [H_p_i, H_q_alpha_forward])\n",
    "\n",
    "        h,forward_state = forward_cell(z_forward, forward_state)\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        H_r_forward.append(h)\n",
    "        if i%100 == 0:\n",
    "            print(i)\n",
    "\n",
    "    H_r_forward = tf.transpose(tf.stack(H_r_forward),[1,0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('backward_match_lstm'):\n",
    "    backward_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(n_hidden), output_keep_prob=dropout_keep_prob)\n",
    "    backward_state = backward_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    h = backward_state.h #h shape =(bathc_size, n_hidden)\n",
    "\n",
    "    for i in reversed(range(passage_max_len)):\n",
    "        H_p_i =tf.squeeze(tf.slice(H_p, [0,i,0], [-1,1,-1]),1)\n",
    "        WH_p_i = tf.matmul(H_p_i, W_p)\n",
    "        \n",
    "        W_rh = tf.matmul(h,W_r)\n",
    "        \n",
    "        G_tile = tf.tile((WH_p_i + W_rh + b_p), [question_max_len, 1])\n",
    "        G_tile = tf.reshape(G_tile, [-1, question_max_len, n_hidden])\n",
    "        G_backward = tf.tanh(WH_q + G_tile)\n",
    "        \n",
    "        G_backward = tf.reshape(G_backward, [-1,n_hidden])\n",
    "        wG_backward = tf.matmul(G_backward,w)\n",
    "        a_tile = tf.tile(b_alpha, [question_max_len])\n",
    "        wG_backward = tf.reshape(tf.squeeze(wG_backward,1),[-1, question_max_len])\n",
    "        alpha_backward = tf.nn.softmax(wG_backward + a_tile)\n",
    "\n",
    "        H_q = tf.reshape(H_q,[-1, 2*n_hidden, question_max_len])\n",
    "        alpha_backward = tf.expand_dims(alpha_backward, 2)\n",
    "        H_q_alpha_backward = tf.batch_matmul(H_q, alpha_backward)\n",
    "        H_q_alpha_backward = tf.squeeze(H_q_alpha_backward, 2)\n",
    "        \n",
    "        z_backward = tf.concat(1, [H_p_i, H_q_alpha_backward])\n",
    "\n",
    "        h,backward_state = backward_cell(z_backward, backward_state)\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        H_r_backward.append(h)\n",
    "        if i%100 == 0:\n",
    "            print(i)\n",
    "            \n",
    "    H_r_backward = list(reversed(H_r_backward))\n",
    "    H_r_backward = tf.transpose(tf.stack(H_r_backward),[1,0,2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concat:0\", shape=(?, 200, 300), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "H_r = tf.concat(2, [H_r_forward, H_r_backward])\n",
    "print(H_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# H_r = H_r_forward\n",
    "# H_r = tf.reshape(H_r,[-1,n_hidden])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Variable_6/read:0\", shape=(300, 150), dtype=float32)\n",
      "Tensor(\"Reshape_2:0\", shape=(?, 300), dtype=float32)\n",
      "0\n",
      "1\n",
      "Tensor(\"concat_1:0\", shape=(?, 2, 200), dtype=float32)\n",
      "Tensor(\"Squeeze:0\", shape=(?, 200), dtype=float32)\n",
      "Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Tensor(\"global_norm/global_norm:0\", shape=(), dtype=float32)\n",
      "name: \"Adam\"\n",
      "op: \"NoOp\"\n",
      "input: \"^Adam/update_passage_lstm/BiRNN/FW/MultiRNNCell/Cell0/LSTMCell/W_0/ApplyAdam\"\n",
      "input: \"^Adam/update_passage_lstm/BiRNN/FW/MultiRNNCell/Cell0/LSTMCell/B/ApplyAdam\"\n",
      "input: \"^Adam/update_passage_lstm/BiRNN/BW/MultiRNNCell/Cell0/LSTMCell/W_0/ApplyAdam\"\n",
      "input: \"^Adam/update_passage_lstm/BiRNN/BW/MultiRNNCell/Cell0/LSTMCell/B/ApplyAdam\"\n",
      "input: \"^Adam/update_question_lstm/BiRNN/FW/MultiRNNCell/Cell0/LSTMCell/W_0/ApplyAdam\"\n",
      "input: \"^Adam/update_question_lstm/BiRNN/FW/MultiRNNCell/Cell0/LSTMCell/B/ApplyAdam\"\n",
      "input: \"^Adam/update_question_lstm/BiRNN/BW/MultiRNNCell/Cell0/LSTMCell/W_0/ApplyAdam\"\n",
      "input: \"^Adam/update_question_lstm/BiRNN/BW/MultiRNNCell/Cell0/LSTMCell/B/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_1/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_2/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_3/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_4/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_5/ApplyAdam\"\n",
      "input: \"^Adam/update_forward_match_lstm/LSTMCell/W_0/ApplyAdam\"\n",
      "input: \"^Adam/update_forward_match_lstm/LSTMCell/B/ApplyAdam\"\n",
      "input: \"^Adam/update_backward_match_lstm/LSTMCell/W_0/ApplyAdam\"\n",
      "input: \"^Adam/update_backward_match_lstm/LSTMCell/B/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_6/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_7/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_8/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_9/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_10/ApplyAdam\"\n",
      "input: \"^Adam/update_answer_pointer_lstm/LSTMCell/W_0/ApplyAdam\"\n",
      "input: \"^Adam/update_answer_pointer_lstm/LSTMCell/B/ApplyAdam\"\n",
      "input: \"^Adam/Assign\"\n",
      "input: \"^Adam/Assign_1\"\n",
      "\n",
      "Tensor(\"All:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Mean_3:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Answer Pointer Layer \n",
    "\n",
    "#Weights and bias to compute 'F'\n",
    "V = tf.Variable(tf.truncated_normal(shape=[2*n_hidden, n_hidden], stddev=0.1))\n",
    "print(V)\n",
    "W_a = tf.Variable(tf.truncated_normal(shape=[n_hidden, n_hidden], stddev=0.1))\n",
    "b_a = tf.Variable(tf.constant(0.1, shape=[n_hidden]))\n",
    "\n",
    "#weight and bias to computer 'beta'\n",
    "v = tf.Variable(tf.truncated_normal(shape=[n_hidden,1], stddev=0.1))\n",
    "b_beta = tf.Variable(tf.constant(0.1, shape=[1]))\n",
    "\n",
    "#Only calculate 'VH' once \n",
    "H_r = tf.reshape(H_r, [-1,2*n_hidden]) #shape = (2*n_hidden, batch&passage_max_len)\n",
    "print(H_r)\n",
    "VH = tf.matmul(H_r,V) #shape (hidden_size, passage_max_len)\n",
    "VH = tf.reshape(VH,[-1, passage_max_len, n_hidden])\n",
    "\n",
    "output_logits = []\n",
    "output_probs = []\n",
    "\n",
    "with tf.variable_scope('answer_pointer_lstm'):\n",
    "    pointer_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(n_hidden), output_keep_prob=dropout_keep_prob)\n",
    "    pointer_state = pointer_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    h = pointer_state.h\n",
    "    \n",
    "    for k in range(2):\n",
    "        W_ah = tf.matmul(h, W_a)\n",
    "        \n",
    "        F_tile = tf.tile(tf.expand_dims(tf.add(W_ah,b_a),1),[1,passage_max_len,1])\n",
    "        F = tf.tanh(tf.add(VH, F_tile))\n",
    "        F = tf.reshape(F, [-1,n_hidden])   \n",
    "        vF = tf.matmul(F,v)\n",
    "        beta_tile = tf.tile(b_beta, [passage_max_len])\n",
    "        vF = tf.reshape(tf.squeeze(vF, 1), [-1,passage_max_len])\n",
    "        beta_logits = tf.add(vF,beta_tile)\n",
    "        beta = tf.nn.softmax(beta_logits)\n",
    "        output_logits.append(beta_logits)\n",
    "        output_probs.append(beta)\n",
    "\n",
    "\n",
    "    #    beta = tf.transpose(beta,[1,0])\n",
    "        beta = tf.expand_dims(beta,2) #unsure and changed \n",
    "        H_r = tf.reshape(H_r, [-1, 2*n_hidden, passage_max_len])\n",
    "        H_r_beta = tf.batch_matmul(H_r, beta)\n",
    "        H_r_beta = tf.squeeze(H_r_beta,2)\n",
    "        h,pointer_state = pointer_cell(H_r_beta, pointer_state)\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        print(k)\n",
    "\n",
    "probs_start = output_probs[0]\n",
    "probs_end = output_probs[1]\n",
    "probs = tf.concat(1,[tf.expand_dims(probs_start,1), tf.expand_dims(probs_end,1)])\n",
    "print(probs)\n",
    "\n",
    "start_label, end_label = tf.split(1,2,y)\n",
    "start_label = tf.squeeze(start_label,1)\n",
    "end_label = tf.squeeze(end_label,1)\n",
    "print(start_label)\n",
    "\n",
    "combined_prob = tf.batch_matmul(tf.batch_matmul(tf.expand_dims(probs_start, 1), tf.expand_dims(start_label, 2)), \n",
    "                                tf.batch_matmul(tf.expand_dims(probs_end, 1), tf.expand_dims(end_label, 2)))\n",
    "loss = -tf.reduce_mean(tf.log(combined_prob))\n",
    "print(loss)\n",
    "\n",
    "params = tf.trainable_variables()\n",
    "gradients = tf.gradients(loss, params)\n",
    "gradient_norm = tf.global_norm(gradients)\n",
    "print(gradient_norm)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "print(optimizer)\n",
    "\n",
    "correct_start = tf.equal(tf.argmax(probs_start, axis=1), tf.argmax(start_label, axis=1))\n",
    "correct_end = tf.equal(tf.argmax(probs_end, axis=1), tf.argmax(end_label, axis=1))\n",
    "equal_vals = tf.equal(tf.stack([tf.argmax(probs_start, axis=1), tf.argmax(probs_end, axis=1)], axis=1),\n",
    "                      tf.stack([tf.argmax(start_label, axis=1), tf.argmax(end_label, axis=1)], axis=1))\n",
    "correct_prediction = tf.reduce_all(equal_vals, axis=1)\n",
    "print(correct_prediction)\n",
    "\n",
    "start_accuracy = tf.reduce_mean(tf.cast(correct_start, tf.float32))\n",
    "end_accuracy = tf.reduce_mean(tf.cast(correct_end, tf.float32))\n",
    "mean_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(mean_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(sess, feed_dict, passage, labels):\n",
    "    model_pred = sess.run(probs, feed_dict= feed_dict)\n",
    "    em_total = 0\n",
    "    \n",
    "    for i in range(len(model_pred)):\n",
    "        label_ids = [np.argmax(x) for x in labels[i]]\n",
    "        pred_ids = [np.argmax(x) for x in model_pred[i]]\n",
    "        \n",
    "        label_string = convert_bounds_to_words(label_ids, passage[i])\n",
    "        pred_string = convert_bounds_to_words(pred_ids, passage[i])\n",
    "        em_total += exact_match_score(pred_string, label_string)\n",
    "        if i <5:\n",
    "            print(i)\n",
    "            print(label_string)\n",
    "            print(i)\n",
    "            print(pred_string)\n",
    "    return em_total / float(len(model_pred)), pred_string, label_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_file = model_dir + 'Pnter_ntwrk_log.txt'\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "log = open(log_file,'w')\n",
    "log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1, start_accuracy = 0.0000, end_accuracy = 0.0000, accuracy = 0.0000, train loss = 10.414585\n",
      "0\n",
      "over three thousand\n",
      "0\n",
      "by\n",
      "1\n",
      "western europe\n",
      "1\n",
      "satellite\n",
      "2\n",
      "the state security administration\n",
      "2\n",
      ".\n",
      "3\n",
      "interfere with commuter and troop travel\n",
      "3\n",
      "world\n",
      "4\n",
      "djibouti\n",
      "4\n",
      "10,000\n",
      "Iter 1, start_accuracy = 0.0000, end_accuracy = 0.0000, accuracy = 0.0000, train loss = 10.414585\n",
      "Exact Match Score: 0.0000\n",
      "Iter 200, start_accuracy = 0.0312, end_accuracy = 0.0469, accuracy = 0.0234, train loss = 8.852318\n",
      "0\n",
      "scholarly tradition of the greek academies was maintained during roman times with several academic institutions\n",
      "0\n",
      "paideia\n",
      "1\n",
      "shenzhen stock exchange\n",
      "1\n",
      "china\n",
      "2\n",
      "separate from ( but in contact with ) each other\n",
      "2\n",
      "from\n",
      "3\n",
      "medieval to modern\n",
      "3\n",
      "1808\n",
      "4\n",
      "dame lois browne-evans , and her trinidadian-born husband , john evans\n",
      "4\n",
      "made\n",
      "Iter 200, start_accuracy = 0.0312, end_accuracy = 0.0469, accuracy = 0.0234, train loss = 8.852318\n",
      "Exact Match Score: 0.0234\n",
      "Iter 400, start_accuracy = 0.1016, end_accuracy = 0.1094, accuracy = 0.0547, train loss = 7.926003\n",
      "0\n",
      "obscurity and mystery\n",
      "0\n",
      "landed property\n",
      "1\n",
      "1875\n",
      "1\n",
      "1875\n",
      "2\n",
      "ifl\n",
      "2\n",
      "12 teams\n",
      "3\n",
      "archulus persons\n",
      "3\n",
      "first edition , \" someone rare has written this very fine first novel : a writer with the liveliest sense of life , and the warmest , most authentic sense of humor\n",
      "4\n",
      "von neumann 's way of thinking might not be visual , but more of an aural one .\n",
      "4\n",
      "absolute recall . as far as i could tell , von neumann was able on once reading a book or article to quote it back verbatim ; moreover , he could do it years later without hesitation . he could also translate it at no diminution in speed from its original language into english . on one occasion i tested his ability by asking him to tell me how a tale of two cities started . whereupon , without any pause\n",
      "Iter 400, start_accuracy = 0.1016, end_accuracy = 0.1094, accuracy = 0.0547, train loss = 7.926003\n",
      "Exact Match Score: 0.0625\n",
      "Iter 600, start_accuracy = 0.1016, end_accuracy = 0.0859, accuracy = 0.0469, train loss = 8.114017\n",
      "0\n",
      "saltillo\n",
      "0\n",
      "president juárez was forced to evacuate the city of saltillo and relocate to chihuahua . juárez stopped in ciudad jiménez , valle de allende , and hidalgo de parral , in turn . he decreed parral the capital of mexico from october 2–5 , 1864 . perceiving the threat from the advancing french forces , the president continued his evacuation through santa rosalía de camargo , santa cruz de rosales , and finally chihuahua , chihuahua . on october 12 , 1864 , the people of the state gave president juárez an overwhelmingly supportive reception , led by governor ángel trías . on october 15\n",
      "1\n",
      "fachhochschule\n",
      "1\n",
      "austria and switzerland . they do not focus exclusively on technology , but may also offer courses in social science , medicine , business and design\n",
      "2\n",
      "relations have thawed since the reforms following the 2010 elections .\n",
      "2\n",
      "western nations\n",
      "3\n",
      "he\n",
      "3\n",
      "he\n",
      "4\n",
      "smith\n",
      "4\n",
      "\n",
      "Iter 600, start_accuracy = 0.1016, end_accuracy = 0.0859, accuracy = 0.0469, train loss = 8.114017\n",
      "Exact Match Score: 0.0469\n",
      "Iter 800, start_accuracy = 0.1016, end_accuracy = 0.0938, accuracy = 0.0625, train loss = 7.998895\n",
      "0\n",
      "japan\n",
      "0\n",
      "\n",
      "1\n",
      "rotational and translational\n",
      "1\n",
      "glass transition and a melting transition of comparable materials are typically of the same order of magnitude\n",
      "2\n",
      "softwood\n",
      "2\n",
      "pine ) is called softwood , and the wood from dicotyledons ( usually broad-leaved trees , e.g . oak ) is called hardwood\n",
      "3\n",
      "humanities , social , and natural sciences\n",
      "3\n",
      "natural sciences\n",
      "4\n",
      "the face or neck .\n",
      "4\n",
      "\n",
      "Iter 800, start_accuracy = 0.1016, end_accuracy = 0.0938, accuracy = 0.0625, train loss = 7.998895\n",
      "Exact Match Score: 0.0703\n",
      "Iter 1000, start_accuracy = 0.1172, end_accuracy = 0.1719, accuracy = 0.0938, train loss = 7.324611\n",
      "0\n",
      "ghon focus\n",
      "0\n",
      "ghon focus\n",
      "1\n",
      "1926\n",
      "1\n",
      "1926\n",
      "2\n",
      "blue ivy carter\n",
      "2\n",
      "two days later , jay z released \" glory \" , a song dedicated to their child , on his website lifeandtimes.com . the song detailed the couple 's pregnancy struggles\n",
      "3\n",
      "at the moment of its occurrence\n",
      "3\n",
      "social norms and roles . instead , the emotional episode is assembled at the moment of its occurrence to suit its specific circumstances\n",
      "4\n",
      "1979\n",
      "4\n",
      "1979\n",
      "Iter 1000, start_accuracy = 0.1172, end_accuracy = 0.1719, accuracy = 0.0938, train loss = 7.324611\n",
      "Exact Match Score: 0.1016\n",
      "Iter 1000, val_start_accuracy = 0.1406, val_end_acc = 0.1016, val_acc = 0.0469, val loss = 7.928077\n",
      "0\n",
      "late 18th and early 19th centuries\n",
      "0\n",
      "different situations\n",
      "1\n",
      "1868\n",
      "1\n",
      "1868 , at which point the first amendment 's application to the state government was recognized . many of these constitutional debates relate to the competing interpretive theories of UNK versus modern , progressivist theories such as the doctrine of the living constitution . other debates center on the principle of the law of the land in america\n",
      "2\n",
      "eisenhower\n",
      "2\n",
      "arms policy became one aimed primarily at arms control as opposed to disarmament\n",
      "3\n",
      "buddhists\n",
      "3\n",
      "hindus and buddhists , and many hindu temples are located on the banks of this river . the importance of the UNK also lies in the fact that hindus are cremated on its banks , and UNK are buried in the hills by its side\n",
      "4\n",
      "20th-century\n",
      "4\n",
      "before anything like the modern two-party congressional system emerged on capitol hill with official titles for those who were its official leaders . however , from the beginning days of congress\n",
      "Iter 1000, val_start_accuracy = 0.1406, val_end_acc = 0.1016, val_acc = 0.0469, val loss = 7.928077\n",
      "Exact Match Score: 0.0547\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--1000.ckpt\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--1000.ckpt\n",
      "Iter 1200, start_accuracy = 0.1250, end_accuracy = 0.1172, accuracy = 0.0547, train loss = 7.290532\n",
      "0\n",
      "the waxwings bombycilla\n",
      "0\n",
      "waxwings bombycilla\n",
      "1\n",
      "the holy roman empire\n",
      "1\n",
      "\n",
      "2\n",
      "of\n",
      "2\n",
      "of\n",
      "3\n",
      "1787\n",
      "3\n",
      "british america\n",
      "4\n",
      "the\n",
      "4\n",
      "the\n",
      "Iter 1200, start_accuracy = 0.1250, end_accuracy = 0.1172, accuracy = 0.0547, train loss = 7.290532\n",
      "Exact Match Score: 0.0703\n",
      "Iter 1400, start_accuracy = 0.1328, end_accuracy = 0.1875, accuracy = 0.0547, train loss = 6.732758\n",
      "0\n",
      "14\n",
      "0\n",
      "\n",
      "1\n",
      "63 german fighters were sent with the bombers\n",
      "1\n",
      "may 1941\n",
      "2\n",
      "28 , 32 , and 33\n",
      "2\n",
      "royal assent is the final stage in the legislative process for acts of the scottish parliament . the process is governed by sections 28 , 32\n",
      "3\n",
      "overuse , especially in livestock raising , prompting bacteria to develop resistance\n",
      "3\n",
      "vaccination led to the near eradication of diseases such as tuberculosis in the developed world . their effectiveness and easy access led to overuse , especially in livestock raising , prompting bacteria to develop resistance . this has led to widespread problems with antimicrobial and antibiotic resistance\n",
      "4\n",
      "primary\n",
      "4\n",
      "motor and sensory areas . the areas of the brain involved in more complex processes lose matter later in development . these include the lateral and prefrontal cortices , among other regions\n",
      "Iter 1400, start_accuracy = 0.1328, end_accuracy = 0.1875, accuracy = 0.0547, train loss = 6.732758\n",
      "Exact Match Score: 0.0547\n",
      "Iter 1600, start_accuracy = 0.1172, end_accuracy = 0.1562, accuracy = 0.0781, train loss = 7.651778\n",
      "0\n",
      "inconclusive\n",
      "0\n",
      "heart attack\n",
      "1\n",
      "cubist architecture is very rare\n",
      "1\n",
      "\n",
      "2\n",
      "define it empirically\n",
      "2\n",
      "treating\n",
      "3\n",
      "one-quarter\n",
      "3\n",
      "two\n",
      "4\n",
      "8-hour\n",
      "4\n",
      "\n",
      "Iter 1600, start_accuracy = 0.1172, end_accuracy = 0.1562, accuracy = 0.0781, train loss = 7.651778\n",
      "Exact Match Score: 0.0938\n",
      "Iter 1800, start_accuracy = 0.1953, end_accuracy = 0.2578, accuracy = 0.1641, train loss = 6.622293\n",
      "0\n",
      "those of ( non-hispanic white ) european ancestry accounted for 57.9 % of florida 's population\n",
      "0\n",
      "57.9 %\n",
      "1\n",
      "database engine\n",
      "1\n",
      "internal\n",
      "2\n",
      "doctoral\n",
      "2\n",
      "ernest l.\n",
      "3\n",
      "introduced financial support , over three years , for the teaching of regional languages in schools and universities\n",
      "3\n",
      "\n",
      "4\n",
      "cold war , first gulf war , kosovo war\n",
      "4\n",
      "\n",
      "Iter 1800, start_accuracy = 0.1953, end_accuracy = 0.2578, accuracy = 0.1641, train loss = 6.622293\n",
      "Exact Match Score: 0.1797\n",
      "Iter 2000, start_accuracy = 0.1406, end_accuracy = 0.1797, accuracy = 0.1094, train loss = 7.176239\n",
      "0\n",
      "backup and restore\n",
      "0\n",
      "windows 7 file recovery\n",
      "1\n",
      "father john francis o'hara\n",
      "1\n",
      "john francis o'hara\n",
      "2\n",
      "directional characteristics\n",
      "2\n",
      "directional characteristics\n",
      "3\n",
      "clade crocodilia\n",
      "3\n",
      "clade crocodilia\n",
      "4\n",
      "burke\n",
      "4\n",
      "fox whispered\n",
      "Iter 2000, start_accuracy = 0.1406, end_accuracy = 0.1797, accuracy = 0.1094, train loss = 7.176239\n",
      "Exact Match Score: 0.1406\n",
      "Iter 2000, val_start_accuracy = 0.1406, val_end_acc = 0.1406, val_acc = 0.0938, val loss = 7.312455\n",
      "0\n",
      "to the copyright holders\n",
      "0\n",
      "\n",
      "1\n",
      "imperial cult\n",
      "1\n",
      "julius caesar\n",
      "2\n",
      "soviet union\n",
      "2\n",
      "1960 u-2 incident and the associated international embarrassment\n",
      "3\n",
      "UNK bc\n",
      "3\n",
      "bronze age\n",
      "4\n",
      "the supreme court 's interpretations of the \" separation of church and state \" doctrine .\n",
      "4\n",
      "congress shall make no law respecting an establishment of religion , or prohibiting the free exercise thereof \" the two parts\n",
      "Iter 2000, val_start_accuracy = 0.1406, val_end_acc = 0.1406, val_acc = 0.0938, val loss = 7.312455\n",
      "Exact Match Score: 0.1172\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--2000.ckpt\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--2000.ckpt\n",
      "Iter 2200, start_accuracy = 0.1641, end_accuracy = 0.2031, accuracy = 0.1094, train loss = 6.798230\n",
      "0\n",
      "molotov\n",
      "0\n",
      "james dunn\n",
      "1\n",
      "the modern kristang creoles of malaysia\n",
      "1\n",
      "modern kristang creoles of malaysia\n",
      "2\n",
      "prince andrew\n",
      "2\n",
      "prince andrew\n",
      "3\n",
      "the unborn and the elderly\n",
      "3\n",
      "terris\n",
      "4\n",
      "conservative party\n",
      "4\n",
      "conservative party\n",
      "Iter 2200, start_accuracy = 0.1641, end_accuracy = 0.2031, accuracy = 0.1094, train loss = 6.798230\n",
      "Exact Match Score: 0.1250\n",
      "Iter 2400, start_accuracy = 0.2031, end_accuracy = 0.2656, accuracy = 0.1719, train loss = 6.503248\n",
      "0\n",
      "third most abundant\n",
      "0\n",
      "\n",
      "1\n",
      "1920s\n",
      "1\n",
      "1920s\n",
      "2\n",
      "france\n",
      "2\n",
      "dartford 's library\n",
      "3\n",
      "olive baboons\n",
      "3\n",
      "olive baboons\n",
      "4\n",
      "rom\n",
      "4\n",
      "ram\n",
      "Iter 2400, start_accuracy = 0.2031, end_accuracy = 0.2656, accuracy = 0.1719, train loss = 6.503248\n",
      "Exact Match Score: 0.1797\n",
      "Iter 2600, start_accuracy = 0.1641, end_accuracy = 0.1562, accuracy = 0.1094, train loss = 6.912565\n",
      "0\n",
      "his conquest of tibet\n",
      "0\n",
      "structures\n",
      "1\n",
      "thomas hobbes\n",
      "1\n",
      "john locke , one of the most influential enlightenment thinkers , based his governance philosophy in social contract theory , a subject that permeated enlightenment political thought . the english philosopher thomas hobbes ushered\n",
      "2\n",
      "mexican\n",
      "2\n",
      "mexican descent as white\n",
      "3\n",
      "winter\n",
      "3\n",
      "manhattan\n",
      "4\n",
      "belgium\n",
      "4\n",
      "\n",
      "Iter 2600, start_accuracy = 0.1641, end_accuracy = 0.1562, accuracy = 0.1094, train loss = 6.912565\n",
      "Exact Match Score: 0.1172\n",
      "Iter 2800, start_accuracy = 0.1562, end_accuracy = 0.2031, accuracy = 0.1094, train loss = 6.736041\n",
      "0\n",
      "shirley williams\n",
      "0\n",
      "\n",
      "1\n",
      "175 bc\n",
      "1\n",
      "bc\n",
      "2\n",
      "lan-nang or\n",
      "2\n",
      "\n",
      "3\n",
      "greatly extended the area of the country\n",
      "3\n",
      "king \" . mswati ii was the greatest of the fighting kings of swaziland , and he greatly extended the area of the country to twice its current size\n",
      "4\n",
      "carolingian empire\n",
      "4\n",
      "alemanni , huns , and franks\n",
      "Iter 2800, start_accuracy = 0.1562, end_accuracy = 0.2031, accuracy = 0.1094, train loss = 6.736041\n",
      "Exact Match Score: 0.1328\n",
      "Iter 3000, start_accuracy = 0.2109, end_accuracy = 0.2109, accuracy = 0.1172, train loss = 6.530580\n",
      "0\n",
      "14th\n",
      "0\n",
      "14th\n",
      "1\n",
      "the reagan administration\n",
      "1\n",
      "reagan administration\n",
      "2\n",
      "president george w. bush\n",
      "2\n",
      "george w. bush\n",
      "3\n",
      "us national institutes of health\n",
      "3\n",
      "scientists first\n",
      "4\n",
      "the zune\n",
      "4\n",
      "zune video marketplace and the xbox live video\n",
      "Iter 3000, start_accuracy = 0.2109, end_accuracy = 0.2109, accuracy = 0.1172, train loss = 6.530580\n",
      "Exact Match Score: 0.1719\n",
      "Iter 3000, val_start_accuracy = 0.2031, val_end_acc = 0.2266, val_acc = 0.1562, val loss = 6.869473\n",
      "0\n",
      "coptic orthodox church\n",
      "0\n",
      "coptic catholic church , the evangelical church of egypt and various other protestant denominations . non-native christian communities are largely found in the urban regions of cairo and alexandria , such as the UNK\n",
      "1\n",
      "UNK\n",
      "1\n",
      "educating parents\n",
      "2\n",
      "minnesota\n",
      "2\n",
      "san pedro\n",
      "3\n",
      "1965\n",
      "3\n",
      "\n",
      "4\n",
      "december 27 , 1657\n",
      "4\n",
      "mid-17th century\n",
      "Iter 3000, val_start_accuracy = 0.2031, val_end_acc = 0.2266, val_acc = 0.1562, val loss = 6.869473\n",
      "Exact Match Score: 0.1719\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--3000.ckpt\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--3000.ckpt\n",
      "Iter 3200, start_accuracy = 0.2344, end_accuracy = 0.2656, accuracy = 0.1484, train loss = 5.955684\n",
      "0\n",
      "often ranks among the lowest in the cable industry\n",
      "0\n",
      "\n",
      "1\n",
      "naples\n",
      "1\n",
      "naples\n",
      "2\n",
      "webcam\n",
      "2\n",
      "consequently , personal videoconferencing systems based on a webcam , personal computer system , software compression and broadband internet connectivity have become affordable to the general public . also , the hardware used for this technology has continued to improve in quality , and prices have dropped dramatically . the availability of freeware\n",
      "3\n",
      "national sanitation foundation international\n",
      "3\n",
      "ann arbor\n",
      "4\n",
      "specialist stores\n",
      "4\n",
      "larger-scale superstores\n",
      "Iter 3200, start_accuracy = 0.2344, end_accuracy = 0.2656, accuracy = 0.1484, train loss = 5.955684\n",
      "Exact Match Score: 0.1875\n",
      "Iter 3400, start_accuracy = 0.2656, end_accuracy = 0.2969, accuracy = 0.1875, train loss = 5.893443\n",
      "0\n",
      "all of his papers be destroyed after his death .\n",
      "0\n",
      "almost fanatical belief\n",
      "1\n",
      "mikhail gorbachev\n",
      "1\n",
      "russia yeltsin strongly\n",
      "2\n",
      "$ 3 million\n",
      "2\n",
      "80 % or more of the cherokee people will be fluent in the language . the cherokee preservation foundation has invested $ 3 million\n",
      "3\n",
      "hawaii\n",
      "3\n",
      "always one hour\n",
      "4\n",
      "coloured according to the distinctive uniform worn\n",
      "4\n",
      "any\n",
      "Iter 3400, start_accuracy = 0.2656, end_accuracy = 0.2969, accuracy = 0.1875, train loss = 5.893443\n",
      "Exact Match Score: 0.2188\n",
      "Iter 3600, start_accuracy = 0.2969, end_accuracy = 0.3203, accuracy = 0.2266, train loss = 5.840979\n",
      "0\n",
      "localizing\n",
      "0\n",
      "\n",
      "1\n",
      "12th century\n",
      "1\n",
      "1295 , roman catholic cardinals began to wear red costumes . when abbe suger rebuilt saint denis basilica outside paris in the early 12th century\n",
      "2\n",
      "thick-walled\n",
      "2\n",
      "good\n",
      "3\n",
      "all of the indian ocean\n",
      "3\n",
      "\n",
      "4\n",
      "35\n",
      "4\n",
      "35\n",
      "Iter 3600, start_accuracy = 0.2969, end_accuracy = 0.3203, accuracy = 0.2266, train loss = 5.840979\n",
      "Exact Match Score: 0.2266\n",
      "Iter 3800, start_accuracy = 0.2734, end_accuracy = 0.2891, accuracy = 0.1953, train loss = 6.148370\n",
      "0\n",
      "lord goring\n",
      "0\n",
      "lord goring\n",
      "1\n",
      "13,000\n",
      "1\n",
      "1,200\n",
      "2\n",
      "stalin\n",
      "2\n",
      "stalin and tito\n",
      "3\n",
      "92 offices\n",
      "3\n",
      "19\n",
      "4\n",
      "zorba the greek\n",
      "4\n",
      "anthony quinn which received best director , best adapted screenplay and best film nominations\n",
      "Iter 3800, start_accuracy = 0.2734, end_accuracy = 0.2891, accuracy = 0.1953, train loss = 6.148370\n",
      "Exact Match Score: 0.2109\n",
      "Iter 4000, start_accuracy = 0.3203, end_accuracy = 0.3984, accuracy = 0.2578, train loss = 5.523516\n",
      "0\n",
      "430,332\n",
      "0\n",
      "5.5 million\n",
      "1\n",
      "laminate\n",
      "1\n",
      "wood fibers\n",
      "2\n",
      "200 million\n",
      "2\n",
      "rca victor and columbia records\n",
      "3\n",
      "cherokee nation\n",
      "3\n",
      "the cherokee nation\n",
      "4\n",
      "lexical\n",
      "4\n",
      "unique lexical\n",
      "Iter 4000, start_accuracy = 0.3203, end_accuracy = 0.3984, accuracy = 0.2578, train loss = 5.523516\n",
      "Exact Match Score: 0.3203\n",
      "Iter 4000, val_start_accuracy = 0.2422, val_end_acc = 0.2891, val_acc = 0.1797, val loss = 6.079658\n",
      "0\n",
      "northwestern\n",
      "0\n",
      "UNK UNK city ( UNK ) with which it today forms one urban area surrounded by a ring road , to the southwest by UNK municipality and to the east by UNK UNK municipality\n",
      "1\n",
      "the cathedral of serres\n",
      "1\n",
      "hagia sophia depicting emperor john ii and empress UNK with the theotokos ( UNK ) . the empress with her long UNK hair and UNK cheeks is especially capturing . it must be a lifelike portrayal because UNK was really a redhead as her original hungarian name , UNK shows . the adjacent portrait of emperor alexios i komnenos on a pier ( from 1122 ) is similarly personal . the imperial mausoleum of the komnenos dynasty , the UNK monastery was certainly decorated with great mosaics but these were later destroyed . the lack of UNK mosaics outside the capital is even more apparent . there is only a \" communion of the apostles \" in the apse of the cathedral of serres\n",
      "2\n",
      "1759\n",
      "2\n",
      "1759\n",
      "3\n",
      "cwa\n",
      "3\n",
      "10.1 °c\n",
      "4\n",
      "religious bias\n",
      "4\n",
      "“the nw [ new world translation ] emerges as the most accurate of the translations UNK although the general public and many bible scholars assume that the differences in the new world translation are the result of religious bias\n",
      "Iter 4000, val_start_accuracy = 0.2422, val_end_acc = 0.2891, val_acc = 0.1797, val loss = 6.079658\n",
      "Exact Match Score: 0.1875\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--4000.ckpt\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--4000.ckpt\n",
      "Iter 4200, start_accuracy = 0.2734, end_accuracy = 0.2578, accuracy = 0.1562, train loss = 5.842139\n",
      "0\n",
      "antioxidant\n",
      "0\n",
      "possess antioxidant properties\n",
      "1\n",
      "perceived promotion of crime\n",
      "1\n",
      "by victorian politicians in 1912 for the perceived promotion of crime\n",
      "2\n",
      "roxbury latin school\n",
      "2\n",
      "at\n",
      "3\n",
      "to have measured the two dimensions separately and report scores independently to avoid loss of information\n",
      "3\n",
      "\n",
      "4\n",
      "benedict of nursia\n",
      "4\n",
      "benedict of nursia ( d. 547 ) wrote the benedictine rule for western monasticism during the 6th century , detailing the administrative and spiritual responsibilities of a community of monks led by an abbot\n",
      "Iter 4200, start_accuracy = 0.2734, end_accuracy = 0.2578, accuracy = 0.1562, train loss = 5.842139\n",
      "Exact Match Score: 0.1797\n",
      "Iter 4400, start_accuracy = 0.2656, end_accuracy = 0.2578, accuracy = 0.1406, train loss = 5.823394\n",
      "0\n",
      "atsc\n",
      "0\n",
      "sdtv and hdtv . in the us the grand alliance proposed atsc as the new standard for sdtv and hdtv . both atsc and dvb were based on the mpeg-2 standard , although dvb systems may also be used to transmit video using the newer and more efficient h.264/mpeg-4 avc compression standards\n",
      "1\n",
      "violent confrontations\n",
      "1\n",
      "connecticut history . black panther party co-founder bobby seale and ten other party members were tried for murdering an alleged informant . beginning on may day , the city became a center of protest for 12,000 panther supporters , college students , and new left activists ( including jean genet , benjamin spock , abbie hoffman , jerry rubin , and john froines ) , who amassed on the new haven green\n",
      "2\n",
      "1997\n",
      "2\n",
      "1992\n",
      "3\n",
      "his swift boat and two other swift boats\n",
      "3\n",
      "star medal . on this occasion , kerry was in tactical command of his swift boat and two\n",
      "4\n",
      "9 august 1945\n",
      "4\n",
      "9 august 1945\n",
      "Iter 4400, start_accuracy = 0.2656, end_accuracy = 0.2578, accuracy = 0.1406, train loss = 5.823394\n",
      "Exact Match Score: 0.1641\n",
      "Iter 4600, start_accuracy = 0.3047, end_accuracy = 0.2812, accuracy = 0.2031, train loss = 5.679663\n",
      "0\n",
      "it is seldom\n",
      "0\n",
      "infrequent errors\n",
      "1\n",
      "gulf of guinea\n",
      "1\n",
      "cosmos 146 on march 10 , 1967 ; cosmos 154 on april 8 , 1967 ; zond 1967a september 27 , 1967 ; zond 1967b on november 22 , 1967 . zond 4 was launched on march 2 , 1968 , and successfully made a circumlunar flight . after its successful flight around the moon , zond 4 encountered problems with its earth reentry on march 9 , and was ordered destroyed by an explosive charge 15,000 meters\n",
      "2\n",
      "18.1 percent\n",
      "2\n",
      "18.1 percent\n",
      "3\n",
      "181,973\n",
      "3\n",
      "4,395\n",
      "4\n",
      "the minish cap\n",
      "4\n",
      "also\n",
      "Iter 4600, start_accuracy = 0.3047, end_accuracy = 0.2812, accuracy = 0.2031, train loss = 5.679663\n",
      "Exact Match Score: 0.2188\n",
      "Iter 4800, start_accuracy = 0.3438, end_accuracy = 0.3438, accuracy = 0.2344, train loss = 5.521011\n",
      "0\n",
      "1968\n",
      "0\n",
      "1968\n",
      "1\n",
      "national archives\n",
      "1\n",
      "national archives\n",
      "2\n",
      "\" preventive \" measures\n",
      "2\n",
      "preventive \" measures\n",
      "3\n",
      "private sector\n",
      "3\n",
      "\n",
      "4\n",
      "minds\n",
      "4\n",
      "spirit\n",
      "Iter 4800, start_accuracy = 0.3438, end_accuracy = 0.3438, accuracy = 0.2344, train loss = 5.521011\n",
      "Exact Match Score: 0.2578\n",
      "Iter 5000, start_accuracy = 0.1953, end_accuracy = 0.2891, accuracy = 0.1484, train loss = 5.844016\n",
      "0\n",
      "climate change\n",
      "0\n",
      "dame would become \" one of the pre–eminent research institutions in the world \" in his inaugural address . the university has many multi-disciplinary institutes devoted to research in varying fields , including the medieval institute , the kellogg institute for international studies , the kroc institute for international peace studies , and the center for social concerns . recent research includes work on family conflict and child development , genome mapping , the increasing trade deficit of the united states with china , studies in fluid mechanics , computational science and engineering , and marketing trends\n",
      "1\n",
      "tlalpan\n",
      "1\n",
      "\n",
      "2\n",
      "gives students an overview of the basics of immunology\n",
      "2\n",
      "\n",
      "3\n",
      "1945\n",
      "3\n",
      "\n",
      "4\n",
      "her mother\n",
      "4\n",
      "middle name\n",
      "Iter 5000, start_accuracy = 0.1953, end_accuracy = 0.2891, accuracy = 0.1484, train loss = 5.844016\n",
      "Exact Match Score: 0.1719\n",
      "Iter 5000, val_start_accuracy = 0.1328, val_end_acc = 0.2188, val_acc = 0.1016, val loss = 6.753597\n",
      "0\n",
      "eight\n",
      "0\n",
      "compatriot\n",
      "1\n",
      "conservative romans\n",
      "1\n",
      "imported mystery religions\n",
      "2\n",
      "local\n",
      "2\n",
      "UNK \" days\n",
      "3\n",
      "camillus\n",
      "3\n",
      "extent\n",
      "4\n",
      "seventh\n",
      "4\n",
      "youtube music video spoof to encourage young americans to enroll in the affordable care act ( UNK ) UNK health insurance\n",
      "Iter 5000, val_start_accuracy = 0.1328, val_end_acc = 0.2188, val_acc = 0.1016, val loss = 6.753597\n",
      "Exact Match Score: 0.1172\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--5000.ckpt\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--5000.ckpt\n",
      "Iter 5200, start_accuracy = 0.2734, end_accuracy = 0.3906, accuracy = 0.2422, train loss = 5.236983\n",
      "0\n",
      "60,000\n",
      "0\n",
      "1993–94 season\n",
      "1\n",
      "coach\n",
      "1\n",
      "eleven players\n",
      "2\n",
      "eucalyptus tree\n",
      "2\n",
      "deforestation and forest fires are a problem in many areas , as is the continual spread of the eucalyptus tree\n",
      "3\n",
      "the reason for these differences is not well known\n",
      "3\n",
      "\n",
      "4\n",
      "the director of central intelligence\n",
      "4\n",
      "director of central intelligence is the only federal government employee who can spend \" un-vouchered \" government money . the government has disclosed a total figure for all non-military intelligence spending since 2007 ; the fiscal 2013 figure is $ 52.6 billion . according to the 2013 mass surveillance disclosures , the cia 's fiscal 2013 budget is $ 14.7 billion , 28 % of the total and almost 50 % more than the budget of the national security agency . cia 's humint budget is $ 2.3 billion\n",
      "Iter 5200, start_accuracy = 0.2734, end_accuracy = 0.3906, accuracy = 0.2422, train loss = 5.236983\n",
      "Exact Match Score: 0.2812\n",
      "Iter 5400, start_accuracy = 0.3516, end_accuracy = 0.3828, accuracy = 0.2734, train loss = 4.732279\n",
      "0\n",
      "$ 15.5m\n",
      "0\n",
      "royal dutch shell and brian anderson , the head of its nigerian operation . in 2009 , shell agreed to pay $ 15.5m in a legal settlement . shell has not accepted any liability over the allegations against it\n",
      "1\n",
      "pacific environment community\n",
      "1\n",
      "european union and australia\n",
      "2\n",
      "warsaw lyceum\n",
      "2\n",
      "warsaw lyceum\n",
      "3\n",
      "walvis bay\n",
      "3\n",
      "walvis bay\n",
      "4\n",
      "1325\n",
      "4\n",
      "1325\n",
      "Iter 5400, start_accuracy = 0.3516, end_accuracy = 0.3828, accuracy = 0.2734, train loss = 4.732279\n",
      "Exact Match Score: 0.2969\n",
      "Iter 5600, start_accuracy = 0.2344, end_accuracy = 0.3359, accuracy = 0.1953, train loss = 5.555788\n",
      "0\n",
      "lessened\n",
      "0\n",
      ".\n",
      "1\n",
      "congress\n",
      "1\n",
      "congress\n",
      "2\n",
      "zelda\n",
      "2\n",
      "\n",
      "3\n",
      "14 june 1925\n",
      "3\n",
      "14 june 1925\n",
      "4\n",
      "salah salem\n",
      "4\n",
      "israeli army . amer and salah salem\n",
      "Iter 5600, start_accuracy = 0.2344, end_accuracy = 0.3359, accuracy = 0.1953, train loss = 5.555788\n",
      "Exact Match Score: 0.2344\n",
      "Iter 5800, start_accuracy = 0.3750, end_accuracy = 0.4141, accuracy = 0.2891, train loss = 5.074060\n",
      "0\n",
      "47\n",
      "0\n",
      "47\n",
      "1\n",
      "their representatives\n",
      "1\n",
      "boilerplate clause about how their representatives have communicated ( or exchanged ) their full powers\n",
      "2\n",
      "2004\n",
      "2\n",
      "2004\n",
      "3\n",
      "around 300\n",
      "3\n",
      "five\n",
      "4\n",
      "three and a half minutes\n",
      "4\n",
      "\n",
      "Iter 5800, start_accuracy = 0.3750, end_accuracy = 0.4141, accuracy = 0.2891, train loss = 5.074060\n",
      "Exact Match Score: 0.2969\n",
      "Iter 6000, start_accuracy = 0.3672, end_accuracy = 0.4219, accuracy = 0.3125, train loss = 4.713524\n",
      "0\n",
      "madonna\n",
      "0\n",
      "michigan\n",
      "1\n",
      "june 28 , 1776\n",
      "1\n",
      "june 28 , 1776\n",
      "2\n",
      "lord president of the council\n",
      "2\n",
      "conservative party\n",
      "3\n",
      "the coffee ceremony\n",
      "3\n",
      "coffee ceremony\n",
      "4\n",
      "infantry positions\n",
      "4\n",
      "generally limited and the muzzle flashes reveal infantry positions\n",
      "Iter 6000, start_accuracy = 0.3672, end_accuracy = 0.4219, accuracy = 0.3125, train loss = 4.713524\n",
      "Exact Match Score: 0.3750\n",
      "Iter 6000, val_start_accuracy = 0.2031, val_end_acc = 0.3047, val_acc = 0.1797, val loss = 6.645287\n",
      "0\n",
      "animal instincts\n",
      "0\n",
      "acceptable occupation\n",
      "1\n",
      "beef\n",
      "1\n",
      "UNK\n",
      "2\n",
      "christ\n",
      "2\n",
      "king david iv\n",
      "3\n",
      "christian , imperial , and \" UNK \"\n",
      "3\n",
      "an instrument of the pax UNK with the power of the christian UNK\n",
      "4\n",
      "UNK salinity crisis\n",
      "4\n",
      "UNK salinity crisis\n",
      "Iter 6000, val_start_accuracy = 0.2031, val_end_acc = 0.3047, val_acc = 0.1797, val loss = 6.645287\n",
      "Exact Match Score: 0.2031\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--6000.ckpt\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--6000.ckpt\n",
      "Iter 6200, start_accuracy = 0.3047, end_accuracy = 0.3594, accuracy = 0.2500, train loss = 4.984369\n",
      "0\n",
      "the achaemenid empire\n",
      "0\n",
      "yet seen\n",
      "1\n",
      "90\n",
      "1\n",
      "four\n",
      "2\n",
      "27 /km² ( 70 /sq mi )\n",
      "2\n",
      "41,285 square kilometres\n",
      "3\n",
      "miami international\n",
      "3\n",
      "busiest airport\n",
      "4\n",
      "\" i 'm [ candidate 's name ] , and i approve this message . \"\n",
      "4\n",
      "the statement , \" i 'm [ candidate 's name ] , and i approve this message\n",
      "Iter 6200, start_accuracy = 0.3047, end_accuracy = 0.3594, accuracy = 0.2500, train loss = 4.984369\n",
      "Exact Match Score: 0.2656\n",
      "Iter 6400, start_accuracy = 0.3281, end_accuracy = 0.3672, accuracy = 0.2422, train loss = 4.655960\n",
      "0\n",
      "2007 and 2015\n",
      "0\n",
      "2007 and 2015\n",
      "1\n",
      "electricity distribution\n",
      "1\n",
      "no practical commercial market\n",
      "2\n",
      "increased from 500,000 francs in 1852 to five million in 1860\n",
      "2\n",
      "1879\n",
      "3\n",
      "j.m . cohen\n",
      "3\n",
      "cohen\n",
      "4\n",
      "maadi , egypt\n",
      "4\n",
      "maadi , egypt\n",
      "Iter 6400, start_accuracy = 0.3281, end_accuracy = 0.3672, accuracy = 0.2422, train loss = 4.655960\n",
      "Exact Match Score: 0.2812\n",
      "Iter 6600, start_accuracy = 0.4531, end_accuracy = 0.4922, accuracy = 0.3984, train loss = 4.281773\n",
      "0\n",
      "the economist\n",
      "0\n",
      "early\n",
      "1\n",
      "noxious and non-noxious\n",
      "1\n",
      "noxious and non-noxious stimuli , while others , nociceptors , respond only to noxious , high intensity stimuli\n",
      "2\n",
      "steven spielberg\n",
      "2\n",
      "\n",
      "3\n",
      "35 %\n",
      "3\n",
      "35 %\n",
      "4\n",
      "1995\n",
      "4\n",
      "1995\n",
      "Iter 6600, start_accuracy = 0.4531, end_accuracy = 0.4922, accuracy = 0.3984, train loss = 4.281773\n",
      "Exact Match Score: 0.4297\n",
      "Iter 6800, start_accuracy = 0.3984, end_accuracy = 0.4297, accuracy = 0.3125, train loss = 4.780118\n",
      "0\n",
      "seed germination and dispersal needs\n",
      "0\n",
      "establishes critical habitat\n",
      "1\n",
      "coming out of the closet to oneself , a spouse of the opposite sex , and children\n",
      "1\n",
      "gay and lesbian\n",
      "2\n",
      "an anti-cnn website\n",
      "2\n",
      "a beijing citizen\n",
      "3\n",
      "reversal ratings\n",
      "3\n",
      "capacitors usually need\n",
      "4\n",
      "explosive ordnance disposal\n",
      "4\n",
      "explosive ordnance disposal\n",
      "Iter 6800, start_accuracy = 0.3984, end_accuracy = 0.4297, accuracy = 0.3125, train loss = 4.780118\n",
      "Exact Match Score: 0.3281\n",
      "Iter 7000, start_accuracy = 0.4375, end_accuracy = 0.4531, accuracy = 0.3516, train loss = 4.576338\n",
      "0\n",
      "donkey kong country\n",
      "0\n",
      "donkey kong country\n",
      "1\n",
      "four\n",
      "1\n",
      "four\n",
      "2\n",
      "judea\n",
      "2\n",
      "6th century bc\n",
      "3\n",
      "young bengal\n",
      "3\n",
      "young bengal movement\n",
      "4\n",
      "john gerard\n",
      "4\n",
      "natural history\n",
      "Iter 7000, start_accuracy = 0.4375, end_accuracy = 0.4531, accuracy = 0.3516, train loss = 4.576338\n",
      "Exact Match Score: 0.3750\n",
      "Iter 7000, val_start_accuracy = 0.1797, val_end_acc = 0.2578, val_acc = 0.1484, val loss = 6.694173\n",
      "0\n",
      "videos that infringe copyright\n",
      "0\n",
      "necessary\n",
      "1\n",
      "mary of guise\n",
      "1\n",
      "regent mary of guise\n",
      "2\n",
      "arabian UNK\n",
      "2\n",
      "arabian UNK\n",
      "3\n",
      "UNK million\n",
      "3\n",
      "UNK million\n",
      "4\n",
      "UNK %\n",
      "4\n",
      "14.6 %\n",
      "Iter 7000, val_start_accuracy = 0.1797, val_end_acc = 0.2578, val_acc = 0.1484, val loss = 6.694173\n",
      "Exact Match Score: 0.1719\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--7000.ckpt\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--7000.ckpt\n",
      "Iter 7200, start_accuracy = 0.3516, end_accuracy = 0.4297, accuracy = 0.3125, train loss = 4.588717\n",
      "0\n",
      "shoichi washizawa\n",
      "0\n",
      "shoichi washizawa\n",
      "1\n",
      "middle east\n",
      "1\n",
      "middle east\n",
      "2\n",
      "1990\n",
      "2\n",
      "1990\n",
      "3\n",
      "japanese\n",
      "3\n",
      "universal studios inc.\n",
      "4\n",
      "west 10th street\n",
      "4\n",
      "newgate prison\n",
      "Iter 7200, start_accuracy = 0.3516, end_accuracy = 0.4297, accuracy = 0.3125, train loss = 4.588717\n",
      "Exact Match Score: 0.3281\n",
      "Iter 7400, start_accuracy = 0.4141, end_accuracy = 0.3750, accuracy = 0.2969, train loss = 4.640382\n",
      "0\n",
      "the dolphin\n",
      "0\n",
      "junk\n",
      "1\n",
      "soul solution\n",
      "1\n",
      "soul solution mix of toni braxton\n",
      "2\n",
      "left arm\n",
      "2\n",
      "cardiac chest pain\n",
      "3\n",
      "kiev\n",
      "3\n",
      "kiev\n",
      "4\n",
      "guns\n",
      "4\n",
      "first line \" service\n",
      "Iter 7400, start_accuracy = 0.4141, end_accuracy = 0.3750, accuracy = 0.2969, train loss = 4.640382\n",
      "Exact Match Score: 0.3438\n",
      "Iter 7600, start_accuracy = 0.3047, end_accuracy = 0.3672, accuracy = 0.2188, train loss = 5.359247\n",
      "0\n",
      "furious\n",
      "0\n",
      "russian black sea fleet was operating against ottoman coastal traffic between constantinople ( currently named istanbul ) and the caucasus ports , while the ottoman fleet sought to protect this supply line . the clash came on 30 november 1853 when a russian fleet attacked an ottoman force in the harbour at sinop , and destroyed it at the battle of sinop . the battle outraged opinion in uk , which called for war . there was little additional naval action until march 1854 when on the declaration of war the british frigate furious\n",
      "1\n",
      "roman catholic\n",
      "1\n",
      "roman catholic church canon law\n",
      "2\n",
      "dan kniley\n",
      "2\n",
      "king lui-wu\n",
      "3\n",
      "nonviolent\n",
      "3\n",
      "300 lives\n",
      "4\n",
      "cantabrian\n",
      "4\n",
      "madrid\n",
      "Iter 7600, start_accuracy = 0.3047, end_accuracy = 0.3672, accuracy = 0.2188, train loss = 5.359247\n",
      "Exact Match Score: 0.2266\n",
      "Iter 7800, start_accuracy = 0.4375, end_accuracy = 0.5156, accuracy = 0.3516, train loss = 4.037447\n",
      "0\n",
      "683–84\n",
      "0\n",
      "684\n",
      "1\n",
      "over 120,000\n",
      "1\n",
      "120,000\n",
      "2\n",
      "1978\n",
      "2\n",
      "1978\n",
      "3\n",
      "britain 's durham university and universiti teknologi mara\n",
      "3\n",
      "understanding politics and politicians\n",
      "4\n",
      "mohammad ali shah\n",
      "4\n",
      "mohammad ali shah\n",
      "Iter 7800, start_accuracy = 0.4375, end_accuracy = 0.5156, accuracy = 0.3516, train loss = 4.037447\n",
      "Exact Match Score: 0.4297\n",
      "Iter 8000, start_accuracy = 0.4375, end_accuracy = 0.5156, accuracy = 0.3672, train loss = 3.851127\n",
      "0\n",
      "duits\n",
      "0\n",
      "nederlands\n",
      "1\n",
      "young\n",
      "1\n",
      "\n",
      "2\n",
      "milton\n",
      "2\n",
      "milton\n",
      "3\n",
      "manhattan\n",
      "3\n",
      "bronx , and staten island\n",
      "4\n",
      "zhang juzheng\n",
      "4\n",
      "was\n",
      "Iter 8000, start_accuracy = 0.4375, end_accuracy = 0.5156, accuracy = 0.3672, train loss = 3.851127\n",
      "Exact Match Score: 0.4062\n",
      "Iter 8000, val_start_accuracy = 0.2344, val_end_acc = 0.2578, val_acc = 0.1406, val loss = 6.506029\n",
      "0\n",
      "UNK\n",
      "0\n",
      "the practice of UNK\n",
      "1\n",
      "mechanistic\n",
      "1\n",
      "\n",
      "2\n",
      "%\n",
      "2\n",
      "independent content creators\n",
      "3\n",
      "interpol\n",
      "3\n",
      "policing agents have undertaken a variety of cross-border police missions for many years ( UNK , 2002 ) . for example , in the 19th century a number of european policing agencies undertook cross-border surveillance because of concerns about anarchist UNK and other political radicals . a notable example of this was the occasional surveillance by prussian police of karl marx\n",
      "4\n",
      "70 %\n",
      "4\n",
      "70 %\n",
      "Iter 8000, val_start_accuracy = 0.2344, val_end_acc = 0.2578, val_acc = 0.1406, val loss = 6.506029\n",
      "Exact Match Score: 0.1641\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--8000.ckpt\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--8000.ckpt\n",
      "Iter 8200, start_accuracy = 0.3828, end_accuracy = 0.3594, accuracy = 0.2266, train loss = 4.569810\n",
      "0\n",
      "addictive nature of cigarettes\n",
      "0\n",
      "addictive nature of cigarettes\n",
      "1\n",
      "coachella\n",
      "1\n",
      "compilation album cruel summer\n",
      "2\n",
      "manchester united did not enter the 1999–2000 fa cup ,\n",
      "2\n",
      "defending holders manchester united did not enter the 1999–2000 fa cup , as they were already in the inaugural club world championship , with the club stating that entering both tournaments would overload their fixture schedule and make it more difficult to defend their champions league and premiership titles . the club claimed that they did not want to devalue the fa cup by fielding a weaker side . the move benefited united as they received a two-week break and won the 1999–2000 league title by an 18-point margin , although they did not progress past the group stage of the club world championship . the withdrawal from the fa cup\n",
      "3\n",
      "1439\n",
      "3\n",
      "1439\n",
      "4\n",
      "richard hagopian\n",
      "4\n",
      "richard hagopian\n",
      "Iter 8200, start_accuracy = 0.3828, end_accuracy = 0.3594, accuracy = 0.2266, train loss = 4.569810\n",
      "Exact Match Score: 0.2656\n",
      "Iter 8400, start_accuracy = 0.4375, end_accuracy = 0.5078, accuracy = 0.3438, train loss = 3.851064\n",
      "0\n",
      "fourteen\n",
      "0\n",
      "fourteen\n",
      "1\n",
      "flowfest or hip hop porto\n",
      "1\n",
      "festival sudoeste in zambujeira do mar , festival de paredes de coura in paredes de coura , festival vilar de mouros near caminha , boom festival in idanha-a-nova municipality , optimus alive ! , sumol summer fest in ericeira , rock in rio lisboa and super bock super rock in greater lisbon . out of the summer season , portugal has a large number of festivals , designed more to an urban audience , like flowfest or hip hop porto . furthermore , one of the largest international goa trance festivals takes place in central portugal every two years , the boom festival , that is also the only festival in portugal to win international awards : european festival award 2010 – green'n'clean festival of the year and the greener festival award outstanding 2008 and 2010 . there is also the student festivals of queima das fitas\n",
      "2\n",
      "12\n",
      "2\n",
      "12\n",
      "3\n",
      "orthodoxy of rituals\n",
      "3\n",
      "extreme asceticism found\n",
      "4\n",
      "warsaw\n",
      "4\n",
      "20\n",
      "Iter 8400, start_accuracy = 0.4375, end_accuracy = 0.5078, accuracy = 0.3438, train loss = 3.851064\n",
      "Exact Match Score: 0.3828\n",
      "Iter 8600, start_accuracy = 0.4219, end_accuracy = 0.4844, accuracy = 0.3672, train loss = 4.001956\n",
      "0\n",
      "directory of public worship\n",
      "0\n",
      "\n",
      "1\n",
      "isotropic\n",
      "1\n",
      "three-dimensional\n",
      "2\n",
      "\" deipara \" and \" dei genetrix \"\n",
      "2\n",
      "oriental orthodoxy , the anglican church , and all eastern catholic churches as theotokos , a title recognized at the third ecumenical council ( held at ephesus to address the teachings of nestorius , in 431 ) . theotokos ( and its latin equivalents , \" deipara \" and \" dei genetrix\n",
      "3\n",
      "back bay\n",
      "3\n",
      "south station\n",
      "4\n",
      "personal information\n",
      "4\n",
      "submit personal information\n",
      "Iter 8600, start_accuracy = 0.4219, end_accuracy = 0.4844, accuracy = 0.3672, train loss = 4.001956\n",
      "Exact Match Score: 0.4062\n",
      "Iter 8800, start_accuracy = 0.3906, end_accuracy = 0.5156, accuracy = 0.3203, train loss = 4.316601\n",
      "0\n",
      "18th and 19th centuries\n",
      "0\n",
      "\n",
      "1\n",
      "new york state empire state development\n",
      "1\n",
      "empire state development\n",
      "2\n",
      "21st street\n",
      "2\n",
      "21st street\n",
      "3\n",
      "1783\n",
      "3\n",
      "1783\n",
      "4\n",
      "black\n",
      "4\n",
      "black\n",
      "Iter 8800, start_accuracy = 0.3906, end_accuracy = 0.5156, accuracy = 0.3203, train loss = 4.316601\n",
      "Exact Match Score: 0.3750\n",
      "Iter 9000, start_accuracy = 0.4453, end_accuracy = 0.5234, accuracy = 0.3594, train loss = 3.973333\n",
      "0\n",
      "18\n",
      "0\n",
      "\n",
      "1\n",
      "military officers and local elites\n",
      "1\n",
      "de facto head of state\n",
      "2\n",
      "west africa\n",
      "2\n",
      "west africa\n",
      "3\n",
      "ikea\n",
      "3\n",
      "marlands\n",
      "4\n",
      "james green\n",
      "4\n",
      "former dean james green\n",
      "Iter 9000, start_accuracy = 0.4453, end_accuracy = 0.5234, accuracy = 0.3594, train loss = 3.973333\n",
      "Exact Match Score: 0.3984\n",
      "Iter 9000, val_start_accuracy = 0.1875, val_end_acc = 0.1797, val_acc = 0.1406, val loss = 7.228805\n",
      "0\n",
      "800 million\n",
      "0\n",
      "more than 15\n",
      "1\n",
      "UNK\n",
      "1\n",
      "\n",
      "2\n",
      "all\n",
      "2\n",
      "all\n",
      "3\n",
      "profile\n",
      "3\n",
      "profile\n",
      "4\n",
      "UNK\n",
      "4\n",
      "restoration works began in 2010 to rehabilitate the UNK UNK into the UNK museum\n",
      "Iter 9000, val_start_accuracy = 0.1875, val_end_acc = 0.1797, val_acc = 0.1406, val loss = 7.228805\n",
      "Exact Match Score: 0.1562\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--9000.ckpt\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--9000.ckpt\n",
      "Iter 9200, start_accuracy = 0.4531, end_accuracy = 0.4688, accuracy = 0.3516, train loss = 4.328259\n",
      "0\n",
      "shoichi washizawa\n",
      "0\n",
      "shoichi washizawa\n",
      "1\n",
      "syrian\n",
      "1\n",
      "ongoing syrian civil war\n",
      "2\n",
      "1718\n",
      "2\n",
      "1701\n",
      "3\n",
      "2006\n",
      "3\n",
      "2006\n",
      "4\n",
      "jilin and heilongjiang\n",
      "4\n",
      "jilin and heilongjiang\n",
      "Iter 9200, start_accuracy = 0.4531, end_accuracy = 0.4688, accuracy = 0.3516, train loss = 4.328259\n",
      "Exact Match Score: 0.3906\n",
      "Iter 9400, start_accuracy = 0.3594, end_accuracy = 0.4297, accuracy = 0.2734, train loss = 4.521218\n",
      "0\n",
      "bismarck 's demand for the return of alsace\n",
      "0\n",
      "dramatic shift\n",
      "1\n",
      "reverend john davenport\n",
      "1\n",
      "reverend john davenport\n",
      "2\n",
      "sony through its red subsidiary\n",
      "2\n",
      "michael jackson\n",
      "3\n",
      "syngman rhee\n",
      "3\n",
      "\n",
      "4\n",
      "the annual installation of the master and officers of the lodge\n",
      "4\n",
      "progressively initiated into freemasonry\n",
      "Iter 9400, start_accuracy = 0.3594, end_accuracy = 0.4297, accuracy = 0.2734, train loss = 4.521218\n",
      "Exact Match Score: 0.2891\n",
      "Iter 9600, start_accuracy = 0.5469, end_accuracy = 0.5312, accuracy = 0.4141, train loss = 3.374159\n",
      "0\n",
      "marcel proust and andré gide\n",
      "0\n",
      "marcel proust and andré gide\n",
      "1\n",
      "their precious metal value\n",
      "1\n",
      "their numismatic value , and for gold and silver coins\n",
      "2\n",
      "2008\n",
      "2\n",
      "1989\n",
      "3\n",
      "20th century mandolin music from europe and one of the most complete collections of mandolin magazines from mandolin 's golden age\n",
      "3\n",
      "europe and one of the most complete collections of mandolin magazines from mandolin 's golden age\n",
      "4\n",
      "september 21 , 19 bc\n",
      "4\n",
      "september 21 , 19 bc\n",
      "Iter 9600, start_accuracy = 0.5469, end_accuracy = 0.5312, accuracy = 0.4141, train loss = 3.374159\n",
      "Exact Match Score: 0.4531\n",
      "Iter 9800, start_accuracy = 0.3984, end_accuracy = 0.4609, accuracy = 0.2656, train loss = 3.942554\n",
      "0\n",
      "kone monospace 's ecodisc\n",
      "0\n",
      "kone monospace 's ecodisc\n",
      "1\n",
      "nashville\n",
      "1\n",
      "1989\n",
      "2\n",
      "dynastic divisions of inheritance among the kings offspring\n",
      "2\n",
      "dynastic divisions of inheritance among the kings offspring\n",
      "3\n",
      "the weekends\n",
      "3\n",
      "the weekends\n",
      "4\n",
      "douro valley , the island of porto santo , and alentejo\n",
      "4\n",
      "douro valley , the island of porto santo , and alentejo\n",
      "Iter 9800, start_accuracy = 0.3984, end_accuracy = 0.4609, accuracy = 0.2656, train loss = 3.942554\n",
      "Exact Match Score: 0.3125\n",
      "Iter 10000, start_accuracy = 0.4141, end_accuracy = 0.4531, accuracy = 0.2812, train loss = 3.986191\n",
      "0\n",
      "40 %\n",
      "0\n",
      "43 % of all jews reside in israel ( 6.1 million ) , and 40 %\n",
      "1\n",
      "new amsterdam\n",
      "1\n",
      "new amsterdam in 1626 . the city and its surroundings came under english control in 1664 . new york served as the capital of the united states from 1785 until 1790 . it has been the country 's largest city since 1790 . the statue of liberty greeted millions of immigrants\n",
      "2\n",
      "its gdp is the fourth largest economy in the united states\n",
      "2\n",
      "2006–07\n",
      "3\n",
      "jordan river\n",
      "3\n",
      "jordan river\n",
      "4\n",
      "philippines\n",
      "4\n",
      "philippines to advise\n",
      "Iter 10000, start_accuracy = 0.4141, end_accuracy = 0.4531, accuracy = 0.2812, train loss = 3.986191\n",
      "Exact Match Score: 0.3281\n",
      "Iter 10000, val_start_accuracy = 0.1562, val_end_acc = 0.2188, val_acc = 0.1094, val loss = 7.834177\n",
      "0\n",
      "god\n",
      "0\n",
      "exactly 144,000\n",
      "1\n",
      "comedy\n",
      "1\n",
      "dr. john becker\n",
      "2\n",
      "voting\n",
      "2\n",
      "\n",
      "3\n",
      "fifty\n",
      "3\n",
      "\n",
      "4\n",
      "cretaceous period\n",
      "4\n",
      "following cretaceous period\n",
      "Iter 10000, val_start_accuracy = 0.1562, val_end_acc = 0.2188, val_acc = 0.1094, val loss = 7.834177\n",
      "Exact Match Score: 0.1172\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--10000.ckpt\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--10000.ckpt\n",
      "Iter 10200, start_accuracy = 0.4844, end_accuracy = 0.5078, accuracy = 0.3750, train loss = 3.670798\n",
      "0\n",
      "linear predictive coding\n",
      "0\n",
      "linear predictive coding\n",
      "1\n",
      "the period spanning the reign of king edward vii up to the end of the first world war ,\n",
      "1\n",
      "period spanning the reign of king edward vii up to the end of the first world war\n",
      "2\n",
      "its medieval church towers\n",
      "2\n",
      "church towers\n",
      "3\n",
      "83 million\n",
      "3\n",
      "$ 457 million\n",
      "4\n",
      "channel incision\n",
      "4\n",
      "place\n",
      "Iter 10200, start_accuracy = 0.4844, end_accuracy = 0.5078, accuracy = 0.3750, train loss = 3.670798\n",
      "Exact Match Score: 0.4141\n",
      "Iter 10400, start_accuracy = 0.4688, end_accuracy = 0.5234, accuracy = 0.3984, train loss = 3.625906\n",
      "0\n",
      "whites\n",
      "0\n",
      "whites\n",
      "1\n",
      "nutrition literacy\n",
      "1\n",
      "nutrition literacy and income level and nutrition literacy and educational attainment\n",
      "2\n",
      "warm-up\n",
      "2\n",
      "\n",
      "3\n",
      "antel telecommunication tower\n",
      "3\n",
      "\n",
      "4\n",
      "the late '80s had marked a low point for english football\n",
      "4\n",
      "late\n",
      "Iter 10400, start_accuracy = 0.4688, end_accuracy = 0.5234, accuracy = 0.3984, train loss = 3.625906\n",
      "Exact Match Score: 0.4375\n",
      "Iter 10600, start_accuracy = 0.3594, end_accuracy = 0.3984, accuracy = 0.2578, train loss = 4.344119\n",
      "0\n",
      "education\n",
      "0\n",
      "ambitious , and they will require enormous efforts across countries , continents , industries and disciplines -\n",
      "1\n",
      "42 %\n",
      "1\n",
      "16 %\n",
      "2\n",
      "rest of roman hispania\n",
      "2\n",
      "roman hispania\n",
      "3\n",
      "religious\n",
      "3\n",
      "swiss defeat\n",
      "4\n",
      "the coleherne\n",
      "4\n",
      "red lion\n",
      "Iter 10600, start_accuracy = 0.3594, end_accuracy = 0.3984, accuracy = 0.2578, train loss = 4.344119\n",
      "Exact Match Score: 0.3047\n",
      "Iter 10800, start_accuracy = 0.4766, end_accuracy = 0.5703, accuracy = 0.4141, train loss = 3.504708\n",
      "0\n",
      "13th and 14th\n",
      "0\n",
      "13th and 14th\n",
      "1\n",
      "the name of the winning team is engraved on the silver band around the base\n",
      "1\n",
      "\n",
      "2\n",
      "some two hundred\n",
      "2\n",
      "more than one\n",
      "3\n",
      "newspaper\n",
      "3\n",
      "newspaper\n",
      "4\n",
      "alice cooper\n",
      "4\n",
      "alice cooper\n",
      "Iter 10800, start_accuracy = 0.4766, end_accuracy = 0.5703, accuracy = 0.4141, train loss = 3.504708\n",
      "Exact Match Score: 0.4766\n",
      "Iter 11000, start_accuracy = 0.4844, end_accuracy = 0.5859, accuracy = 0.4453, train loss = 3.395760\n",
      "0\n",
      "vajrayana\n",
      "0\n",
      "vajrayana\n",
      "1\n",
      "the newly formed top division would have commercial independence from the football association and the football league\n",
      "1\n",
      "the basic principles\n",
      "2\n",
      "rodolphe reuss\n",
      "2\n",
      "historian rodolphe reuss and art\n",
      "3\n",
      "sir baldwin spencer\n",
      "3\n",
      "sir baldwin spencer\n",
      "4\n",
      "two\n",
      "4\n",
      "roman pontiff 's wishes\n",
      "Iter 11000, start_accuracy = 0.4844, end_accuracy = 0.5859, accuracy = 0.4453, train loss = 3.395760\n",
      "Exact Match Score: 0.4844\n",
      "Iter 11000, val_start_accuracy = 0.1719, val_end_acc = 0.1875, val_acc = 0.1172, val loss = 7.854659\n",
      "0\n",
      "indigenous , mediterranean , african and western\n",
      "0\n",
      "\n",
      "1\n",
      "uni\n",
      "1\n",
      "\n",
      "2\n",
      "the jewish kabbalah\n",
      "2\n",
      "christian gospels\n",
      "3\n",
      "ashoka\n",
      "3\n",
      "\n",
      "4\n",
      "archaeologists and UNK\n",
      "4\n",
      "scavengers\n",
      "Iter 11000, val_start_accuracy = 0.1719, val_end_acc = 0.1875, val_acc = 0.1172, val loss = 7.854659\n",
      "Exact Match Score: 0.1641\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--11000.ckpt\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--11000.ckpt\n",
      "Iter 11200, start_accuracy = 0.5391, end_accuracy = 0.6094, accuracy = 0.4297, train loss = 3.059008\n",
      "0\n",
      "meacon\n",
      "0\n",
      "meacon system\n",
      "1\n",
      "70\n",
      "1\n",
      "70\n",
      "2\n",
      "arbitrary arrest , prolonged pretrial detention and denial of a fair trial\n",
      "2\n",
      "arbitrary arrest\n",
      "3\n",
      "direct light\n",
      "3\n",
      "light\n",
      "4\n",
      "10th millennium bc\n",
      "4\n",
      "\n",
      "Iter 11200, start_accuracy = 0.5391, end_accuracy = 0.6094, accuracy = 0.4297, train loss = 3.059008\n",
      "Exact Match Score: 0.5000\n",
      "Iter 11400, start_accuracy = 0.4688, end_accuracy = 0.5469, accuracy = 0.3750, train loss = 3.454912\n",
      "0\n",
      "urban\n",
      "0\n",
      "urban\n",
      "1\n",
      "rupert murdoch\n",
      "1\n",
      "rupert murdoch\n",
      "2\n",
      "princess victoria of saxe-coburg-saalfeld\n",
      "2\n",
      "prince of leiningen\n",
      "3\n",
      "events\n",
      "3\n",
      "commonly used today\n",
      "4\n",
      "bartolomeo bortolazzi of venice and pietro vimercati\n",
      "4\n",
      "bartolomeo bortolazzi of venice and pietro vimercati\n",
      "Iter 11400, start_accuracy = 0.4688, end_accuracy = 0.5469, accuracy = 0.3750, train loss = 3.454912\n",
      "Exact Match Score: 0.4062\n",
      "Iter 11600, start_accuracy = 0.5547, end_accuracy = 0.5859, accuracy = 0.4609, train loss = 3.314844\n",
      "0\n",
      "15 september\n",
      "0\n",
      "15 september\n",
      "1\n",
      "their relatives on the island seemed \" miserable , degraded savages \"\n",
      "1\n",
      "the three fuegian missionaries the expedition returned to tierra del fuego were friendly and civilised , yet to darwin their relatives on the island seemed \" miserable , degraded savages \" , and he no longer saw an unbridgeable gap between humans and animals\n",
      "2\n",
      "major news and media\n",
      "2\n",
      "major news and media websites\n",
      "3\n",
      "rarely used kanji or chinese characters\n",
      "3\n",
      "kanji or chinese characters\n",
      "4\n",
      "upper ohio river basin\n",
      "4\n",
      "village of shackamaxon\n",
      "Iter 11600, start_accuracy = 0.5547, end_accuracy = 0.5859, accuracy = 0.4609, train loss = 3.314844\n",
      "Exact Match Score: 0.5000\n",
      "Iter 11800, start_accuracy = 0.4375, end_accuracy = 0.5547, accuracy = 0.3359, train loss = 3.547019\n",
      "0\n",
      "371 bc\n",
      "0\n",
      "371 bc ) , but after the battle of mantinea ( 362 bc\n",
      "1\n",
      "agglutination\n",
      "1\n",
      "\n",
      "2\n",
      "domestic organization\n",
      "2\n",
      "domestic organization \"\n",
      "3\n",
      "celts\n",
      "3\n",
      "celts\n",
      "4\n",
      "slum-dwelling\n",
      "4\n",
      "22 %\n",
      "Iter 11800, start_accuracy = 0.4375, end_accuracy = 0.5547, accuracy = 0.3359, train loss = 3.547019\n",
      "Exact Match Score: 0.3828\n",
      "Iter 12000, start_accuracy = 0.4531, end_accuracy = 0.5781, accuracy = 0.3828, train loss = 3.352694\n",
      "0\n",
      "the willingness of roman troops to wage war against one another\n",
      "0\n",
      "one another\n",
      "1\n",
      "un under-secretary general\n",
      "1\n",
      "general achim\n",
      "2\n",
      "riba\n",
      "2\n",
      "riba\n",
      "3\n",
      "saad zaghlul\n",
      "3\n",
      "saad zaghlul\n",
      "4\n",
      "berlioz 's grande symphonie funèbre et triomphale\n",
      "4\n",
      "dress rehearsal of berlioz 's grande symphonie funèbre et triomphale\n",
      "Iter 12000, start_accuracy = 0.4531, end_accuracy = 0.5781, accuracy = 0.3828, train loss = 3.352694\n",
      "Exact Match Score: 0.4141\n",
      "Iter 12000, val_start_accuracy = 0.1719, val_end_acc = 0.1875, val_acc = 0.1094, val loss = 7.568612\n",
      "0\n",
      "royal irish constabulary\n",
      "0\n",
      "authorities\n",
      "1\n",
      "pennsylvania\n",
      "1\n",
      "pennsylvania also\n",
      "2\n",
      "wood\n",
      "2\n",
      "wood \" and UNK ( UNK ) means \" covered shelter\n",
      "3\n",
      "multan\n",
      "3\n",
      "UNK\n",
      "4\n",
      "never\n",
      "4\n",
      "62\n",
      "Iter 12000, val_start_accuracy = 0.1719, val_end_acc = 0.1875, val_acc = 0.1094, val loss = 7.568612\n",
      "Exact Match Score: 0.1328\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--12000.ckpt\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--12000.ckpt\n",
      "Iter 12200, start_accuracy = 0.5781, end_accuracy = 0.6094, accuracy = 0.4688, train loss = 2.955755\n",
      "0\n",
      "south shore of the island\n",
      "0\n",
      "south shore of the island\n",
      "1\n",
      "burma\n",
      "1\n",
      "burma\n",
      "2\n",
      "gino severini\n",
      "2\n",
      "gino severini\n",
      "3\n",
      "\" it 's my life \"\n",
      "3\n",
      "\n",
      "4\n",
      "dicondylia\n",
      "4\n",
      "apterygota\n",
      "Iter 12200, start_accuracy = 0.5781, end_accuracy = 0.6094, accuracy = 0.4688, train loss = 2.955755\n",
      "Exact Match Score: 0.4844\n",
      "Iter 12400, start_accuracy = 0.4766, end_accuracy = 0.5547, accuracy = 0.3750, train loss = 3.342433\n",
      "0\n",
      "their own private systems of canon law\n",
      "0\n",
      "canon law\n",
      "1\n",
      "intelligence reform and terrorism prevention act\n",
      "1\n",
      "intelligence reform and terrorism prevention act\n",
      "2\n",
      "use of antibiotics , early diagnosis , and other factors\n",
      "2\n",
      "42 % drop\n",
      "3\n",
      "20th-safest city in america\n",
      "3\n",
      "20th-safest city\n",
      "4\n",
      "united states census bureau\n",
      "4\n",
      "united states census bureau\n",
      "Iter 12400, start_accuracy = 0.4766, end_accuracy = 0.5547, accuracy = 0.3750, train loss = 3.342433\n",
      "Exact Match Score: 0.4219\n",
      "Iter 12600, start_accuracy = 0.5078, end_accuracy = 0.5000, accuracy = 0.3828, train loss = 3.436230\n",
      "0\n",
      "standard catalan\n",
      "0\n",
      "standard catalan\n",
      "1\n",
      "austria , france , germany , luxemburg , portugal , spain and south africa\n",
      "1\n",
      "austria , france , germany , luxemburg , portugal , spain and south africa\n",
      "2\n",
      "susa in persia\n",
      "2\n",
      "egypt\n",
      "3\n",
      "pregnancy .\n",
      "3\n",
      "twelve months for both males and females , although this can be delayed until up to two years old for some large breeds . this is the time at which female dogs will have their first estrous cycle . they will experience subsequent estrous cycles biannually , during which the body prepares for pregnancy\n",
      "4\n",
      "hundreds\n",
      "4\n",
      "hundreds\n",
      "Iter 12600, start_accuracy = 0.5078, end_accuracy = 0.5000, accuracy = 0.3828, train loss = 3.436230\n",
      "Exact Match Score: 0.4141\n",
      "Iter 12800, start_accuracy = 0.5234, end_accuracy = 0.5859, accuracy = 0.4297, train loss = 3.064582\n",
      "0\n",
      "the \" bible belt \"\n",
      "0\n",
      "bible belt\n",
      "1\n",
      "november 7 , 1917\n",
      "1\n",
      "november 7 , 1917\n",
      "2\n",
      "private television networks\n",
      "2\n",
      "private television networks\n",
      "3\n",
      "harvard medical school\n",
      "3\n",
      "harvard medical school\n",
      "4\n",
      "rous\n",
      "4\n",
      "aeronautical , african , alexander cozens ( art ) , amnesty , archeological , architectural , astronomy , banks ( conservation ) , caledonian , cheese , classical , comedy , cosmopolitan , debating , design , entrepreneurship , geographical , henry fielding , hispanic , history , keynes ( economics ) , law , literary , mathematical , medical , middle eastern , model united nations , modern languages , oriental , orwell ( left-wing ) , simeon ( christian ) , parry ( music ) , photographic , political , praed ( poetry ) , rock ( music ) , rous\n",
      "Iter 12800, start_accuracy = 0.5234, end_accuracy = 0.5859, accuracy = 0.4297, train loss = 3.064582\n",
      "Exact Match Score: 0.4688\n",
      "Iter 13000, start_accuracy = 0.5312, end_accuracy = 0.5469, accuracy = 0.3906, train loss = 3.103943\n",
      "0\n",
      "zipingpu dam\n",
      "0\n",
      "zipingpu dam\n",
      "1\n",
      "senegal\n",
      "1\n",
      "senegal\n",
      "2\n",
      "new signings\n",
      "2\n",
      "malcolm macdonald and pat jennings\n",
      "3\n",
      "stan lee\n",
      "3\n",
      "stan lee\n",
      "4\n",
      "2010\n",
      "4\n",
      "\n",
      "Iter 13000, start_accuracy = 0.5312, end_accuracy = 0.5469, accuracy = 0.3906, train loss = 3.103943\n",
      "Exact Match Score: 0.4375\n",
      "Iter 13000, val_start_accuracy = 0.1719, val_end_acc = 0.2188, val_acc = 0.1328, val loss = 7.538536\n",
      "0\n",
      "more than 80 %\n",
      "0\n",
      "80 %\n",
      "1\n",
      "the scientific revolution\n",
      "1\n",
      "UNK nature and knowledge\n",
      "2\n",
      "large production companies\n",
      "2\n",
      "large\n",
      "3\n",
      "UNK\n",
      "3\n",
      "UNK UNK\n",
      "4\n",
      "bronze age\n",
      "4\n",
      "bronze age UNK mosaics have been found at UNK ; mosaics of the 4th century bc are found in the macedonian UNK of UNK , and the UNK bc mosaic of the beauty of UNK discovered in UNK , albania in 1916 , is an early UNK example ; the greek UNK style was mostly formed in the 3rd century bc . mythological subjects , or scenes of hunting or other pursuits of the wealthy , were popular as the centrepieces of a larger geometric design , with strongly emphasized borders . pliny the elder mentions the artist sosus of pergamon by name\n",
      "Iter 13000, val_start_accuracy = 0.1719, val_end_acc = 0.2188, val_acc = 0.1328, val loss = 7.538536\n",
      "Exact Match Score: 0.1719\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--13000.ckpt\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--13000.ckpt\n",
      "Iter 13200, start_accuracy = 0.6328, end_accuracy = 0.6172, accuracy = 0.4688, train loss = 2.844094\n",
      "0\n",
      "4\n",
      "0\n",
      "4\n",
      "1\n",
      "u.s. news and world report 's \" great schools , great prices \" lineup\n",
      "1\n",
      "the princeton review has ranked byu the best value for college in 2007 , and its library is consistently ranked in the nation 's top ten — no . 1 in 2004 and no . 4 in 2007 . byu is also ranked no . 19 in the u.s. news and world report 's \" great schools , great prices \" lineup , and no . 12 in lowest student-incurred debt . due in part to the school 's emphasis on undergraduate research , in rankings for 2008-2009 , byu was ranked no . 10 nationally for the number of students who go on to earn phds , no . 1 nationally for students who go on to dental school , no . 6 nationally for students who go on to law school , and no . 10 nationally for students who go on to medical school . byu\n",
      "2\n",
      "roman empire\n",
      "2\n",
      "roman empire\n",
      "3\n",
      "ibiza\n",
      "3\n",
      "ibiza\n",
      "4\n",
      "kane kramer\n",
      "4\n",
      "kane kramer\n",
      "Iter 13200, start_accuracy = 0.6328, end_accuracy = 0.6172, accuracy = 0.4688, train loss = 2.844094\n",
      "Exact Match Score: 0.5000\n",
      "Iter 13400, start_accuracy = 0.6172, end_accuracy = 0.6250, accuracy = 0.4688, train loss = 2.726962\n",
      "0\n",
      "fusion of art and music\n",
      "0\n",
      "a fusion of art and music\n",
      "1\n",
      "6,000\n",
      "1\n",
      "6,000\n",
      "2\n",
      "12,500 years\n",
      "2\n",
      "12,500 years\n",
      "3\n",
      "bourbon\n",
      "3\n",
      "bourbon restoration\n",
      "4\n",
      "african-iraquis\n",
      "4\n",
      "african-iraquis\n",
      "Iter 13400, start_accuracy = 0.6172, end_accuracy = 0.6250, accuracy = 0.4688, train loss = 2.726962\n",
      "Exact Match Score: 0.5312\n",
      "Iter 13600, start_accuracy = 0.6016, end_accuracy = 0.6172, accuracy = 0.4531, train loss = 2.658191\n",
      "0\n",
      "1975\n",
      "0\n",
      "1975\n",
      "1\n",
      "100 w\n",
      "1\n",
      "aluminum rods can be calculated and the efficiency of an antenna using such materials predicted\n",
      "2\n",
      "giovanni tacci porcelli\n",
      "2\n",
      "gasparri summoned him\n",
      "3\n",
      "he possesses every quality that could be desired to render me perfectly happy\n",
      "3\n",
      "me perfectly happy\n",
      "4\n",
      "dreamgirls\n",
      "4\n",
      "dreamgirls\n",
      "Iter 13600, start_accuracy = 0.6016, end_accuracy = 0.6172, accuracy = 0.4531, train loss = 2.658191\n",
      "Exact Match Score: 0.5156\n",
      "Iter 13800, start_accuracy = 0.5859, end_accuracy = 0.5625, accuracy = 0.4297, train loss = 2.955445\n",
      "0\n",
      "in 1932 ,\n",
      "0\n",
      "june 25 , 1910\n",
      "1\n",
      "adventures of asterix\n",
      "1\n",
      "goscinny and uderzo\n",
      "2\n",
      "red\n",
      "2\n",
      "red\n",
      "3\n",
      "a kiln\n",
      "3\n",
      "a kiln\n",
      "4\n",
      "austria , particularly kaunitz , used this situation to their utmost advantage . the now-isolated france\n",
      "4\n",
      "particularly kaunitz\n",
      "Iter 13800, start_accuracy = 0.5859, end_accuracy = 0.5625, accuracy = 0.4297, train loss = 2.955445\n",
      "Exact Match Score: 0.4766\n",
      "Iter 14000, start_accuracy = 0.5781, end_accuracy = 0.5938, accuracy = 0.4219, train loss = 2.795780\n",
      "0\n",
      "computers\n",
      "0\n",
      "information age and computers\n",
      "1\n",
      "cochlea\n",
      "1\n",
      "cochlea of the ear , or pressure-sensitive neurons in the skin\n",
      "2\n",
      "80,000\n",
      "2\n",
      "80,000\n",
      "3\n",
      "unclear strategy\n",
      "3\n",
      "unclear strategy\n",
      "4\n",
      "187th street\n",
      "4\n",
      "187th street\n",
      "Iter 14000, start_accuracy = 0.5781, end_accuracy = 0.5938, accuracy = 0.4219, train loss = 2.795780\n",
      "Exact Match Score: 0.4844\n",
      "Iter 14000, val_start_accuracy = 0.2188, val_end_acc = 0.2344, val_acc = 0.1328, val loss = 7.624809\n",
      "0\n",
      "captain\n",
      "0\n",
      "four days\n",
      "1\n",
      "number of tags issued\n",
      "1\n",
      "hunting big game typically requires a \" tag \" for each animal harvested . tags must be purchased in addition to the hunting license , and the number of tags issued to an individual is typically limited . in cases where there are more prospective hunters than the quota for that species , tags are usually assigned by lottery . tags may be further restricted to a specific area , or wildlife management unit . hunting migratory waterfowl requires a duck stamp from the fish and wildlife service\n",
      "2\n",
      "strict UNK\n",
      "2\n",
      "UNK\n",
      "3\n",
      "the harder policing agencies work to produce security , the greater are feelings of insecurity\n",
      "3\n",
      "UNK processes in transnational policing deeply problematic . he argues that transnational police information circuits help to \" compose the panic scenes of the UNK society \" . the paradoxical effect is that , the harder policing agencies work to produce security , the greater are feelings of insecurity\n",
      "4\n",
      "house-to-house preaching\n",
      "4\n",
      "christ 's UNK kingdom\n",
      "Iter 14000, val_start_accuracy = 0.2188, val_end_acc = 0.2344, val_acc = 0.1328, val loss = 7.624809\n",
      "Exact Match Score: 0.1562\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--14000.ckpt\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--14000.ckpt\n",
      "Iter 14200, start_accuracy = 0.6172, end_accuracy = 0.6016, accuracy = 0.4531, train loss = 2.755088\n",
      "0\n",
      "around 1800 bc\n",
      "0\n",
      "1800 bc\n",
      "1\n",
      "linear and reciprocal\n",
      "1\n",
      "linear and reciprocal\n",
      "2\n",
      "two\n",
      "2\n",
      "two\n",
      "3\n",
      "ballhof theatre\n",
      "3\n",
      "ballhof theatre\n",
      "4\n",
      "defence of british air space\n",
      "4\n",
      "british air space and the failure of the luftwaffe to achieve air superiority . however , dowding had spent so much effort preparing day fighter defences\n",
      "Iter 14200, start_accuracy = 0.6172, end_accuracy = 0.6016, accuracy = 0.4531, train loss = 2.755088\n",
      "Exact Match Score: 0.4844\n",
      "Iter 14400, start_accuracy = 0.5469, end_accuracy = 0.5625, accuracy = 0.4141, train loss = 2.929978\n",
      "0\n",
      "poetry\n",
      "0\n",
      "martyn sanderson . another novel sons for the return home had also been made into a feature film in 1979 , directed by paul maunder . the late john kneubuhl , born in american samoa , was an accomplished playwright and screenwriter and writer . sia figiel won the 1997 commonwealth writers ' prize for fiction in the south-east asia/south pacific region with her novel \" where we once belonged \" . momoe von reiche is an internationally recognised poet and artist . tusiata avia is a performance\n",
      "1\n",
      "the bureau of topography\n",
      "1\n",
      "bureau\n",
      "2\n",
      "1992\n",
      "2\n",
      "1992\n",
      "3\n",
      "systemic fairness for minorities\n",
      "3\n",
      "minorities\n",
      "4\n",
      "syllabus\n",
      "4\n",
      "the daily northwestern\n",
      "Iter 14400, start_accuracy = 0.5469, end_accuracy = 0.5625, accuracy = 0.4141, train loss = 2.929978\n",
      "Exact Match Score: 0.4531\n",
      "Iter 14600, start_accuracy = 0.5234, end_accuracy = 0.5859, accuracy = 0.3984, train loss = 3.267101\n",
      "0\n",
      "in india , the matches are broadcast live on star sports . in china\n",
      "0\n",
      "sportsnet and tsn\n",
      "1\n",
      "mudflat\n",
      "1\n",
      "mudflat\n",
      "2\n",
      "laidlines\n",
      "2\n",
      "deckle edges \" , or rough and feathery borders\n",
      "3\n",
      "above 70 %\n",
      "3\n",
      "70 %\n",
      "4\n",
      "assertions and the facts to which they refer\n",
      "4\n",
      "assertions and the facts\n",
      "Iter 14600, start_accuracy = 0.5234, end_accuracy = 0.5859, accuracy = 0.3984, train loss = 3.267101\n",
      "Exact Match Score: 0.4453\n",
      "Iter 14800, start_accuracy = 0.6016, end_accuracy = 0.6172, accuracy = 0.4609, train loss = 2.844239\n",
      "0\n",
      "a\n",
      "0\n",
      "a\n",
      "1\n",
      "science and technology\n",
      "1\n",
      "science and technology\n",
      "2\n",
      "100 million\n",
      "2\n",
      "100 million\n",
      "3\n",
      "mixture between formerly distinct populations\n",
      "3\n",
      "primordial , natural , enduring and distinct\n",
      "4\n",
      "the british\n",
      "4\n",
      "the british\n",
      "Iter 14800, start_accuracy = 0.6016, end_accuracy = 0.6172, accuracy = 0.4609, train loss = 2.844239\n",
      "Exact Match Score: 0.4844\n",
      "Iter 15000, start_accuracy = 0.5547, end_accuracy = 0.6094, accuracy = 0.4766, train loss = 2.852221\n",
      "0\n",
      "absolute space exists\n",
      "0\n",
      "absolute space exists\n",
      "1\n",
      "1789\n",
      "1\n",
      "1789\n",
      "2\n",
      "users and small businesses\n",
      "2\n",
      "users and small businesses\n",
      "3\n",
      "june 2013\n",
      "3\n",
      "june 2013\n",
      "4\n",
      "1–3 %\n",
      "4\n",
      "those\n",
      "Iter 15000, start_accuracy = 0.5547, end_accuracy = 0.6094, accuracy = 0.4766, train loss = 2.852221\n",
      "Exact Match Score: 0.5469\n",
      "Iter 15000, val_start_accuracy = 0.2656, val_end_acc = 0.2734, val_acc = 0.2188, val loss = 7.677537\n",
      "0\n",
      "three\n",
      "0\n",
      "three\n",
      "1\n",
      "mosaics\n",
      "1\n",
      "1884\n",
      "2\n",
      "industrial output\n",
      "2\n",
      "2005 levels out of all european union members\n",
      "3\n",
      "robin hood\n",
      "3\n",
      "hunt the king 's deer\n",
      "4\n",
      "roger ii\n",
      "4\n",
      "obviously executed\n",
      "Iter 15000, val_start_accuracy = 0.2656, val_end_acc = 0.2734, val_acc = 0.2188, val loss = 7.677537\n",
      "Exact Match Score: 0.2188\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--15000.ckpt\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--15000.ckpt\n",
      "Iter 15200, start_accuracy = 0.5781, end_accuracy = 0.5625, accuracy = 0.4219, train loss = 3.267979\n",
      "0\n",
      "her majesty 's most honourable privy council\n",
      "0\n",
      "prime minister of canada\n",
      "1\n",
      "arabic\n",
      "1\n",
      "arabic\n",
      "2\n",
      "fever and nausea\n",
      "2\n",
      "photodermatitis and anaphylaxis\n",
      "3\n",
      "the top 10 to 15 in the nation\n",
      "3\n",
      "most representative university\n",
      "4\n",
      "1930s\n",
      "4\n",
      "20th century\n",
      "Iter 15200, start_accuracy = 0.5781, end_accuracy = 0.5625, accuracy = 0.4219, train loss = 3.267979\n",
      "Exact Match Score: 0.4609\n",
      "Iter 15400, start_accuracy = 0.6172, end_accuracy = 0.6484, accuracy = 0.4844, train loss = 2.460377\n",
      "0\n",
      "world war i .\n",
      "0\n",
      "world war i\n",
      "1\n",
      "the eucharist\n",
      "1\n",
      "partake of the eucharist\n",
      "2\n",
      "adam smith\n",
      "2\n",
      "adam smith\n",
      "3\n",
      "robert guiscard\n",
      "3\n",
      "robert guiscard and his army\n",
      "4\n",
      "russians under zakhar chernyshev and pyotr rumyantsev stormed kolberg in pomerania\n",
      "4\n",
      "progress was very slow . the russian army was heavily dependent upon its main magazines in poland , and the prussian army launched several successful raids against them . one of them , led by general platen in september resulted in the loss of 2,000 russians , mostly captured , and the destruction of 5,000 wagons . deprived of men , the prussians had to resort to this new sort of warfare , raiding , to delay the advance of their enemies . nonetheless , at the end of the year , they suffered two critical setbacks . the russians under zakhar chernyshev and pyotr rumyantsev stormed kolberg in pomerania\n",
      "Iter 15400, start_accuracy = 0.6172, end_accuracy = 0.6484, accuracy = 0.4844, train loss = 2.460377\n",
      "Exact Match Score: 0.5234\n",
      "Iter 15600, start_accuracy = 0.6172, end_accuracy = 0.6641, accuracy = 0.4844, train loss = 2.556796\n",
      "0\n",
      "numerical\n",
      "0\n",
      "numerical\n",
      "1\n",
      "moscow\n",
      "1\n",
      "moscow\n",
      "2\n",
      "christians\n",
      "2\n",
      "john xxiii\n",
      "3\n",
      "in a double minority ,\n",
      "3\n",
      "a number of culturally specific support networks\n",
      "4\n",
      "religious and social needs\n",
      "4\n",
      "religious and social needs of the society . examples can be found throughout the middle east , north africa , spain and the indian sub-continent\n",
      "Iter 15600, start_accuracy = 0.6172, end_accuracy = 0.6641, accuracy = 0.4844, train loss = 2.556796\n",
      "Exact Match Score: 0.5234\n",
      "Iter 15800, start_accuracy = 0.6016, end_accuracy = 0.6719, accuracy = 0.4922, train loss = 2.374183\n",
      "0\n",
      "united states has no de jure official language , english is the dominant language\n",
      "0\n",
      "although the united states has no de jure official language , english is the dominant language of business\n",
      "1\n",
      "mahayana sutras\n",
      "1\n",
      "mahayana sutras\n",
      "2\n",
      "six\n",
      "2\n",
      "six\n",
      "3\n",
      "twenty\n",
      "3\n",
      "twenty\n",
      "4\n",
      "american english\n",
      "4\n",
      "british\n",
      "Iter 15800, start_accuracy = 0.6016, end_accuracy = 0.6719, accuracy = 0.4922, train loss = 2.374183\n",
      "Exact Match Score: 0.5469\n",
      "Iter 16000, start_accuracy = 0.6641, end_accuracy = 0.6719, accuracy = 0.5078, train loss = 2.055294\n",
      "0\n",
      "longleaf pine\n",
      "0\n",
      "longleaf\n",
      "1\n",
      "24,000\n",
      "1\n",
      "24,000\n",
      "2\n",
      "home\n",
      "2\n",
      "home\n",
      "3\n",
      "most domestic ducks are too heavy to fly\n",
      "3\n",
      "drakes\n",
      "4\n",
      "does not have to be pinned or involved in the decision to lose the championship .\n",
      "4\n",
      "\n",
      "Iter 16000, start_accuracy = 0.6641, end_accuracy = 0.6719, accuracy = 0.5078, train loss = 2.055294\n",
      "Exact Match Score: 0.5625\n",
      "Iter 16000, val_start_accuracy = 0.1328, val_end_acc = 0.1719, val_acc = 0.1172, val loss = 9.983021\n",
      "0\n",
      "preservation of wildlife habitat\n",
      "0\n",
      "many companies specialising in hunting equipment or speciality tourism . many different technologies have been created to assist hunters , even including iphone applications . today 's hunters come from a broad range of economic , social , and cultural backgrounds\n",
      "1\n",
      "executive officer\n",
      "1\n",
      "general conner\n",
      "2\n",
      "419 to 359 ma .\n",
      "2\n",
      "collision\n",
      "3\n",
      "1980s\n",
      "3\n",
      "1949 and the 1980s\n",
      "4\n",
      "gives the maker time to rework\n",
      "4\n",
      "it gives the maker time to rework areas\n",
      "Iter 16000, val_start_accuracy = 0.1328, val_end_acc = 0.1719, val_acc = 0.1172, val loss = 9.983021\n",
      "Exact Match Score: 0.1406\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--16000.ckpt\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--16000.ckpt\n",
      "Iter 16200, start_accuracy = 0.6094, end_accuracy = 0.6641, accuracy = 0.5000, train loss = 2.623524\n",
      "0\n",
      "1.16 %\n",
      "0\n",
      "1.16 %\n",
      "1\n",
      "the yongle emperor\n",
      "1\n",
      "the yongle emperor\n",
      "2\n",
      "holes existing in the p-region\n",
      "2\n",
      "holes existing\n",
      "3\n",
      "mark kamins\n",
      "3\n",
      "author carol gnojewski\n",
      "4\n",
      "metaphysics\n",
      "4\n",
      "metaphysics and physics\n",
      "Iter 16200, start_accuracy = 0.6094, end_accuracy = 0.6641, accuracy = 0.5000, train loss = 2.623524\n",
      "Exact Match Score: 0.5156\n",
      "Iter 16400, start_accuracy = 0.5781, end_accuracy = 0.6016, accuracy = 0.4766, train loss = 2.854188\n",
      "0\n",
      "michael foot\n",
      "0\n",
      "michael foot\n",
      "1\n",
      "hot-summer mediterranean climate ( köppen climate classification : csa ) .\n",
      "1\n",
      "cool and wet\n",
      "2\n",
      "the benedictines\n",
      "2\n",
      "the benedictines\n",
      "3\n",
      "centre of bern\n",
      "3\n",
      "the centre of bern\n",
      "4\n",
      "instrumental in transmitting the ideas of western romantic nationalism and philhellenism , which together with the conception of hellenism ,\n",
      "4\n",
      "greek enlightenment\n",
      "Iter 16400, start_accuracy = 0.5781, end_accuracy = 0.6016, accuracy = 0.4766, train loss = 2.854188\n",
      "Exact Match Score: 0.5547\n",
      "Iter 16600, start_accuracy = 0.6797, end_accuracy = 0.7109, accuracy = 0.5625, train loss = 2.194650\n",
      "0\n",
      "israel\n",
      "0\n",
      "israel\n",
      "1\n",
      "ulster\n",
      "1\n",
      "ulster\n",
      "2\n",
      "berlin\n",
      "2\n",
      "berlin\n",
      "3\n",
      "lourdes maria ciccone leon\n",
      "3\n",
      "lourdes maria ciccone leon\n",
      "4\n",
      "the altitude and size of the range\n",
      "4\n",
      "altitude and size of the range\n",
      "Iter 16600, start_accuracy = 0.6797, end_accuracy = 0.7109, accuracy = 0.5625, train loss = 2.194650\n",
      "Exact Match Score: 0.5938\n",
      "Iter 16800, start_accuracy = 0.6562, end_accuracy = 0.7031, accuracy = 0.5703, train loss = 2.414567\n",
      "0\n",
      "bipolar junction transistor\n",
      "0\n",
      "bipolar junction transistor\n",
      "1\n",
      "morningside park\n",
      "1\n",
      "morningside park\n",
      "2\n",
      "prince oleg\n",
      "2\n",
      "prince oleg\n",
      "3\n",
      "first south west\n",
      "3\n",
      "\n",
      "4\n",
      "france\n",
      "4\n",
      "france\n",
      "Iter 16800, start_accuracy = 0.6562, end_accuracy = 0.7031, accuracy = 0.5703, train loss = 2.414567\n",
      "Exact Match Score: 0.6328\n",
      "Iter 17000, start_accuracy = 0.6406, end_accuracy = 0.6719, accuracy = 0.5000, train loss = 2.343698\n",
      "0\n",
      "artemis\n",
      "0\n",
      "sister artemis\n",
      "1\n",
      "60 out of 100\n",
      "1\n",
      "60 out of 100\n",
      "2\n",
      "address limitations inherent in ucr data\n",
      "2\n",
      "address limitations inherent in ucr data\n",
      "3\n",
      "canton of bern\n",
      "3\n",
      "bern\n",
      "4\n",
      "kelly clarkson\n",
      "4\n",
      "justin guarini , one of the early favorites , and kelly clarkson\n",
      "Iter 17000, start_accuracy = 0.6406, end_accuracy = 0.6719, accuracy = 0.5000, train loss = 2.343698\n",
      "Exact Match Score: 0.5547\n",
      "Iter 17000, val_start_accuracy = 0.1484, val_end_acc = 0.2266, val_acc = 0.0938, val loss = 8.375496\n",
      "0\n",
      "pamboukjian\n",
      "0\n",
      "richard hagopian\n",
      "1\n",
      "quakers\n",
      "1\n",
      "peter stuyvesant\n",
      "2\n",
      "abuse allegations\n",
      "2\n",
      "of\n",
      "3\n",
      "legal precedence\n",
      "3\n",
      "issue\n",
      "4\n",
      "rhea UNK\n",
      "4\n",
      "rhea UNK\n",
      "Iter 17000, val_start_accuracy = 0.1484, val_end_acc = 0.2266, val_acc = 0.0938, val loss = 8.375496\n",
      "Exact Match Score: 0.1250\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--17000.ckpt\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--17000.ckpt\n",
      "Iter 17200, start_accuracy = 0.6328, end_accuracy = 0.7031, accuracy = 0.5234, train loss = 2.444872\n",
      "0\n",
      "agustawestland manufactures helicopters in yeovil , and normalair garratt , builder of aircraft oxygen systems\n",
      "0\n",
      "yeovil , and normalair garratt\n",
      "1\n",
      "3rd\n",
      "1\n",
      "3rd century\n",
      "2\n",
      "conan the barbarian\n",
      "2\n",
      "conan the barbarian\n",
      "3\n",
      "[ æ ] , as in english mat\n",
      "3\n",
      "alphabet\n",
      "4\n",
      "germany\n",
      "4\n",
      "germany\n",
      "Iter 17200, start_accuracy = 0.6328, end_accuracy = 0.7031, accuracy = 0.5234, train loss = 2.444872\n",
      "Exact Match Score: 0.5703\n",
      "Iter 17400, start_accuracy = 0.6016, end_accuracy = 0.6484, accuracy = 0.4844, train loss = 2.440304\n",
      "0\n",
      "of\n",
      "0\n",
      "of\n",
      "1\n",
      "postwar\n",
      "1\n",
      "latino population\n",
      "2\n",
      "partus sequitur ventrem\n",
      "2\n",
      "\n",
      "3\n",
      "def leppard\n",
      "3\n",
      "the\n",
      "4\n",
      "in 1951\n",
      "4\n",
      "1951\n",
      "Iter 17400, start_accuracy = 0.6016, end_accuracy = 0.6484, accuracy = 0.4844, train loss = 2.440304\n",
      "Exact Match Score: 0.5625\n",
      "Iter 17600, start_accuracy = 0.6406, end_accuracy = 0.7344, accuracy = 0.5625, train loss = 2.331035\n",
      "0\n",
      "la femme au cheval ( woman with a horse ) 1911-1912\n",
      "0\n",
      "national gallery of denmark\n",
      "1\n",
      "architectural material\n",
      "1\n",
      "ancient times as a durable , corrosion resistant , and weatherproof architectural material\n",
      "2\n",
      "the esteem and prestige of the senators\n",
      "2\n",
      "the esteem and prestige of the senators\n",
      "3\n",
      "mary mcdonough murphy\n",
      "3\n",
      "author mary mcdonough murphy\n",
      "4\n",
      "chat programs\n",
      "4\n",
      "programs\n",
      "Iter 17600, start_accuracy = 0.6406, end_accuracy = 0.7344, accuracy = 0.5625, train loss = 2.331035\n",
      "Exact Match Score: 0.6094\n",
      "Iter 17800, start_accuracy = 0.7422, end_accuracy = 0.7734, accuracy = 0.6406, train loss = 1.859657\n",
      "0\n",
      "the industrial revolution\n",
      "0\n",
      "industrial revolution\n",
      "1\n",
      "april\n",
      "1\n",
      "april\n",
      "2\n",
      "it is seldom\n",
      "2\n",
      "\n",
      "3\n",
      "founding members\n",
      "3\n",
      "15 founding members of the club\n",
      "4\n",
      "zhejiang province\n",
      "4\n",
      "zhejiang province\n",
      "Iter 17800, start_accuracy = 0.7422, end_accuracy = 0.7734, accuracy = 0.6406, train loss = 1.859657\n",
      "Exact Match Score: 0.6797\n",
      "Iter 18000, start_accuracy = 0.5703, end_accuracy = 0.6562, accuracy = 0.4375, train loss = 2.446054\n",
      "0\n",
      "sine , cosine ,\n",
      "0\n",
      "sine , cosine , etc. , and square roots\n",
      "1\n",
      "sunday of orthodoxy\n",
      "1\n",
      "sunday of orthodoxy directly\n",
      "2\n",
      "sensations of exhilaration and palpitations\n",
      "2\n",
      "exhilaration and palpitations\n",
      "3\n",
      "embankment\n",
      "3\n",
      "surviving embankment\n",
      "4\n",
      "46 %\n",
      "4\n",
      "46 %\n",
      "Iter 18000, start_accuracy = 0.5703, end_accuracy = 0.6562, accuracy = 0.4375, train loss = 2.446054\n",
      "Exact Match Score: 0.4844\n",
      "Iter 18000, val_start_accuracy = 0.1953, val_end_acc = 0.1797, val_acc = 0.1406, val loss = 8.830352\n",
      "0\n",
      "borough of the bronx\n",
      "0\n",
      "bronx\n",
      "1\n",
      "the appointment of commissioners of police for scotland in 1714\n",
      "1\n",
      "police \" recorded\n",
      "2\n",
      "UNK\n",
      "2\n",
      "pigs invasion\n",
      "3\n",
      "various reformed denominations\n",
      "3\n",
      "dissenters and nonconformists\n",
      "4\n",
      "the UNK\n",
      "4\n",
      "east 132nd street\n",
      "Iter 18000, val_start_accuracy = 0.1953, val_end_acc = 0.1797, val_acc = 0.1406, val loss = 8.830352\n",
      "Exact Match Score: 0.1562\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--18000.ckpt\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--18000.ckpt\n",
      "Iter 18200, start_accuracy = 0.6406, end_accuracy = 0.6953, accuracy = 0.5078, train loss = 2.431810\n",
      "0\n",
      "the daily telegraph\n",
      "0\n",
      "daily telegraph\n",
      "1\n",
      "skin-lighting\n",
      "1\n",
      "skin-lighting mutation\n",
      "2\n",
      "the son to be \" like the father \"\n",
      "2\n",
      "like the father\n",
      "3\n",
      "1885\n",
      "3\n",
      "1885\n",
      "4\n",
      "pleyel\n",
      "4\n",
      "pleyel piano sent\n",
      "Iter 18200, start_accuracy = 0.6406, end_accuracy = 0.6953, accuracy = 0.5078, train loss = 2.431810\n",
      "Exact Match Score: 0.5625\n",
      "Iter 18400, start_accuracy = 0.6797, end_accuracy = 0.6641, accuracy = 0.5391, train loss = 2.270241\n",
      "0\n",
      "corruption and political repression\n",
      "0\n",
      "corruption and political repression\n",
      "1\n",
      "2007\n",
      "1\n",
      "april 2007\n",
      "2\n",
      "perching birds\n",
      "2\n",
      "perching birds\n",
      "3\n",
      "11\n",
      "3\n",
      "11\n",
      "4\n",
      "12 april 2012\n",
      "4\n",
      "12 april 2012\n",
      "Iter 18400, start_accuracy = 0.6797, end_accuracy = 0.6641, accuracy = 0.5391, train loss = 2.270241\n",
      "Exact Match Score: 0.5703\n",
      "Iter 18600, start_accuracy = 0.7344, end_accuracy = 0.8047, accuracy = 0.6328, train loss = 1.730867\n",
      "0\n",
      "traffic pattern\n",
      "0\n",
      "traffic pattern\n",
      "1\n",
      "guano\n",
      "1\n",
      "guano\n",
      "2\n",
      "german\n",
      "2\n",
      "german\n",
      "3\n",
      "combining it with one or more other metals or non-metals\n",
      "3\n",
      "metals\n",
      "4\n",
      "traditionally free people of color in louisiana\n",
      "4\n",
      "traditionally free people of color\n",
      "Iter 18600, start_accuracy = 0.7344, end_accuracy = 0.8047, accuracy = 0.6328, train loss = 1.730867\n",
      "Exact Match Score: 0.6953\n",
      "Iter 18800, start_accuracy = 0.6562, end_accuracy = 0.6875, accuracy = 0.5078, train loss = 2.159742\n",
      "0\n",
      "american troops\n",
      "0\n",
      "american troops\n",
      "1\n",
      "800,000\n",
      "1\n",
      "800,000\n",
      "2\n",
      "three and a quarter years\n",
      "2\n",
      "quarter years\n",
      "3\n",
      "paris\n",
      "3\n",
      "university of paris\n",
      "4\n",
      "2,145 ft\n",
      "4\n",
      "about 2,000 ft\n",
      "Iter 18800, start_accuracy = 0.6562, end_accuracy = 0.6875, accuracy = 0.5078, train loss = 2.159742\n",
      "Exact Match Score: 0.5391\n",
      "Iter 19000, start_accuracy = 0.6719, end_accuracy = 0.7344, accuracy = 0.5547, train loss = 2.215510\n",
      "0\n",
      "nico frijda\n",
      "0\n",
      "nico frijda\n",
      "1\n",
      "david hernandez\n",
      "1\n",
      "contestant david hernandez\n",
      "2\n",
      "about a millennium after their composition\n",
      "2\n",
      "their composition\n",
      "3\n",
      "artificial sea-level waterway\n",
      "3\n",
      "artificial sea-level waterway\n",
      "4\n",
      "90.4\n",
      "4\n",
      "90.4 %\n",
      "Iter 19000, start_accuracy = 0.6719, end_accuracy = 0.7344, accuracy = 0.5547, train loss = 2.215510\n",
      "Exact Match Score: 0.6172\n",
      "Iter 19000, val_start_accuracy = 0.2109, val_end_acc = 0.2734, val_acc = 0.1641, val loss = 8.994303\n",
      "0\n",
      "third\n",
      "0\n",
      "\n",
      "1\n",
      "cost and application\n",
      "1\n",
      "\n",
      "2\n",
      "john f. kennedy , james monroe , taft , theodore roosevelt , adlai stevenson , evander childs , christopher columbus , morris , walton , and south bronx high schools\n",
      "2\n",
      "the city\n",
      "3\n",
      "1320 ad\n",
      "3\n",
      "sixteenth century UNK\n",
      "4\n",
      "guiding legislation favored by his party through the house , or in resisting those programs of the other party that are considered undesirable by his own party\n",
      "4\n",
      "devising and implementing his party 's strategy on the floor with respect to promoting or opposing legislation . he is kept constantly informed as to the status of legislative business and as to the sentiment of his party respecting particular legislation under consideration . such information is derived in part from the floor leader 's contacts\n",
      "Iter 19000, val_start_accuracy = 0.2109, val_end_acc = 0.2734, val_acc = 0.1641, val loss = 8.994303\n",
      "Exact Match Score: 0.1953\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--19000.ckpt\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--19000.ckpt\n",
      "Iter 19200, start_accuracy = 0.6797, end_accuracy = 0.7422, accuracy = 0.5625, train loss = 2.016523\n",
      "0\n",
      "grand duchy of lithuania\n",
      "0\n",
      "multicultural and multilingual grand duchy of lithuania\n",
      "1\n",
      "he could not play for a club associated with francisco franco\n",
      "1\n",
      "real madrid because he could not play for a club associated with francisco franco\n",
      "2\n",
      "kyoto prize in arts and philosophy\n",
      "2\n",
      "the kyoto prize in arts and philosophy\n",
      "3\n",
      "hubei and hunan\n",
      "3\n",
      "hubei and hunan provinces\n",
      "4\n",
      "winning the big game\n",
      "4\n",
      "winning the big game\n",
      "Iter 19200, start_accuracy = 0.6797, end_accuracy = 0.7422, accuracy = 0.5625, train loss = 2.016523\n",
      "Exact Match Score: 0.6016\n",
      "Iter 19400, start_accuracy = 0.6641, end_accuracy = 0.6719, accuracy = 0.5312, train loss = 2.140049\n",
      "0\n",
      "dutch\n",
      "0\n",
      "dutch synchronically\n",
      "1\n",
      "19th century\n",
      "1\n",
      "19th century\n",
      "2\n",
      "ramsgate\n",
      "2\n",
      "ramsgate\n",
      "3\n",
      "renewable energy sources\n",
      "3\n",
      "renewable energy sources\n",
      "4\n",
      "the treaty of campo formio\n",
      "4\n",
      "campo formio\n",
      "Iter 19400, start_accuracy = 0.6641, end_accuracy = 0.6719, accuracy = 0.5312, train loss = 2.140049\n",
      "Exact Match Score: 0.5859\n",
      "Iter 19600, start_accuracy = 0.6250, end_accuracy = 0.6719, accuracy = 0.5000, train loss = 2.347919\n",
      "0\n",
      "french trapper toussaint charbonneau\n",
      "0\n",
      "the indigenous american woman sacagawea\n",
      "1\n",
      "st columba 's , cambridge ( founded in 1879 ) , and st columba 's , oxford\n",
      "1\n",
      "congregations , st columba 's , cambridge ( founded in 1879 ) , and st columba 's , oxford\n",
      "2\n",
      "will champion\n",
      "2\n",
      "r'n'b singer craig david , coldplay drummer will champion\n",
      "3\n",
      "wisdom\n",
      "3\n",
      "wisdom and understanding enhance one\n",
      "4\n",
      "terminal 2\n",
      "4\n",
      "terminal 2\n",
      "Iter 19600, start_accuracy = 0.6250, end_accuracy = 0.6719, accuracy = 0.5000, train loss = 2.347919\n",
      "Exact Match Score: 0.5469\n",
      "Iter 19800, start_accuracy = 0.6953, end_accuracy = 0.7109, accuracy = 0.5391, train loss = 2.036687\n",
      "0\n",
      "enormous compression of trapped air\n",
      "0\n",
      "gansu\n",
      "1\n",
      "hymns\n",
      "1\n",
      "hymns and verses\n",
      "2\n",
      "syria\n",
      "2\n",
      "syria\n",
      "3\n",
      "1,200 mi\n",
      "3\n",
      "1,200\n",
      "4\n",
      "drones\n",
      "4\n",
      "drones\n",
      "Iter 19800, start_accuracy = 0.6953, end_accuracy = 0.7109, accuracy = 0.5391, train loss = 2.036687\n",
      "Exact Match Score: 0.6094\n",
      "Iter 20000, start_accuracy = 0.6484, end_accuracy = 0.6484, accuracy = 0.4766, train loss = 2.374797\n",
      "0\n",
      "10 to 30 cells thick\n",
      "0\n",
      "the dermis\n",
      "1\n",
      "nbc\n",
      "1\n",
      "nbc contract\n",
      "2\n",
      "1019\n",
      "2\n",
      "1019\n",
      "3\n",
      "the rule of any human leader\n",
      "3\n",
      "any human leader\n",
      "4\n",
      "less than 20 %\n",
      "4\n",
      "less than 20 %\n",
      "Iter 20000, start_accuracy = 0.6484, end_accuracy = 0.6484, accuracy = 0.4766, train loss = 2.374797\n",
      "Exact Match Score: 0.5234\n",
      "Iter 20000, val_start_accuracy = 0.1562, val_end_acc = 0.2109, val_acc = 0.1250, val loss = 9.469135\n",
      "0\n",
      "managing populations\n",
      "0\n",
      "by a combination habitat and food availability\n",
      "1\n",
      "thomas jefferson\n",
      "1\n",
      "\n",
      "2\n",
      "historian\n",
      "2\n",
      "keep the spiritual and secular worlds apart\n",
      "3\n",
      "bread and wine\n",
      "3\n",
      "christ and his body and blood\n",
      "4\n",
      "19th\n",
      "4\n",
      "19th century\n",
      "Iter 20000, val_start_accuracy = 0.1562, val_end_acc = 0.2109, val_acc = 0.1250, val loss = 9.469135\n",
      "Exact Match Score: 0.1406\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--20000.ckpt\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--20000.ckpt\n",
      "Iter 20200, start_accuracy = 0.6484, end_accuracy = 0.7188, accuracy = 0.5312, train loss = 2.144459\n",
      "0\n",
      "forwards and in reverse\n",
      "0\n",
      "forwards and in reverse\n",
      "1\n",
      "rhode island\n",
      "1\n",
      "rhode island\n",
      "2\n",
      "dj toomp\n",
      "2\n",
      "jon brion for late registration , dj toomp\n",
      "3\n",
      "ringing church bells\n",
      "3\n",
      "ringing church bells\n",
      "4\n",
      "commercially unsuccessful\n",
      "4\n",
      "four separate sound signals\n",
      "Iter 20200, start_accuracy = 0.6484, end_accuracy = 0.7188, accuracy = 0.5312, train loss = 2.144459\n",
      "Exact Match Score: 0.5703\n",
      "Iter 20400, start_accuracy = 0.7656, end_accuracy = 0.8125, accuracy = 0.6406, train loss = 1.723786\n",
      "0\n",
      "honeywell , and chevron corporation\n",
      "0\n",
      "chevron corporation\n",
      "1\n",
      "paris\n",
      "1\n",
      "paris\n",
      "2\n",
      "time\n",
      "2\n",
      "time actually\n",
      "3\n",
      "both county and municipal governments\n",
      "3\n",
      "both county and municipal governments\n",
      "4\n",
      "annie besant\n",
      "4\n",
      "annie besant\n",
      "Iter 20400, start_accuracy = 0.7656, end_accuracy = 0.8125, accuracy = 0.6406, train loss = 1.723786\n",
      "Exact Match Score: 0.6953\n",
      "Iter 20600, start_accuracy = 0.6094, end_accuracy = 0.7578, accuracy = 0.5234, train loss = 2.211035\n",
      "0\n",
      "the capital , istanbul , and the regional capital cities\n",
      "0\n",
      "the cuisine of the capital , istanbul , and the regional capital cities\n",
      "1\n",
      "myanmar president declared that all of his nation 's political prisoners will be released by the end of 2013\n",
      "1\n",
      "his nation 's political prisoners will be released by the end of 2013\n",
      "2\n",
      "roman catholic\n",
      "2\n",
      "southern baptist church\n",
      "3\n",
      "process of scholarly research\n",
      "3\n",
      "professor of legal opinions ) took this question , studied it , researched it intensively in the sacred scriptures , in order to find a solution to it . this process of scholarly research\n",
      "4\n",
      "fertility\n",
      "4\n",
      "the daylight\n",
      "Iter 20600, start_accuracy = 0.6094, end_accuracy = 0.7578, accuracy = 0.5234, train loss = 2.211035\n",
      "Exact Match Score: 0.5625\n",
      "Iter 20800, start_accuracy = 0.6094, end_accuracy = 0.7031, accuracy = 0.5000, train loss = 2.372738\n",
      "0\n",
      "56\n",
      "0\n",
      "56\n",
      "1\n",
      "a natural harbour\n",
      "1\n",
      "natural harbour\n",
      "2\n",
      "daoist\n",
      "2\n",
      "large daoist religious\n",
      "3\n",
      "sheffield scientific school\n",
      "3\n",
      "sheffield scientific school\n",
      "4\n",
      "ostrich feathers\n",
      "4\n",
      "ostrich feathers\n",
      "Iter 20800, start_accuracy = 0.6094, end_accuracy = 0.7031, accuracy = 0.5000, train loss = 2.372738\n",
      "Exact Match Score: 0.5547\n",
      "Iter 21000, start_accuracy = 0.7031, end_accuracy = 0.7656, accuracy = 0.6172, train loss = 1.788713\n",
      "0\n",
      "divide into two distinct membrane-bound cells\n",
      "0\n",
      "physically separate the two copies of the genome and divide\n",
      "1\n",
      "accommodation\n",
      "1\n",
      "accommodation\n",
      "2\n",
      "king james bible old testament\n",
      "2\n",
      "king james bible old testament and the last half of the new testament\n",
      "3\n",
      "the office of management and budget ( omb )\n",
      "3\n",
      "office of management and budget\n",
      "4\n",
      "liechtenstein\n",
      "4\n",
      "liechtenstein\n",
      "Iter 21000, start_accuracy = 0.7031, end_accuracy = 0.7656, accuracy = 0.6172, train loss = 1.788713\n",
      "Exact Match Score: 0.6484\n",
      "Iter 21000, val_start_accuracy = 0.1797, val_end_acc = 0.2109, val_acc = 0.1484, val loss = 10.017295\n",
      "0\n",
      "minority leader\n",
      "0\n",
      "minority whip\n",
      "1\n",
      "allowable fractions and treatments they will personally accept\n",
      "1\n",
      "jehovah 's witnesses have established hospital liaison committees as a cooperative arrangement between individual jehovah 's witnesses and medical professionals and hospitals\n",
      "2\n",
      "nordic\n",
      "2\n",
      "pursuit of trophies\n",
      "3\n",
      "blocked\n",
      "3\n",
      "\n",
      "4\n",
      "as an offering\n",
      "4\n",
      "a UNK\n",
      "Iter 21000, val_start_accuracy = 0.1797, val_end_acc = 0.2109, val_acc = 0.1484, val loss = 10.017295\n",
      "Exact Match Score: 0.1641\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--21000.ckpt\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/PntrNtwrk_weights_iter--21000.ckpt\n",
      "Iter 21200, start_accuracy = 0.6641, end_accuracy = 0.7109, accuracy = 0.5312, train loss = 2.084714\n",
      "0\n",
      "1950\n",
      "0\n",
      "1950\n",
      "1\n",
      "the baptisms were genuine\n",
      "1\n",
      "genuine\n",
      "2\n",
      "may day\n",
      "2\n",
      "may day\n",
      "3\n",
      "nucleus\n",
      "3\n",
      "class schizomycetes\n",
      "4\n",
      "church of scotland and the united free church of scotland\n",
      "4\n",
      "scotland and the united free church of scotland\n",
      "Iter 21200, start_accuracy = 0.6641, end_accuracy = 0.7109, accuracy = 0.5312, train loss = 2.084714\n",
      "Exact Match Score: 0.5625\n",
      "Iter 21400, start_accuracy = 0.6875, end_accuracy = 0.7578, accuracy = 0.5547, train loss = 1.849271\n",
      "0\n",
      "kinshasa\n",
      "0\n",
      "kinshasa\n",
      "1\n",
      "1961\n",
      "1\n",
      "1961\n",
      "2\n",
      "saint fm and the st helena independent\n",
      "2\n",
      "saint fm and the st helena independent\n",
      "3\n",
      "the waxwings bombycilla\n",
      "3\n",
      "waxwings bombycilla\n",
      "4\n",
      "gregorian leap year rule\n",
      "4\n",
      "the gregorian leap year rule\n",
      "Iter 21400, start_accuracy = 0.6875, end_accuracy = 0.7578, accuracy = 0.5547, train loss = 1.849271\n",
      "Exact Match Score: 0.6562\n",
      "Iter 21600, start_accuracy = 0.7031, end_accuracy = 0.8203, accuracy = 0.5938, train loss = 1.728533\n",
      "0\n",
      "24,000\n",
      "0\n",
      "24,000\n",
      "1\n",
      "hypothetical\n",
      "1\n",
      "hypothetical thinking\n",
      "2\n",
      "became a de facto \" grand lodge \"\n",
      "2\n",
      "\n",
      "3\n",
      "2009\n",
      "3\n",
      "2009\n",
      "4\n",
      "beli dvor\n",
      "4\n",
      "beli dvor\n",
      "Iter 21600, start_accuracy = 0.7031, end_accuracy = 0.8203, accuracy = 0.5938, train loss = 1.728533\n",
      "Exact Match Score: 0.6406\n",
      "Iter 21800, start_accuracy = 0.7031, end_accuracy = 0.7031, accuracy = 0.5703, train loss = 1.961103\n",
      "0\n",
      "recognize habitats and form mental maps\n",
      "0\n",
      "the cognitive ability of the birds to recognize habitats and form mental maps\n",
      "1\n",
      "sage system\n",
      "1\n",
      "sage system\n",
      "2\n",
      "western dialect\n",
      "2\n",
      "a western dialect\n",
      "3\n",
      "it violated electoral college rules\n",
      "3\n",
      "it violated electoral college rules\n",
      "4\n",
      "tenochtitlan\n",
      "4\n",
      "tenochtitlan\n",
      "Iter 21800, start_accuracy = 0.7031, end_accuracy = 0.7031, accuracy = 0.5703, train loss = 1.961103\n",
      "Exact Match Score: 0.5938\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-9d56a4714d6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                                    \u001b[0mp_seq_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpassage_max_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                                    \u001b[0mq_seq_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mquestion_max_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                                    batch_size: train_data_iter.batch_size})\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrain_iter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         disp_feed_dict = {passage: train_context, \n",
      "\u001b[0;32m/home/ai2-rey/anaconda3/envs/tensorflow_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ai2-rey/anaconda3/envs/tensorflow_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ai2-rey/anaconda3/envs/tensorflow_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/ai2-rey/anaconda3/envs/tensorflow_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ai2-rey/anaconda3/envs/tensorflow_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for train_iter in range(train_iters):\n",
    "    \n",
    "    train_iter +=1\n",
    "    train_batch, _ = train_data_iter.next_batch()\n",
    "    train_context, train_question, train_answer = zip(*train_batch)\n",
    "    if train_iter == 1:\n",
    "        train_feed_dict = {passage: train_context,\n",
    "                           question: train_question,\n",
    "                           y: train_answer,\n",
    "                           dropout_keep_prob: 1.0, \n",
    "                           p_seq_len:np.ones(train_data_iter.batch_size) * passage_max_len,\n",
    "                           q_seq_len:np.ones(train_data_iter.batch_size) * question_max_len,\n",
    "                           batch_size: train_data_iter.batch_size}\n",
    "        \n",
    "        training_loss, start_acc, end_acc, acc, grad_norm = tuple(sess.run([loss, start_accuracy, end_accuracy,\n",
    "                                                                            mean_accuracy, gradient_norm],\n",
    "                                                                  feed_dict=train_feed_dict))\n",
    "\n",
    "        log_output= (\"Iter {}, start_accuracy = {:.4f}, end_accuracy = {:.4f}, accuracy = {:.4f}, train loss = {:.6f}\"\n",
    "                     .format(train_iter, start_acc, end_acc, acc, training_loss))\n",
    "        print(log_output)\n",
    "        em_score, _, _ =evaluate(sess, train_feed_dict, train_context, train_answer)\n",
    "        \n",
    "        write_to_log(log_output, log_file)\n",
    "        write_to_log('Exact Match Score: {:.4f}'.format(em_score), log_file)\n",
    "        \n",
    "    sess.run(optimizer, feed_dict={passage: train_context,\n",
    "                                   question: train_question, \n",
    "                                   y: train_answer, \n",
    "                                   dropout_keep_prob: keep_prob, \n",
    "                                   p_seq_len:np.ones(train_data_iter.batch_size) * passage_max_len,\n",
    "                                   q_seq_len:np.ones(train_data_iter.batch_size) * question_max_len,\n",
    "                                   batch_size: train_data_iter.batch_size})\n",
    "    if train_iter % display_step == 0:\n",
    "        disp_feed_dict = {passage: train_context, \n",
    "                          question: train_question, \n",
    "                          y: train_answer,\n",
    "                          dropout_keep_prob: 1.0,\n",
    "                          p_seq_len:np.ones(train_data_iter.batch_size) * passage_max_len,\n",
    "                          q_seq_len:np.ones(train_data_iter.batch_size) * question_max_len,\n",
    "                          batch_size: train_data_iter.batch_size\n",
    "                         }\n",
    "        training_loss, start_acc, end_acc, acc= tuple(sess.run([loss, start_accuracy, end_accuracy, \n",
    "                                                                mean_accuracy],\n",
    "                                                               feed_dict=disp_feed_dict))\n",
    "        log_output = (\"Iter {}, start_accuracy = {:.4f}, end_accuracy = {:.4f}, accuracy = {:.4f}, train loss = {:.6f}\"\n",
    "              .format(train_iter, start_acc, end_acc, acc, training_loss))\n",
    "        print(log_output)\n",
    "        em_score, _, _ = evaluate(sess, disp_feed_dict, train_context, train_answer)\n",
    "        write_to_log(log_output, log_file)\n",
    "        write_to_log('Exact Match Score: {:.4f}'.format(em_score), log_file)\n",
    "        \n",
    "    if val_data_iter !=None and train_iter%val_interval ==0:\n",
    "        val_batch, _ = val_data_iter.next_batch()\n",
    "        val_context, val_question, val_answer = zip(*val_batch)\n",
    "        val_feed_dict = {passage: val_context,\n",
    "                         question: val_question, \n",
    "                         y: val_answer,\n",
    "                         dropout_keep_prob: 1.0,\n",
    "                         p_seq_len:np.ones(val_data_iter.batch_size) * passage_max_len,\n",
    "                         q_seq_len:np.ones(val_data_iter.batch_size) * question_max_len,\n",
    "                         batch_size: val_data_iter.batch_size}\n",
    "        \n",
    "        val_loss, val_start_acc, val_end_acc, val_acc = tuple(sess.run([loss, start_accuracy, end_accuracy, \n",
    "                                                       mean_accuracy],\n",
    "                                                      feed_dict=val_feed_dict))\n",
    "        \n",
    "        log_output = (\"Iter {}, val_start_accuracy = {:.4f}, val_end_acc = {:.4f}, val_acc = {:.4f}, val loss = {:.6f}\"\n",
    "                      .format(train_iter, val_start_acc, val_end_acc, val_acc, val_loss))\n",
    "        print(log_output)\n",
    "        em_score, _, _ = evaluate(sess, val_feed_dict, val_context, val_answer)\n",
    "        write_to_log(log_output, log_file)\n",
    "        write_to_log('Exact Match Score: {:.4f}'.format(em_score), log_file)\n",
    "    \n",
    "    if train_iter%save_weights_interval ==0:\n",
    "        weights_dir = model_dir  + \"weights/PntrNtwrk_weights_iter--{}.ckpt\".format(train_iter)\n",
    "        save_path = saver.save(sess, weights_dir)\n",
    "        save_string = \"Model saved in file: {}\".format(save_path)\n",
    "        print(save_string)\n",
    "        write_to_log(save_string, log_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ".\n",
    "\n",
    "128, 766, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_env]",
   "language": "python",
   "name": "conda-env-tensorflow_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
