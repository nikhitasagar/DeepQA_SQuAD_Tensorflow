{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import shuffle\n",
    "import os\n",
    "import gensim\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import operator\n",
    "import math\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import string\n",
    "import fileinput\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import nltk\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import split, explode\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors, DenseVector, VectorUDT\n",
    "\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.models.rnn.translate import seq2seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_PYTHON\"] = 'python2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext('local[4]')\n",
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/media/ai2-rey/data_disk/data_sets/SQuAD/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_dir, prefix):\n",
    "    filenames = [data_dir + prefix + tag for tag in ['.context', '.question', '.answer', '.span']]\n",
    "\n",
    "    data = []\n",
    "    column = []\n",
    "    line_iter = fileinput.input(filenames)\n",
    "    for line in line_iter:\n",
    "        if line_iter.isfirstline():\n",
    "            data.append(column)\n",
    "            column = []\n",
    "        column.append(line.rstrip())\n",
    "    data.append(column)\n",
    "    line_iter.close()\n",
    "    data = data[1:]\n",
    "    return list(zip(data[0], data[1], data[2], data[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = load_data(data_dir, 'train')\n",
    "train_cont, train_ques, train_ans, train_span = list(zip(*train_data))\n",
    "\n",
    "val_data = load_data(data_dir, 'val')\n",
    "val_cont, val_ques, val_ans, val_span = list(zip(*val_data))\n",
    "\n",
    "test_data = load_data(data_dir, 'dev')\n",
    "test_cont, test_ques, test_ans, test_span = list(zip(*test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Martorana church ( decorated around 1143 ) looked originally even more Byzantine although important parts were later demolished . The dome mosaic is similar to that of the Cappella Palatina , with Christ enthroned in the middle and four bowed , elongated angels . The Greek inscriptions , decorative patterns , and evangelists in the squinches are obviously executed by the same Greek masters who worked on the Cappella Palatina . The mosaic depicting Roger II of Sicily , dressed in Byzantine imperial robes and receiving the crown by Christ , was originally in the demolished narthex together with another panel , the Theotokos with Georgios of Antiochia , the founder of the church .\n",
      "The Martorana church was likely decorated by the same greek masters who decorated which church ?\n",
      "the Cappella Palatina\n",
      "['68', '70']\n"
     ]
    }
   ],
   "source": [
    "print(val_cont[120])\n",
    "print(val_ques[120])\n",
    "print(val_ans[120])\n",
    "print(val_span[120].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QA_Dataset:\n",
    "  \n",
    "    def __init__(self, context, question, answer, span,\n",
    "                 max_vocab_size=None, autopad_context=None, \n",
    "                 autopad_ques = None, autopad_answer=None, \n",
    "                 context_pad_len=None, ques_pad_len = None, answer_pad_len=None):\n",
    "\n",
    "        assert autopad_context in {'min', 'max', 'avg'}\n",
    "        assert autopad_answer in {'min', 'max', 'avg'}\n",
    "        assert autopad_ques in {'min', 'max', 'avg'}\n",
    "        \n",
    "        self.context = context\n",
    "        self.question = question\n",
    "        self.answer = answer\n",
    "        self.span = span\n",
    "        \n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.context_pad_len = context_pad_len\n",
    "        self.ques_pad_len = ques_pad_len\n",
    "        self.answer_pad_len = answer_pad_len\n",
    "        self.__autopad_context = autopad_context\n",
    "        self.__autopad_ques = autopad_ques\n",
    "        self.__autopad_answer = autopad_answer\n",
    "        \n",
    "        self.vocab = set()\n",
    "        self.word_counter = dict()\n",
    "        self.word2id_dict = dict()\n",
    "        self.id2word_dict = dict()\n",
    "        \n",
    "        self.context_ids = None\n",
    "        self.context_sentences_ids = [] \n",
    "        self.question_ids = None\n",
    "#        self.answer_ids = None\n",
    "#         self.answer_masks = []\n",
    "        self.answer_one_hot_labels = []\n",
    "\n",
    "#         self.context_text = None\n",
    "#         self.question_text = None\n",
    "#         self.answer_text = None\n",
    "        \n",
    "#         self.num_tokens = None\n",
    "#         self.max_sent_len_context = 0\n",
    "#         self.max_num_sentences = 0\n",
    "#         self.max_sent_len_question = 0\n",
    "        \n",
    "        self.__parse_data()\n",
    "        self.__create_word2id_dict()\n",
    "        self.__numericize_data()\n",
    "\n",
    "    def __update_word_counter(self, sequence):\n",
    "        \"\"\" Update word_counter with counts for words in a sentence\n",
    "        \n",
    "        Args:\n",
    "            sequence (list<str>) : list of words in a sequence\n",
    "        \n",
    "        \"\"\"\n",
    "        for word in sequence:\n",
    "            self.word_counter[word] = self.word_counter.get(word, 0) + 1\n",
    "            \n",
    "    def __create_vocab(self):\n",
    "        \"\"\" Create set of most frequent unique words found in the training data \"\"\"\n",
    "        \n",
    "        if self.max_vocab_size == None:\n",
    "            self.vocab = set(self.word_counter.keys())\n",
    "        else:\n",
    "            self.vocab = set(sorted(self.word_counter, key=self.word_counter.get, reverse=True)[:self.max_vocab_size])\n",
    "        \n",
    "    def __shuffle_data(self, data):\n",
    "        random.shuffle(data)\n",
    "        return list(zip(*data))\n",
    "    \n",
    "    def __parse_data(self):\n",
    "        \n",
    "        for idx in range(len(self.context)):\n",
    "            self.__update_word_counter(self.context[idx].split())\n",
    "            self.__update_word_counter(self.question[idx].split())\n",
    "            self.__update_word_counter(self.answer[idx].split())\n",
    "        \n",
    "        self.__create_vocab()\n",
    "        \n",
    "        shuffle = list(zip(self.context, self.question, self.answer, self.span))        \n",
    "        self.context, self.question, self.answer, self.span = self.__shuffle_data(shuffle)\n",
    "    \n",
    "    def __create_word2id_dict(self):\n",
    "        \n",
    "        misc_tokens = ['PAD', 'UNK']\n",
    "            \n",
    "        for i, token in enumerate(misc_tokens):\n",
    "            self.word2id_dict[token] = i\n",
    "            \n",
    "        for word in self.vocab:\n",
    "            self.word2id_dict[word] = len(self.word2id_dict)\n",
    "\n",
    "        self.vocab |= set(misc_tokens)\n",
    "        self.id2word_dict = dict(zip(self.word2id_dict.values(), self.word2id_dict.keys()))\n",
    "        self.num_tokens = len(self.word2id_dict)\n",
    "    \n",
    "    def __convert_word2id(self, word):\n",
    "        try:\n",
    "            word_id = self.word2id_dict[word]\n",
    "        except:\n",
    "            word_id = self.word2id_dict['UNK']\n",
    "        return word_id\n",
    "    \n",
    "    def __apply_padding(self, s, pad_len):\n",
    "        sequence = s[:]\n",
    "        if len(sequence) < pad_len:\n",
    "            sequence += [self.word2id_dict['PAD'] for i in range(pad_len - len(sequence))]\n",
    "        elif len(sequence) > pad_len:\n",
    "            sequence = sequence[:pad_len]\n",
    "        else:\n",
    "            pass\n",
    "        return sequence\n",
    "        \n",
    "    def __get_seq_length_stats(self, sequences):\n",
    "        max_len = 0\n",
    "        min_len = 100000\n",
    "        avg_len = 0\n",
    "        for sequence in sequences:\n",
    "            max_len = max(max_len, len(sequence))\n",
    "            min_len = min(min_len, len(sequence))\n",
    "            avg_len += len(sequence)\n",
    "        avg_len = int(float(avg_len) / len(sequences))\n",
    "        return min_len, max_len, avg_len\n",
    "\n",
    "    def __get_max_sequence_lengths(self, context_ids, question_ids, answer_ids):\n",
    "        min_context_len, max_context_len, avg_context_len = self.__get_seq_length_stats(context_ids)\n",
    "        min_ques_len, max_ques_len, avg_ques_len = self.__get_seq_length_stats(question_ids)\n",
    "        min_answer_len, max_answer_len, avg_answer_len = self.__get_seq_length_stats(answer_ids)\n",
    "\n",
    "        if self.context_pad_len == None:\n",
    "            if self.__autopad_context != None:\n",
    "                if self.__autopad_context == 'min':\n",
    "                    self.context_pad_len = min_context_len\n",
    "                elif self.__autopad_context == 'max':\n",
    "                    self.context_pad_len = max_context_len\n",
    "                elif self.__autopad_context == 'avg':\n",
    "                    self.context_pad_len = avg_context_len\n",
    "            else:\n",
    "                self.context_pad_len = avg_context_len\n",
    "                \n",
    "        if self.ques_pad_len == None:\n",
    "            if self.__autopad_ques != None:\n",
    "                if self.__autopad_ques == 'min':\n",
    "                    self.ques_pad_len = min_ques_len\n",
    "                elif self.__autopad_ques == 'max':\n",
    "                    self.ques_pad_len = max_ques_len\n",
    "                elif self.__autopad_ques == 'avg':\n",
    "                    self.ques_pad_len = avg_ques_len\n",
    "            else:\n",
    "                self.ques_pad_len = avg_ques_len\n",
    "                \n",
    "        if self.answer_pad_len == None:\n",
    "            if self.__autopad_answer != None:\n",
    "                if self.__autopad_answer == 'min':\n",
    "                    self.answer_pad_len = min_answer_len\n",
    "                elif self.__autopad_answer == 'max':\n",
    "                    self.answer_pad_len = max_answer_len\n",
    "                elif self.__autopad_answer == 'avg':\n",
    "                    self.answer_pad_len = avg_answer_len\n",
    "            else:\n",
    "                self.answer_pad_len = avg_answer_len \n",
    "                \n",
    "        #print('Context length stats: min = {}, max = {}, avg = {}'.format(min_context_len, max_context_len, avg_context_len))\n",
    "        #print('Context pad length set to: {}'.format(self.context_pad_len))\n",
    "        \n",
    "        #print('Question length stats: min = {}, max = {}, avg = {}'.format(min_ques_len, max_ques_len, avg_ques_len))\n",
    "        #print('Question pad length set to: {}'.format(self.ques_pad_len))        \n",
    "        \n",
    "        #print('Answer length stats: min = {}, max = {}, avg = {}'.format(min_answer_len, max_answer_len, avg_answer_len))\n",
    "        #print('Answer pad length set to: {}'.format(self.answer_pad_len))        \n",
    "    def __convert_to_one_hot(self, span_ids, num_classes):\n",
    "        label = []\n",
    "        for idx in span_ids:\n",
    "            one_hot = [0 for i in range(num_classes)]\n",
    "            if idx >= num_classes - 1:\n",
    "                one_hot[-1] = 1\n",
    "            else:\n",
    "                one_hot[idx] = 1\n",
    "            label.append(one_hot)\n",
    "        return label\n",
    "    \n",
    "    def __convert_to_mask(self, i, a):\n",
    "        mask = (np.in1d(np.array(self.context_ids[i]), a[i]))*1\n",
    "        if len(a[i])>1:\n",
    "            count = 0\n",
    "            while count < len(mask):\n",
    "                if mask[count:count+len(a[i])].all():\n",
    "                    count+=len(a[i])\n",
    "                else:\n",
    "                    mask[count] = False\n",
    "                    count +=1\n",
    "        return mask\n",
    "        \n",
    "    def __tokenize_sentences(self, context_ids, question_ids, answer_ids):\n",
    "        \"\"\" Tokenizes sentences.\n",
    "        :param raw: dict returned from load_babi\n",
    "        :param word_table: WordTable\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        context_tokens = []\n",
    "        question_tokens = []\n",
    "        #answer_tokens = []\n",
    "        \n",
    "        for i in range(len(context_ids)):\n",
    "            c_tkn_ids = self.__apply_padding(context_ids[i], self.context_pad_len)\n",
    "            context_tokens.append(c_tkn_ids)\n",
    "            q_tkn_ids = self.__apply_padding(question_ids[i], self.ques_pad_len)\n",
    "            question_tokens.append(q_tkn_ids)\n",
    "            \n",
    "            span_ids = [int(x) for x in self.span[i].split()]\n",
    "            label = self.__convert_to_one_hot(span_ids, self.context_pad_len)\n",
    "            self.answer_one_hot_labels.append(label)\n",
    "            \n",
    "            if i%100 == 0:\n",
    "                print(i)\n",
    "            #a_tkn_ids = self.__apply_padding(answer_ids[i], self.answer_pad_len)\n",
    "            #answer_tokens.append(a_tkn_ids)\n",
    "        return context_tokens, question_tokens#, answer_tokens\n",
    "        \n",
    "    def __convert_text2ids(self, context, question, answer):\n",
    "        \"\"\" Tokenizes sentences.\n",
    "        :param raw: dict returned from load_babi\n",
    "        :param word_table: WordTable\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        context_ids = []\n",
    "        question_ids = []\n",
    "        answer_ids = []\n",
    "        for i in range(len(context)):\n",
    "            c_ids = [self.__convert_word2id(word) for word in context[i].split()]\n",
    "            context_ids.append(c_ids)\n",
    "            q_ids = [self.__convert_word2id(word) for word in question[i].split()]\n",
    "            question_ids.append(q_ids)\n",
    "            a_ids = [self.__convert_word2id(word) for word in answer[i].split()]\n",
    "            answer_ids.append(a_ids)            \n",
    "        return context_ids, question_ids, answer_ids\n",
    "          \n",
    "    def __convert_text2words(self, context, question, answer):\n",
    "        context_words = []\n",
    "        question_words = []\n",
    "        answer_words = []\n",
    "        for i in range(len(context)):\n",
    "            c_words = context[i].split()\n",
    "            context_words.append(c_words)\n",
    "            q_words = question[i].split()\n",
    "            question_words.append(q_words)\n",
    "            a_words = answer[i].split()\n",
    "            answer_words.append(a_words)\n",
    "        return context_words, question_words, answer_words    \n",
    "\n",
    "    def __get_max_sentence_length(self, stories):\n",
    "        num_sentences = 0\n",
    "        sen_len = 0\n",
    "        for story in stories: \n",
    "            sentences = sent_tokenize(story)\n",
    "            num_sentences = max(num_sentences, len(sentences))\n",
    "            word_count = lambda sentence: len(word_tokenize(sentence))\n",
    "            sen_len = max(sen_len, len((max(sentences, key=word_count)).split()))\n",
    "        return sen_len, num_sentences\n",
    "        \n",
    "    def __context2sentences(self, story):\n",
    "        story_sentences = []\n",
    "        curr_sentence = []\n",
    "        for word_id in story:\n",
    "            if self.id2word_dict[word_id] == '.':\n",
    "                if len(curr_sentence) < self.max_sent_len_context:\n",
    "                    curr_sentence += [0 for i in range(self.max_sent_len_context - len(curr_sentence))]\n",
    "                elif len(curr_sentence) > self.max_sent_len_context:\n",
    "                    curr_sentence = curr_sentence[:self.max_sent_len_context]\n",
    "                story_sentences.append(curr_sentence)\n",
    "                curr_sentence = []\n",
    "            else:\n",
    "                curr_sentence.append(word_id)\n",
    "            \n",
    "        return story_sentences\n",
    "    \n",
    "    def __numericize_data(self):\n",
    "        c, q, a = self.__convert_text2ids(self.context, self.question, self.answer)\n",
    "        \n",
    "        self.__get_max_sequence_lengths(c, q, a)\n",
    "        \n",
    "#         self.context_ids, self.question_ids, self.answer_ids = self.__tokenize_sentences(c, q, a)\n",
    "        self.context_ids, self.question_ids = self.__tokenize_sentences(c, q, a)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "UNKNOWN = \"*UNKNOWN*\"\n",
    "\n",
    "def generate_embedding_files(filename):\n",
    "    embeddings = {}\n",
    "    for line in open(filename):\n",
    "        parts = line.split()\n",
    "        embeddings[parts[0]] = list(map(float, parts[1:]))\n",
    "    embedding_size = len(list(embeddings.values())[0])\n",
    "    embeddings[UNKNOWN] = [0.0 for _ in range(embedding_size)]\n",
    "    \n",
    "    words = embeddings.keys()\n",
    "    embedding_matrix = np.array([embeddings[word] for word in list(embeddings.keys())])\n",
    "    return words, embedding_matrix\n",
    "\n",
    "glove_words, glove_embedding_matrix = generate_embedding_files('/media/ai2-rey/data_disk/data_sets/glove.6B/glove.6B.50d.txt')\n",
    "glove_vocab_lookup = {word: i for i, word  in enumerate(glove_words)}\n",
    "glove_vocab_size, glove_embedding_size= glove_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataIterator:  \n",
    "    def __init__(self, data, batch_size):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.data_iterator = self.make_random_iter()\n",
    "        \n",
    "    def next_batch(self):\n",
    "        try:\n",
    "            idxs = next(self.data_iterator)\n",
    "        except StopIteration:\n",
    "            self.data_iterator = self.make_random_iter()\n",
    "            idxs = next(self.data_iterator)\n",
    "            \n",
    "        batch = [self.data[i] for i in idxs]\n",
    "        batch_idxs = [idx for idx in idxs]\n",
    "        return batch, batch_idxs\n",
    "\n",
    "    def make_random_iter(self):\n",
    "        splits = np.arange(self.batch_size, len(self.data), self.batch_size)\n",
    "        it = np.split(np.random.permutation(range(len(self.data))), splits)[:-1]\n",
    "        return iter(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n",
      "26500\n",
      "26600\n",
      "26700\n",
      "26800\n",
      "26900\n",
      "27000\n",
      "27100\n",
      "27200\n",
      "27300\n",
      "27400\n",
      "27500\n",
      "27600\n",
      "27700\n",
      "27800\n",
      "27900\n",
      "28000\n",
      "28100\n",
      "28200\n",
      "28300\n",
      "28400\n",
      "28500\n",
      "28600\n",
      "28700\n",
      "28800\n",
      "28900\n",
      "29000\n",
      "29100\n",
      "29200\n",
      "29300\n",
      "29400\n",
      "29500\n",
      "29600\n",
      "29700\n",
      "29800\n",
      "29900\n",
      "30000\n",
      "30100\n",
      "30200\n",
      "30300\n",
      "30400\n",
      "30500\n",
      "30600\n",
      "30700\n",
      "30800\n",
      "30900\n",
      "31000\n",
      "31100\n",
      "31200\n",
      "31300\n",
      "31400\n",
      "31500\n",
      "31600\n",
      "31700\n",
      "31800\n",
      "31900\n",
      "32000\n",
      "32100\n",
      "32200\n",
      "32300\n",
      "32400\n",
      "32500\n",
      "32600\n",
      "32700\n",
      "32800\n",
      "32900\n",
      "33000\n",
      "33100\n",
      "33200\n",
      "33300\n",
      "33400\n",
      "33500\n",
      "33600\n",
      "33700\n",
      "33800\n",
      "33900\n",
      "34000\n",
      "34100\n",
      "34200\n",
      "34300\n",
      "34400\n",
      "34500\n",
      "34600\n",
      "34700\n",
      "34800\n",
      "34900\n",
      "35000\n",
      "35100\n",
      "35200\n",
      "35300\n",
      "35400\n",
      "35500\n",
      "35600\n",
      "35700\n",
      "35800\n",
      "35900\n",
      "36000\n",
      "36100\n",
      "36200\n",
      "36300\n",
      "36400\n",
      "36500\n",
      "36600\n",
      "36700\n",
      "36800\n",
      "36900\n",
      "37000\n",
      "37100\n",
      "37200\n",
      "37300\n",
      "37400\n",
      "37500\n",
      "37600\n",
      "37700\n",
      "37800\n",
      "37900\n",
      "38000\n",
      "38100\n",
      "38200\n",
      "38300\n",
      "38400\n",
      "38500\n",
      "38600\n",
      "38700\n",
      "38800\n",
      "38900\n",
      "39000\n",
      "39100\n",
      "39200\n",
      "39300\n",
      "39400\n",
      "39500\n",
      "39600\n",
      "39700\n",
      "39800\n",
      "39900\n",
      "40000\n",
      "40100\n",
      "40200\n",
      "40300\n",
      "40400\n",
      "40500\n",
      "40600\n",
      "40700\n",
      "40800\n",
      "40900\n",
      "41000\n",
      "41100\n",
      "41200\n",
      "41300\n",
      "41400\n",
      "41500\n",
      "41600\n",
      "41700\n",
      "41800\n",
      "41900\n",
      "42000\n",
      "42100\n",
      "42200\n",
      "42300\n",
      "42400\n",
      "42500\n",
      "42600\n",
      "42700\n",
      "42800\n",
      "42900\n",
      "43000\n",
      "43100\n",
      "43200\n",
      "43300\n",
      "43400\n",
      "43500\n",
      "43600\n",
      "43700\n",
      "43800\n",
      "43900\n",
      "44000\n",
      "44100\n",
      "44200\n",
      "44300\n",
      "44400\n",
      "44500\n",
      "44600\n",
      "44700\n",
      "44800\n",
      "44900\n",
      "45000\n",
      "45100\n",
      "45200\n",
      "45300\n",
      "45400\n",
      "45500\n",
      "45600\n",
      "45700\n",
      "45800\n",
      "45900\n",
      "46000\n",
      "46100\n",
      "46200\n",
      "46300\n",
      "46400\n",
      "46500\n",
      "46600\n",
      "46700\n",
      "46800\n",
      "46900\n",
      "47000\n",
      "47100\n",
      "47200\n",
      "47300\n",
      "47400\n",
      "47500\n",
      "47600\n",
      "47700\n",
      "47800\n",
      "47900\n",
      "48000\n",
      "48100\n",
      "48200\n",
      "48300\n",
      "48400\n",
      "48500\n",
      "48600\n",
      "48700\n",
      "48800\n",
      "48900\n",
      "49000\n",
      "49100\n",
      "49200\n",
      "49300\n",
      "49400\n",
      "49500\n",
      "49600\n",
      "49700\n",
      "49800\n",
      "49900\n",
      "50000\n",
      "50100\n",
      "50200\n",
      "50300\n",
      "50400\n",
      "50500\n",
      "50600\n",
      "50700\n",
      "50800\n",
      "50900\n",
      "51000\n",
      "51100\n",
      "51200\n",
      "51300\n",
      "51400\n",
      "51500\n",
      "51600\n",
      "51700\n",
      "51800\n",
      "51900\n",
      "52000\n",
      "52100\n",
      "52200\n",
      "52300\n",
      "52400\n",
      "52500\n",
      "52600\n",
      "52700\n",
      "52800\n",
      "52900\n",
      "53000\n",
      "53100\n",
      "53200\n",
      "53300\n",
      "53400\n",
      "53500\n",
      "53600\n",
      "53700\n",
      "53800\n",
      "53900\n",
      "54000\n",
      "54100\n",
      "54200\n",
      "54300\n",
      "54400\n",
      "54500\n",
      "54600\n",
      "54700\n",
      "54800\n",
      "54900\n",
      "55000\n",
      "55100\n",
      "55200\n",
      "55300\n",
      "55400\n",
      "55500\n",
      "55600\n",
      "55700\n",
      "55800\n",
      "55900\n",
      "56000\n",
      "56100\n",
      "56200\n",
      "56300\n",
      "56400\n",
      "56500\n",
      "56600\n",
      "56700\n",
      "56800\n",
      "56900\n",
      "57000\n",
      "57100\n",
      "57200\n",
      "57300\n",
      "57400\n",
      "57500\n",
      "57600\n",
      "57700\n",
      "57800\n",
      "57900\n",
      "58000\n",
      "58100\n",
      "58200\n",
      "58300\n",
      "58400\n",
      "58500\n",
      "58600\n",
      "58700\n",
      "58800\n",
      "58900\n",
      "59000\n",
      "59100\n",
      "59200\n",
      "59300\n",
      "59400\n",
      "59500\n",
      "59600\n",
      "59700\n",
      "59800\n",
      "59900\n",
      "60000\n",
      "60100\n",
      "60200\n",
      "60300\n",
      "60400\n",
      "60500\n",
      "60600\n",
      "60700\n",
      "60800\n",
      "60900\n",
      "61000\n",
      "61100\n",
      "61200\n",
      "61300\n",
      "61400\n",
      "61500\n",
      "61600\n",
      "61700\n",
      "61800\n",
      "61900\n",
      "62000\n",
      "62100\n",
      "62200\n",
      "62300\n",
      "62400\n",
      "62500\n",
      "62600\n",
      "62700\n",
      "62800\n",
      "62900\n",
      "63000\n",
      "63100\n",
      "63200\n",
      "63300\n",
      "63400\n",
      "63500\n",
      "63600\n",
      "63700\n",
      "63800\n",
      "63900\n",
      "64000\n",
      "64100\n",
      "64200\n",
      "64300\n",
      "64400\n",
      "64500\n",
      "64600\n",
      "64700\n",
      "64800\n",
      "64900\n",
      "65000\n",
      "65100\n",
      "65200\n",
      "65300\n",
      "65400\n",
      "65500\n",
      "65600\n",
      "65700\n",
      "65800\n",
      "65900\n",
      "66000\n",
      "66100\n",
      "66200\n",
      "66300\n",
      "66400\n",
      "66500\n",
      "66600\n",
      "66700\n",
      "66800\n",
      "66900\n",
      "67000\n",
      "67100\n",
      "67200\n",
      "67300\n",
      "67400\n",
      "67500\n",
      "67600\n",
      "67700\n",
      "67800\n",
      "67900\n",
      "68000\n",
      "68100\n",
      "68200\n",
      "68300\n",
      "68400\n",
      "68500\n",
      "68600\n",
      "68700\n",
      "68800\n",
      "68900\n",
      "69000\n",
      "69100\n",
      "69200\n",
      "69300\n",
      "69400\n",
      "69500\n",
      "69600\n",
      "69700\n",
      "69800\n",
      "69900\n",
      "70000\n",
      "70100\n",
      "70200\n",
      "70300\n",
      "70400\n",
      "70500\n",
      "70600\n",
      "70700\n",
      "70800\n",
      "70900\n",
      "71000\n",
      "71100\n",
      "71200\n",
      "71300\n",
      "71400\n",
      "71500\n",
      "71600\n",
      "71700\n",
      "71800\n",
      "71900\n",
      "72000\n",
      "72100\n",
      "72200\n",
      "72300\n",
      "72400\n",
      "72500\n",
      "72600\n",
      "72700\n",
      "72800\n",
      "72900\n",
      "73000\n",
      "73100\n",
      "73200\n",
      "73300\n",
      "73400\n",
      "73500\n",
      "73600\n",
      "73700\n",
      "73800\n",
      "73900\n",
      "74000\n",
      "74100\n",
      "74200\n",
      "74300\n",
      "74400\n",
      "74500\n",
      "74600\n",
      "74700\n",
      "74800\n",
      "74900\n",
      "75000\n",
      "75100\n",
      "75200\n",
      "75300\n",
      "75400\n",
      "75500\n",
      "75600\n",
      "75700\n",
      "75800\n",
      "75900\n",
      "76000\n",
      "76100\n",
      "76200\n",
      "76300\n",
      "76400\n",
      "76500\n",
      "76600\n",
      "76700\n",
      "76800\n",
      "76900\n",
      "77000\n",
      "77100\n",
      "77200\n",
      "77300\n",
      "77400\n",
      "77500\n",
      "77600\n",
      "77700\n",
      "77800\n",
      "77900\n",
      "78000\n",
      "78100\n",
      "78200\n",
      "78300\n",
      "78400\n",
      "78500\n",
      "78600\n",
      "78700\n",
      "78800\n",
      "78900\n",
      "79000\n",
      "79100\n",
      "79200\n",
      "79300\n",
      "79400\n",
      "79500\n",
      "79600\n",
      "79700\n",
      "79800\n",
      "79900\n",
      "80000\n",
      "80100\n",
      "80200\n",
      "80300\n",
      "80400\n",
      "80500\n",
      "80600\n",
      "80700\n",
      "80800\n",
      "80900\n",
      "81000\n",
      "81100\n",
      "81200\n",
      "81300\n"
     ]
    }
   ],
   "source": [
    "train = QA_Dataset(train_cont, train_ques, train_ans, train_span,\n",
    "                   context_pad_len = 200, ques_pad_len = 20,\n",
    "                   autopad_context='max', autopad_ques = 'max', autopad_answer='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n"
     ]
    }
   ],
   "source": [
    "val = QA_Dataset(val_cont, val_ques, val_ans, val_span,\n",
    "                 context_pad_len=train.context_pad_len, \n",
    "                 ques_pad_len = train.ques_pad_len, \n",
    "                 answer_pad_len=train.answer_pad_len,\n",
    "                 autopad_context='max', autopad_ques='max', autopad_answer='max')\n",
    "\n",
    "test = QA_Dataset(test_cont, test_ques, test_ans, test_span,\n",
    "                  context_pad_len=train.context_pad_len, \n",
    "                  ques_pad_len = train.ques_pad_len,\n",
    "                  answer_pad_len=train.answer_pad_len,\n",
    "                  autopad_context='max', autopad_ques='max', autopad_answer='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_cont, train_ques, train_ans, train_span\n",
    "del val_cont, val_ques, val_ans, val_span\n",
    "del test_cont, test_ques, test_ans, test_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ids = list(zip(train.context_ids, train.question_ids, train.answer_one_hot_labels))\n",
    "val_ids = list(zip(val.context_ids,val.question_ids,val.answer_one_hot_labels))\n",
    "test_ids = list(zip(test.context_ids,test.question_ids, test.answer_one_hot_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_iter = DataIterator(train_ids,128)\n",
    "val_data_iter = DataIterator(val_ids,999)\n",
    "test_data_iter = DataIterator(test_ids,999)\n",
    "deploy_data_iter = DataIterator(train_ids,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.ques_pad_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.context_pad_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "passage_max_len = train.context_pad_len\n",
    "question_max_len = train.ques_pad_len\n",
    "#output of lstm is n_steps or passage length by hidden size \n",
    "n_hidden = 50 \n",
    "num_indices = 2\n",
    "#batch_size = 128\n",
    "n_hidden_layers = 2\n",
    "learning_rate = 0.001\n",
    "blend_units = 6\n",
    "train_iters = 100000\n",
    "keep_prob = 0.5\n",
    "display_step = 200\n",
    "val_interval = 1000\n",
    "#word_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.22882 , -0.037135,  0.33333 ,  0.82409 ,  0.37912 ,  0.87029 ,\n",
       "       -0.25803 ,  0.89802 ,  1.0447  , -0.53276 , -0.27966 ,  0.22481 ,\n",
       "        0.25515 , -0.73782 , -0.43686 ,  0.84488 , -0.77032 ,  0.36941 ,\n",
       "        1.723   ,  0.091053,  0.69399 ,  0.67251 , -0.16291 ,  0.65524 ,\n",
       "        1.178   ,  1.0568  ,  0.67001 , -0.2123  ,  0.90982 , -0.19816 ,\n",
       "       -1.1016  ,  0.86196 ,  0.20359 ,  1.2622  ,  1.6196  ,  0.93234 ,\n",
       "        0.14488 , -0.18434 , -1.1684  ,  1.4614  ,  0.039604,  1.1774  ,\n",
       "       -1.6755  , -1.1671  , -0.5517  ,  0.35379 , -0.1919  ,  0.35912 ,\n",
       "       -0.2064  ,  0.29453 ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embedding_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "100\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "passage = tf.placeholder(tf.int32, [None, passage_max_len], name='passage')\n",
    "question = tf.placeholder(tf.int32,[None, question_max_len], name='question')\n",
    "#desired_output = tf.placeholder(tf.int32, [num_indices, batch_size, passage_max_len], name='desired_output')\n",
    "y = tf.placeholder(tf.float32,[None, 2, passage_max_len])\n",
    "\n",
    "dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "batch_size = tf.placeholder(tf.int32)\n",
    "\n",
    "embeddings = tf.constant(glove_embedding_matrix, name='embeddings', dtype=tf.float32)\n",
    "\n",
    "#preprocessing layer \n",
    "passage_embedded = tf.nn.embedding_lookup(embeddings, passage)\n",
    "question_embedded = tf.nn.embedding_lookup(embeddings, question)\n",
    "\n",
    "with tf.variable_scope('passage_lstm'):\n",
    "    passage_cell = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
    "    passage_cell = tf.nn.rnn_cell.DropoutWrapper(passage_cell, output_keep_prob=dropout_keep_prob)\n",
    "    passage_cell = tf.nn.rnn_cell.MultiRNNCell([passage_cell]*n_hidden_layers)\n",
    "    #H_p shape = batch_size, max_passage_len, n_hidden\n",
    "    H_p, _ = tf.nn.dynamic_rnn(passage_cell, passage_embedded, dtype=tf.float32) \n",
    "    \n",
    "with tf.variable_scope('question_lstm'):\n",
    "    question_cell = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
    "    question_cell = tf.nn.rnn_cell.DropoutWrapper(question_cell, output_keep_prob=dropout_keep_prob)\n",
    "    question_cell = tf.nn.rnn_cell.MultiRNNCell([question_cell]*n_hidden_layers)\n",
    "    H_q, _ = tf.nn.dynamic_rnn(question_cell, question_embedded, dtype=tf.float32)\n",
    "\n",
    "#Match LSTM layer \n",
    "#Weights and bias to compute G\n",
    "W_q = tf.Variable(tf.truncated_normal(shape=[n_hidden, n_hidden], stddev=0.1))\n",
    "W_p = tf.Variable(tf.truncated_normal(shape=[n_hidden, n_hidden], stddev=0.1))\n",
    "W_r = tf.Variable(tf.truncated_normal(shape=[n_hidden, n_hidden], stddev=0.1))\n",
    "b_p = tf.Variable(tf.truncated_normal(shape=[n_hidden], stddev=0.1))\n",
    "\n",
    "#Weight and bias to compute a\n",
    "w= tf.Variable(tf.truncated_normal(shape=[1, n_hidden], stddev=0.1))\n",
    "#b_alpha shape = 1,none\n",
    "b_alpha = tf.Variable(tf.constant(0.1, shape = [1]))\n",
    "\n",
    "#only calculate WH_q once \n",
    "#H_q shape = (batch_size&max_passage_len merge, n_hidden)\n",
    "H_q = tf.reshape(H_q, [-1, H_q.get_shape().as_list()[-1]])\n",
    "#H_q shape = (n_hidden, batch_size&max_passage_len merge)\n",
    "H_q = tf.transpose(H_q, [1,0])\n",
    "WH_q = tf.batch_matmul(W_q, H_q)\n",
    "WH_q = tf.reshape(WH_q, [n_hidden, question_max_len, -1])\n",
    "#WH_q shape =(batch_size, n_hidden, max_ques_len)\n",
    "WH_q = tf.transpose(WH_q, [2,0,1])\n",
    "\n",
    "H_r_forward = []\n",
    "H_r_forward_states = []\n",
    "H_r_backward = []\n",
    "H_r_backward_states = []\n",
    "\n",
    "with tf.variable_scope('forward_match_lstm'):\n",
    "    forward_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(n_hidden), output_keep_prob=dropout_keep_prob)\n",
    "    forward_state = forward_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    h = forward_state.h #h shape =(batch_size, n_hidden)\n",
    "    c = forward_state.c\n",
    "\n",
    "    for i in range(passage_max_len):\n",
    "        H_p_i = tf.slice(H_p, [0,i,0], [-1,1,-1])\n",
    "        H_p_i = tf.transpose(tf.squeeze(H_p_i,1),[1,0])\n",
    "        WH_p_i = tf.transpose(tf.matmul(W_p, H_p_i), [1,0])\n",
    "        h = tf.transpose(h,[1,0])\n",
    "        W_rh = tf.transpose(tf.matmul(W_r, h),[1,0])\n",
    "        G_tile = tf.tile((WH_p_i + W_rh + b_p), [question_max_len, 1])\n",
    "        G_tile = tf.reshape(G_tile, [-1, n_hidden, question_max_len])\n",
    "        G_forward = tf.tanh(WH_q + G_tile)\n",
    "        G_forward = tf.reshape(G_forward, [n_hidden, -1])\n",
    "        wG_forward = tf.matmul(w,G_forward)\n",
    "        a_tile = tf.tile(b_alpha, [question_max_len])\n",
    "        wG_forward = tf.reshape(tf.squeeze(wG_forward,0),[-1, question_max_len])\n",
    "        alpha_forward = tf.nn.softmax(wG_forward + a_tile)\n",
    "\n",
    "        H_q = tf.reshape(H_q,[-1, n_hidden, question_max_len])\n",
    "        alpha_forward = tf.expand_dims(alpha_forward, 2)\n",
    "        H_q_alpha_forward = tf.batch_matmul(H_q, alpha_forward)\n",
    "        H_q_alpha_forward = tf.reshape(H_q_alpha_forward,[-1, n_hidden])\n",
    "        H_p_i = tf.transpose(H_p_i,[1,0])\n",
    "        z_forward = tf.concat(1, [H_p_i, H_q_alpha_forward])\n",
    "\n",
    "        h,forward_state = forward_cell(z_forward, forward_state)\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        H_r_forward.append(h)\n",
    "        H_r_forward_states.append(forward_state.c)\n",
    "        if i%100 == 0:\n",
    "            print(i)\n",
    "\n",
    "    H_r_forward = tf.transpose(tf.stack(H_r_forward),[1,0,2])\n",
    "    \n",
    "with tf.variable_scope('backward_match_lstm'):\n",
    "    backward_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(n_hidden), output_keep_prob=dropout_keep_prob)\n",
    "    backward_state = backward_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    h = backward_state.h #h shape =(bathc_size, n_hidden)\n",
    "\n",
    "    for i in reversed(range(passage_max_len)):\n",
    "        H_p_i = tf.slice(H_p, [0,i,0], [-1,1,-1])\n",
    "        H_p_i = tf.transpose(tf.squeeze(H_p_i,1),[1,0])\n",
    "        WH_p_i = tf.transpose(tf.matmul(W_p, H_p_i), [1,0])\n",
    "        h = tf.transpose(h,[1,0])\n",
    "        W_rh = tf.transpose(tf.matmul(W_r, h),[1,0])\n",
    "        G_tile = tf.tile((WH_p_i + W_rh + b_p), [question_max_len, 1])\n",
    "        G_tile = tf.reshape(G_tile, [-1, n_hidden, question_max_len])\n",
    "        G_backward = tf.tanh(WH_q + G_tile)\n",
    "        G_backward = tf.reshape(G_backward, [n_hidden, -1])\n",
    "        wG_backward = tf.matmul(w,G_backward)\n",
    "        a_tile = tf.tile(b_alpha, [question_max_len])\n",
    "        wG_backward = tf.reshape(tf.squeeze(wG_backward,0),[-1, question_max_len])\n",
    "        alpha_backward = tf.nn.softmax(wG_backward + a_tile)\n",
    "\n",
    "#         H_q = tf.reshape(H_q, [self.n_hidden, self.n_steps_question, -1])\n",
    "#         H_q = tf.reshape(H_q, [-1, self.n_steps_question])\n",
    "#         alpha_backward = tf.transpose(alpha_backward, [1, 0])\n",
    "#         H_qalpha_backward = tf.matmul(H_q, alpha_backward)\n",
    "#         H_qalpha_backward = tf.reshape(H_qalpha_backward, [self.n_hidden, -1])\n",
    "        \n",
    "        H_q = tf.reshape(H_q,[-1, n_hidden, question_max_len])\n",
    "        alpha_backward = tf.expand_dims(alpha_backward, 2)\n",
    "        H_q_alpha_backward = tf.batch_matmul(H_q, alpha_backward)\n",
    "        H_q_alpha_backward = tf.reshape(H_q_alpha_backward,[-1, n_hidden])\n",
    "        H_p_i = tf.transpose(H_p_i,[1,0])\n",
    "        z_backward = tf.concat(1, [H_p_i, H_q_alpha_backward])\n",
    "\n",
    "        h,backward_state = backward_cell(z_backward, backward_state)\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        H_r_backward.append(h)\n",
    "        H_r_backward_states.append(backward_state.c)\n",
    "        if i%100==0:\n",
    "            print(i)\n",
    "    H_r_backward = list(reversed(H_r_backward))\n",
    "    H_r_backward = tf.transpose(tf.stack(H_r_backward),[1,0,2])\n",
    "    \n",
    "    H_r_backward_states = list(reversed(H_r_backward_states))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H_r_forward_try = tf.stack(H_r_forward_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H_r_forward_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H_r_states = tf.concat(2, [H_r_forward_states, H_r_backward_states])\n",
    "print(H_r_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "H_r = tf.concat(2, [H_r_forward, H_r_backward])\n",
    "print(H_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H_r = H_r_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_2:0' shape=(?, 50) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_r = tf.reshape(H_r, [-1, n_hidden])\n",
    "H_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid reduction dimension 1 for input with 1 dimensions. for 'All' (op: 'All') with input shapes: [?], [].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/home/ai2-rey/anaconda3/envs/tensorflow_env/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    669\u001b[0m           \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors_as_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m           status)\n\u001b[0m\u001b[1;32m    671\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ai2-rey/anaconda3/envs/tensorflow_env/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ai2-rey/anaconda3/envs/tensorflow_env/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Invalid reduction dimension 1 for input with 1 dimensions. for 'All' (op: 'All') with input shapes: [?], [].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-f93ebc793269>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mcorrect_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mcorrect_span\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#correct_pred = tf.equal(preds, labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ai2-rey/anaconda3/envs/tensorflow_env/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_all\u001b[0;34m(input_tensor, axis, keep_dims, name, reduction_indices)\u001b[0m\n\u001b[1;32m   1435\u001b[0m       \u001b[0m_ReductionDims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m       \u001b[0mkeep_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1437\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ai2-rey/anaconda3/envs/tensorflow_env/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_all\u001b[0;34m(input, reduction_indices, keep_dims, name)\u001b[0m\n\u001b[1;32m    117\u001b[0m   result = _op_def_lib.apply_op(\"All\", input=input,\n\u001b[1;32m    118\u001b[0m                                 \u001b[0mreduction_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                                 keep_dims=keep_dims, name=name)\n\u001b[0m\u001b[1;32m    120\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ai2-rey/anaconda3/envs/tensorflow_env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    757\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    758\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    760\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ai2-rey/anaconda3/envs/tensorflow_env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2240\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2242\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2243\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ai2-rey/anaconda3/envs/tensorflow_env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1615\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1617\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1618\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/home/ai2-rey/anaconda3/envs/tensorflow_env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ai2-rey/anaconda3/envs/tensorflow_env/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ai2-rey/anaconda3/envs/tensorflow_env/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid reduction dimension 1 for input with 1 dimensions. for 'All' (op: 'All') with input shapes: [?], []."
     ]
    }
   ],
   "source": [
    "W_output_start = tf.Variable(tf.truncated_normal([n_hidden,1], stddev=0.1))\n",
    "b_output_start = tf.Variable(tf.zeros([passage_max_len]))\n",
    "# dense layer\n",
    "logits_start = tf.reshape(tf.matmul(H_r, W_output_start), [-1, passage_max_len]) + b_output_start\n",
    "\n",
    "probs_start = tf.nn.softmax(logits_start)\n",
    "\n",
    "start_label, end_label = tf.split(1, 2, y)\n",
    "start_label = tf.squeeze(start_label, 1)\n",
    "end_label = tf.squeeze(end_label, 1)\n",
    "\n",
    "combined_prob =tf.batch_matmul(tf.expand_dims(probs_start, 1), tf.expand_dims(start_label, 2))\n",
    "loss = -tf.reduce_mean(tf.log(combined_prob))\n",
    "\n",
    "#                 loss_start = tf.nn.softmax_cross_entropy_with_logits(logits_start, start_label)\n",
    "#                 loss_end = tf.nn.softmax_cross_entropy_with_logits(logits_end, end_label)\n",
    "#                 self.__loss = tf.reduce_mean(loss_start + loss_end)\n",
    "\n",
    "#                 loss_start = tf.sqrt(tf.reduce_mean(tf.pow(probs_start - start_label, 2.0)))\n",
    "#                 loss_end = tf.sqrt(tf.reduce_mean(tf.pow(probs_end - end_label, 2.0)))\n",
    "#                 self.__loss = loss_start + loss_end\n",
    "#                 loss = tf.sqrt(tf.reduce_mean(tf.pow(tf.add(probs_start, probs_end) - tf.add(start_label, end_label), 2.0)))\n",
    "#self.__loss = loss\n",
    "\n",
    "params = tf.trainable_variables()\n",
    "gradients = tf.gradients(loss, params)\n",
    "gradient_norm = tf.global_norm(gradients)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# Evaluate model\n",
    "#preds = tf.stack([probs_start, probs_end], axis=1)\n",
    "#labels = tf.stack([start_label, end_label], axis=1)\n",
    "\n",
    "correct_start = tf.equal(tf.argmax(probs_start, axis=1), tf.argmax(start_label, axis=1))\n",
    "\n",
    "correct_span = tf.reduce_all(correct_start, axis=1)\n",
    "\n",
    "#correct_pred = tf.equal(preds, labels)\n",
    "start_accuracy = tf.reduce_mean(tf.cast(correct_start, tf.float32))\n",
    "mean_accuracy = tf.reduce_mean(tf.cast(correct_span, tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx_distributions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(tf.argmax(start_label_sq, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a,b = val_data_iter.next_batch()\n",
    "c,d,e = zip(*a)\n",
    "list(np.array(e).transpose(1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for train_iter in range(train_iters):\n",
    "\n",
    "    train_iter +=1\n",
    "    train_batch, _ = train_data_iter.next_batch()\n",
    "    train_context, train_question, train_answer = zip(*train_batch)\n",
    "    train_answer = np.array(train_answer).transpose(1,0,2)\n",
    "    if train_iter == 1:\n",
    "        train_feed_dict = {passage: train_context,\n",
    "                           question: train_question,\n",
    "                           y: train_answer,\n",
    "                           dropout_keep_prob: 1.0}\n",
    "        training_loss, start_accuracy = tuple(sess.run([loss, start_accuracy_sq],\n",
    "                                                       feed_dict=train_feed_dict))\n",
    "\n",
    "        print(\"Iter {}, start_accuracy = {:.4f}, train loss = {:.6f}\"\n",
    "              .format(train_iter, start_accuracy, training_loss))\n",
    "\n",
    "    sess.run(optimizer, feed_dict={passage: train_context,\n",
    "                                   question: train_question, \n",
    "                                   y: train_answer, \n",
    "                                   dropout_keep_prob: keep_prob})\n",
    "    if train_iter % display_step == 0:\n",
    "        disp_feed_dict = {passage: train_context, \n",
    "                          question: train_question, \n",
    "                          y: train_answer,\n",
    "                          dropout_keep_prob: keep_prob\n",
    "                         }\n",
    "        training_loss, start_accuracy = tuple(sess.run([loss, start_accuracy_sq],\n",
    "                                                       feed_dict={passage: train_context, \n",
    "                                                                  question: train_question, \n",
    "                                                                  y: train_answer,\n",
    "                                                                  dropout_keep_prob: keep_prob\n",
    "                                                                 }))\n",
    "        print(\"Iter {}, start_accuracy = {:.4f}, train loss = {:.6f}\"\n",
    "              .format(train_iter, start_accuracy, training_loss))\n",
    "        t = sess.run(idx_distributions[0], feed_dict=disp_feed_dict)\n",
    "        l = sess.run(start_label_sq, feed_dict=disp_feed_dict)\n",
    "        print(t[0])\n",
    "        print(l[0])\n",
    "        print(np.argmax(t[0]))\n",
    "        print(np.argmax(l[0]))\n",
    "    \n",
    "    if val_data_iter !=None and train_iter%val_interval ==0:\n",
    "        val_batch, _ = val_data_iter.next_batch()\n",
    "        val_context, val_question, val_answer = zip(*val_batch)\n",
    "        val_answer = np.array(val_answer).transpose(1,0,2)\n",
    "        val_loss, val_start_accuracy = tuple(sess.run([loss, start_accuracy_sq],\n",
    "                                                      feed_dict={passage: val_context, \n",
    "                                                                 question: val_question, \n",
    "                                                                 y: val_answer, \n",
    "                                                                 dropout_keep_prob: 1.0 \n",
    "                                                                }))\n",
    "        print(\"Iter {}, val_start_accuracy = {:.4f}, val loss = {:.6f}\"\n",
    "              .format(train_iter, val_start_accuracy, val_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "128, 766, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_env]",
   "language": "python",
   "name": "conda-env-tensorflow_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
