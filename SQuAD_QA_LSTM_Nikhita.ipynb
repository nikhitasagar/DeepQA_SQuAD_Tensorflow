{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import shuffle\n",
    "import os\n",
    "import gensim\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import operator\n",
    "import math\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.models.rnn.translate import seq2seq_model\n",
    "import string\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import split, explode\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext('local[4]')\n",
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_punct(line):    \n",
    "    punctuation = [x for x in list(string.punctuation)]\n",
    "    space_punct = [' {0}'.format(elem) for elem in punctuation]\n",
    "    replace_punctuation = str.maketrans(dict(zip(punctuation, space_punct)))\n",
    "    line = line.translate(replace_punctuation)\n",
    "    return line.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preproccessData(filename):\n",
    "    \n",
    "    data_dir = '/media/ai2-rey/data_disk/data_sets/SQuAD/'\n",
    "    df = sqlcontext.read.json(data_dir+filename)\n",
    "    df = df.withColumn('data', explode('data'))\n",
    "    df = df.select('data.paragraphs')\n",
    "    df = df.withColumn('paragraphs', explode('paragraphs'))\n",
    "    df = df.select(['paragraphs.context','paragraphs.qas'])\n",
    "    df = df.withColumn('qas', explode('qas'))\n",
    "    df = df.selectExpr(['context as context','qas.question as question','qas.answers.text as answer'])\n",
    "    df = df.withColumn('answer', explode('answer'))\n",
    "    \n",
    "    lowercase_UDF = func.UserDefinedFunction(replace_punct, StringType())\n",
    "\n",
    "    df = (df.withColumn('context_lower', lowercase_UDF(df.context))\n",
    "            .withColumn('question_lower', lowercase_UDF(df.question))\n",
    "            .withColumn('answer_lower', lowercase_UDF(df.answer))\n",
    "            .drop('context')\n",
    "            .drop('question')\n",
    "            .drop('answer')\n",
    "            .withColumnRenamed('context_lower', 'context')\n",
    "            .withColumnRenamed('question_lower', 'question')\n",
    "            .withColumnRenamed('answer_lower', 'answer'))\n",
    "    \n",
    "    print(df.printSchema())\n",
    "    \n",
    "    context = df.select(\"context\").rdd.flatMap(lambda x: x).collect()\n",
    "    question = df.select(\"question\").rdd.flatMap(lambda x: x).collect()\n",
    "    answer = df.select(\"answer\").rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    return context, question, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QA_Dataset:\n",
    "  \n",
    "    def __init__(self, train_context, train_question, train_answer,\n",
    "                 test_context, test_question, test_answer, max_vocab_size=None, \n",
    "                 autopad_context=None, autopad_ques = None, autopad_answer=None,\n",
    "                 context_pad_len=None, ques_pad_len = None, answer_pad_len=None):\n",
    "\n",
    "        assert autopad_context in {'min', 'max', 'avg'}\n",
    "        assert autopad_answer in {'min', 'max', 'avg'}\n",
    "        assert autopad_ques in {'min', 'max', 'avg'}\n",
    "        \n",
    "        self.train_context = train_context\n",
    "        self.train_question = train_question\n",
    "        self.train_answer = train_answer\n",
    "        \n",
    "        self.test_context = test_context\n",
    "        self.test_question = test_question\n",
    "        self.test_answer = test_answer\n",
    "        \n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.context_pad_len = context_pad_len\n",
    "        self.ques_pad_len = ques_pad_len\n",
    "        self.answer_pad_len = answer_pad_len\n",
    "        self.__autopad_context = autopad_context\n",
    "        self.__autopad_ques = autopad_ques\n",
    "        self.__autopad_answer = autopad_answer\n",
    "        \n",
    "        self.vocab = set()\n",
    "        self.word_counter = dict()\n",
    "        self.word2id_dict = dict()\n",
    "        self.id2word_dict = dict()\n",
    "        \n",
    "        self.train_context_ids = None\n",
    "        self.train_question_ids = None\n",
    "        self.train_answer_ids = None\n",
    "        self.test_context_ids = None\n",
    "        self.test_question_ids = None\n",
    "        self.test_answer_ids = None\n",
    "        \n",
    "        self.train_context_text = None\n",
    "        self.train_question_text = None\n",
    "        self.train_answer_text = None\n",
    "        self.test_context_text = None\n",
    "        self.test_question_text = None\n",
    "        self.test_answer_text = None\n",
    "        \n",
    "        self.num_tokens = None\n",
    "        \n",
    "        self.__parse_data()\n",
    "        self.__create_word2id_dict()\n",
    "        self.__numericize_data()\n",
    "\n",
    "    def __update_word_counter(self, sequence):\n",
    "        \"\"\" Update word_counter with counts for words in a sentence\n",
    "        \n",
    "        Args:\n",
    "            sequence (list<str>) : list of words in a sequence\n",
    "        \n",
    "        \"\"\"\n",
    "        for word in sequence:\n",
    "            self.word_counter[word] = self.word_counter.get(word, 0) + 1\n",
    "            \n",
    "    def __create_vocab(self):\n",
    "        \"\"\" Create set of most frequent unique words found in the training data \"\"\"\n",
    "        \n",
    "        if self.max_vocab_size == None:\n",
    "            self.vocab = set(self.word_counter.keys())\n",
    "        else:\n",
    "            self.vocab = set(sorted(self.word_counter, key=self.word_counter.get, reverse=True)[:self.max_vocab_size])\n",
    "        \n",
    "    def __shuffle_data(self, data):\n",
    "        random.shuffle(data)\n",
    "        return list(zip(*data))\n",
    "    \n",
    "    def __parse_data(self):\n",
    "        \n",
    "        for idx in range(len(self.train_context)):\n",
    "            self.__update_word_counter(self.train_context[idx].split())\n",
    "            self.__update_word_counter(self.train_question[idx].split())\n",
    "            self.__update_word_counter(self.train_answer[idx].split())\n",
    "        \n",
    "        self.__create_vocab()\n",
    "        \n",
    "        train_shuffle = list(zip(self.train_context, self.train_question, self.train_answer))        \n",
    "        self.train_context, self.train_question, self.train_answer = self.__shuffle_data(train_shuffle)\n",
    "        \n",
    "        test_shuffle = list(zip(self.test_context, self.test_question, self.test_answer))\n",
    "        self.test_context, self.test_question, self.test_answer = self.__shuffle_data(test_shuffle)\n",
    "        \n",
    "    \n",
    "    def __create_word2id_dict(self):\n",
    "        \n",
    "        misc_tokens = ['PAD', 'UNK']\n",
    "            \n",
    "        for i, token in enumerate(misc_tokens):\n",
    "            self.word2id_dict[token] = i\n",
    "            \n",
    "        for word in self.vocab:\n",
    "            self.word2id_dict[word] = len(self.word2id_dict)\n",
    "\n",
    "        self.vocab |= set(misc_tokens)\n",
    "        self.id2word_dict = dict(zip(self.word2id_dict.values(), self.word2id_dict.keys()))\n",
    "        self.num_tokens = len(self.word2id_dict)\n",
    "    \n",
    "    def __convert_word2id(self, word):\n",
    "        try:\n",
    "            word_id = self.word2id_dict[word]\n",
    "        except:\n",
    "            word_id = self.word2id_dict['UNK']\n",
    "        return word_id\n",
    "\n",
    "    def __convert_to_one_hot(self, answer_id):\n",
    "        one_hot = [0 for i in range(self.num_tokens)]\n",
    "        one_hot[answer_id] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    def __apply_padding(self, sequence, pad_len):\n",
    "        if len(sequence) < pad_len:\n",
    "            sequence += [self.word2id_dict['PAD'] for i in range(pad_len - len(sequence))]\n",
    "        elif len(sequence) > pad_len:\n",
    "            sequence = sequence[:pad_len]\n",
    "        else:\n",
    "            pass\n",
    "        return sequence\n",
    "        \n",
    "    def __get_seq_length_stats(self, sequences):\n",
    "        max_len = 0\n",
    "        min_len = 100000\n",
    "        avg_len = 0\n",
    "        for sequence in sequences:\n",
    "            max_len = max(max_len, len(sequence))\n",
    "            min_len = min(min_len, len(sequence))\n",
    "            avg_len += len(sequence)\n",
    "        avg_len = int(float(avg_len) / len(sequences))\n",
    "        return min_len, max_len, avg_len\n",
    "\n",
    "    def __get_max_sequence_lengths(self, context_ids, question_ids, answer_ids):\n",
    "        min_context_len, max_context_len, avg_context_len = self.__get_seq_length_stats(context_ids)\n",
    "        min_ques_len, max_ques_len, avg_ques_len = self.__get_seq_length_stats(question_ids)\n",
    "        min_answer_len, max_answer_len, avg_answer_len = self.__get_seq_length_stats(answer_ids)\n",
    "\n",
    "        if self.context_pad_len == None:\n",
    "            if self.__autopad_context != None:\n",
    "                if self.__autopad_context == 'min':\n",
    "                    self.context_pad_len = min_context_len\n",
    "                elif self.__autopad_context == 'max':\n",
    "                    self.context_pad_len = max_context_len\n",
    "                elif self.__autopad_context == 'avg':\n",
    "                    self.context_pad_len = avg_context_len\n",
    "            else:\n",
    "                self.context_pad_len = avg_context_len\n",
    "                \n",
    "        if self.ques_pad_len == None:\n",
    "            if self.__autopad_ques != None:\n",
    "                if self.__autopad_ques == 'min':\n",
    "                    self.ques_pad_len = min_ques_len\n",
    "                elif self.__autopad_ques == 'max':\n",
    "                    self.ques_pad_len = max_ques_len\n",
    "                elif self.__autopad_ques == 'avg':\n",
    "                    self.ques_pad_len = avg_ques_len\n",
    "            else:\n",
    "                self.ques_pad_len = avg_ques_len\n",
    "                \n",
    "        if self.answer_pad_len == None:\n",
    "            if self.__autopad_answer != None:\n",
    "                if self.__autopad_answer == 'min':\n",
    "                    self.answer_pad_len = min_answer_len\n",
    "                elif self.__autopad_answer == 'max':\n",
    "                    self.answer_pad_len = max_answer_len\n",
    "                elif self.__autopad_answer == 'avg':\n",
    "                    self.answer_pad_len = avg_answer_len\n",
    "            else:\n",
    "                self.answer_pad_len = avg_answer_len \n",
    "                \n",
    "        print('Context length stats: min = {}, max = {}, avg = {}'.format(min_context_len, max_context_len, avg_context_len))\n",
    "        print('Context pad length set to: {}'.format(self.context_pad_len))\n",
    "        \n",
    "        print('Question length stats: min = {}, max = {}, avg = {}'.format(min_ques_len, max_ques_len, avg_ques_len))\n",
    "        print('Question pad length set to: {}'.format(self.ques_pad_len))        \n",
    "        \n",
    "        print('Answer length stats: min = {}, max = {}, avg = {}'.format(min_answer_len, max_answer_len, avg_answer_len))\n",
    "        print('Answer pad length set to: {}'.format(self.answer_pad_len))        \n",
    "\n",
    "    def __tokenize_sentences(self, context_ids, question_ids, answer_ids):\n",
    "        \"\"\" Tokenizes sentences.\n",
    "        :param raw: dict returned from load_babi\n",
    "        :param word_table: WordTable\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        context_tokens = []\n",
    "        question_tokens = []\n",
    "        answer_tokens = []\n",
    "        \n",
    "        for i in range(len(context_ids)):\n",
    "            c_tkn_ids = self.__apply_padding(context_ids[i], self.context_pad_len)\n",
    "            context_tokens.append(c_tkn_ids)\n",
    "            q_tkn_ids = self.__apply_padding(question_ids[i], self.ques_pad_len)\n",
    "            question_tokens.append(q_tkn_ids)\n",
    "            a_tkn_ids = self.__apply_padding(answer_ids[i], self.answer_pad_len)\n",
    "            answer_tokens.append(a_tkn_ids)\n",
    "        return context_tokens, question_tokens, answer_tokens\n",
    "        \n",
    "    def __convert_text2ids(self, context, question, answer):\n",
    "        \"\"\" Tokenizes sentences.\n",
    "        :param raw: dict returned from load_babi\n",
    "        :param word_table: WordTable\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        context_ids = []\n",
    "        question_ids = []\n",
    "        answer_ids = []\n",
    "        for i in range(len(context)):\n",
    "            c_ids = [self.__convert_word2id(word) for word in context[i].split()]\n",
    "            context_ids.append(c_ids)\n",
    "            q_ids = [self.__convert_word2id(word) for word in question[i].split()]\n",
    "            question_ids.append(q_ids)\n",
    "            a_ids = [self.__convert_word2id(word) for word in answer[i].split()]\n",
    "            answer_ids.append(a_ids)            \n",
    "        return context_ids, question_ids, answer_ids\n",
    "          \n",
    "    def __convert_text2words(self, context, question, answer):\n",
    "        context_words = []\n",
    "        question_words = []\n",
    "        answer_words = []\n",
    "        for i in range(len(context)):\n",
    "            c_words = context[i].split()\n",
    "            context_words.append(c_words)\n",
    "            q_words = question[i].split()\n",
    "            question_words.append(q_words)\n",
    "            a_words = answer[i].split()\n",
    "            answer_words.append(a_words)\n",
    "        return context_words, question_words, answer_words    \n",
    "            \n",
    "        \n",
    "    def __numericize_data(self):\n",
    "        train_c, train_q, train_a = self.__convert_text2ids(self.train_context, self.train_question, self.train_answer)\n",
    "        test_c, test_q, test_a = self.__convert_text2ids(self.test_context, self.test_question, self.test_answer)\n",
    "        \n",
    "        self.__get_max_sequence_lengths(train_c, train_q, train_a)\n",
    "        \n",
    "        self.train_context_ids, self.train_question_ids, self.train_answer_ids = self.__tokenize_sentences(train_c, train_q, train_a)\n",
    "        self.test_context_ids, self.test_question_ids, self.test_answer_ids = self.__tokenize_sentences(test_c, test_q, test_a)\n",
    "        \n",
    "        self.train_context_text, self.train_question_text, self.train_answer_text = self.__convert_text2words(self.train_context,\n",
    "                                                                                                              self.train_question,\n",
    "                                                                                                              self.train_answer)\n",
    "        self.test_context_text, self.test_question_text, self.test_answer_text = self.__convert_text2words(self.test_context,\n",
    "                                                                                                           self.test_question,\n",
    "                                                                                                           self.test_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2Seq:\n",
    " \n",
    "    def __init__(self, vocab_size, xseq_len, yseq_len, test_batches, num_layers, lr_rate=0.001, \n",
    "                 momentum = 0.9, n_hidden=256, word_dim=100, \n",
    "                 dropout_rate=1., gpu_device=0, model_dir=None):\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.xseq_len = xseq_len\n",
    "        self.yseq_len = yseq_len\n",
    "        self.test_batches = test_batches\n",
    "        self.num_layers = num_layers\n",
    "        self.lr_rate = lr_rate\n",
    "        self.momentum = momentum\n",
    "        self.n_hidden = n_hidden\n",
    "        self.word_dim = word_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.gpu_device = gpu_device\n",
    "        self.model_dir = self.__prepare_model_dir(model_dir)\n",
    "        self.__keep_prob = None\n",
    "        \n",
    "        self.__graph = tf.Graph()\n",
    "        \n",
    "        self.__build_model()\n",
    "    \n",
    "    def __prepare_model_dir(self, model_dir):\n",
    "        \"\"\" Checks model directory for a weights folder and creates one if none exists\n",
    "        \n",
    "        Args:\n",
    "            model_dir (str) : defines directory location to save weights and training log file\n",
    "            \n",
    "        Returns:\n",
    "            str : directory location with weights folder\n",
    "        \n",
    "        \"\"\"\n",
    "        if model_dir == None:\n",
    "            model_dir = os.getcwd() + '/'\n",
    "        else:\n",
    "            if model_dir[-1] != '/':\n",
    "                model_dir = model_dir + '/'\n",
    "            else:\n",
    "                model_dir = model_dir\n",
    "        \n",
    "        if not os.path.exists(model_dir + 'weights'):\n",
    "            os.makedirs(model_dir + 'weights')\n",
    "        return model_dir\n",
    "                   \n",
    "    def __build_model(self):\n",
    "        \"\"\" Creates computation graph for dual encoder LSTM. Includes structure for training and deploying. \"\"\"\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        gpu_device_name = '/gpu:{}'.format(self.gpu_device)\n",
    "        \n",
    "        with self.__graph.as_default():\n",
    "            with tf.device(gpu_device_name):\n",
    "                # define placeholder variables for model inputs\n",
    "                self.enc_inp = [tf.placeholder(shape=[None,], \n",
    "                                               dtype=tf.int32, \n",
    "                                               name='ei_{}'.format(t)) for t in range(self.xseq_len)]\n",
    "                self.labels = [tf.placeholder(shape=[None,], \n",
    "                                               dtype=tf.int32, \n",
    "                                               name='ei_{}'.format(t)) for t in range(self.yseq_len)]\n",
    "                self.dec_inp = ([tf.zeros_like(self.enc_inp[0], dtype=np.int32, name=\"GO\")]+self.labels[:-1]) \n",
    "                \n",
    "                self.__keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "                # Build the RNN\n",
    "                with tf.variable_scope(\"decoder\"):\n",
    "                    # We use an LSTM Cell\n",
    "                    #cell = tf.nn.rnn_cell.LSTMCell(self.n_hidden, forget_bias=2.0, state_is_tuple=True)\n",
    "                    cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                           tf.nn.rnn_cell.BasicLSTMCell(word_dim, state_is_tuple=True),\n",
    "                           output_keep_prob=self.__keep_prob)\n",
    "                    \n",
    "                    stacked_lstm = tf.nn.rnn_cell.MultiRNNCell([cell]*self.num_layers, state_is_tuple=True)\n",
    "                    \n",
    "                    self.dec_outputs, self.dec_states = tf.nn.seq2seq.embedding_rnn_seq2seq(\n",
    "                                                        self.enc_inp, self.dec_inp, stacked_lstm, \n",
    "                                                        vocab_size, vocab_size, word_dim)\n",
    "                    \n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                    self.dec_outputs_test, self.dec_states_test = tf.nn.seq2seq.embedding_rnn_seq2seq(\n",
    "                                                        self.enc_inp, self.dec_inp, stacked_lstm, \n",
    "                                                        vocab_size, vocab_size, word_dim,\n",
    "                                                        feed_previous=True) \n",
    "                    \n",
    "                loss_weights = [tf.ones_like(l, dtype=tf.float32) for l in self.labels]\n",
    "\n",
    "                self.__loss = tf.nn.seq2seq.sequence_loss(self.dec_outputs, self.labels, loss_weights, vocab_size)\n",
    "\n",
    "                self.__optimizer = tf.train.AdamOptimizer(learning_rate=self.lr_rate).minimize(self.__loss)\n",
    "\n",
    "                self.__init = tf.global_variables_initializer()\n",
    "    \n",
    "    def get_feed(self,X,Y, keep_prob):\n",
    "        feed_dict={self.enc_inp[t]: X[t] for t in range(self.xseq_len)}\n",
    "        feed_dict.update({self.labels[t]: Y[t] for t in range(self.yseq_len)})\n",
    "        feed_dict[self.__keep_prob]=keep_prob\n",
    "        return feed_dict\n",
    "\n",
    "    def train_batch(self,sess, data_iter):\n",
    "        X,Y = data_iter.next_batch()\n",
    "        feed_dict = self.get_feed(X,Y, self.dropout_rate)\n",
    "        _, loss_v = sess.run([self.__optimizer, self.__loss], feed_dict)\n",
    "        return loss_v\n",
    "     \n",
    "    def train(self, train_data_iter, test_data_iter, deploy_data_iter,\n",
    "              deploy_interval = 1000, train_iters=10000, display_step=200, \n",
    "              save_weights_interval=5000, id2word_dict=None, weights_prefix=None):\n",
    "\n",
    "        \n",
    "        with tf.Session(graph=self.__graph, config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "            sess.run(self.__init)\n",
    "            saver = tf.train.Saver(max_to_keep=100)\n",
    "            # Keep training until reach max iterations\n",
    "            for train_iter in range(train_iters):\n",
    "                train_iter += 1\n",
    "                \n",
    "                loss = self.train_batch(sess, train_data_iter)\n",
    "                if train_iter % display_step == 0:\n",
    "\n",
    "                    train_loss_string = \"Iter {}, Minibatch Loss = {:.6f}\".format(train_iter, loss)\n",
    "                    print(train_loss_string)\n",
    "                \n",
    "                if train_iter % save_weights_interval ==0:\n",
    "                    if weights_prefix != None:\n",
    "                        weights_dir = self.model_dir + \"weights/{}_iter-{}.cpkt\".format(weights_prefix,train_iter)\n",
    "                    else:\n",
    "                        weights_dir = self.model_dir + \"weights/QA_seq2seq_weights_iter-{}.ckpt\".format(train_iter)\n",
    "                    save_path = saver.save(sess, weights_dir)\n",
    "                    save_string = \"Model saved in file: {}\".format(save_path)\n",
    "                    print(save_string)\n",
    "                    test_loss, test_accuracy = self.test(sess, test_data_iter, self.test_batches)\n",
    "                    print('Test loss @ iter {}: {} '.format(train_iter, test_loss))\n",
    "                    print('Test Accuracy @ iter {}: {} '.format(train_iter, test_accuracy))\n",
    "                \n",
    "#                 if deploy_data_iter !=None and train_iter % deploy_interval ==0:\n",
    "#                     d_X, d_Y = deploy_data_iter.next_batch()\n",
    "#                     d_feed_dict = self.get_feed(d_X,d_Y, 1.)\n",
    "                                        \n",
    "                    \n",
    "    def test_step(self, test_data_iter, sess):\n",
    "        testX, testY = test_data_iter.next_batch()\n",
    "        feed_dict = self.get_feed(testX, testY, keep_prob=1.)\n",
    "        loss_v, dec_op_v = sess.run([self.__loss, self.dec_outputs_test], feed_dict)\n",
    "        dec_op_v = np.array(dec_op_v).transpose([1,0,2])\n",
    "        return loss_v, dec_op_v, testX, testY\n",
    "    \n",
    "    def test(self, sess, test_data_iter, num_batches):\n",
    "        losses= []\n",
    "        predict_loss = []\n",
    "        for i in range(num_batches):\n",
    "            loss_t, dec_op_t, batchX, batchY = self.test_step(test_data_iter, sess)\n",
    "            losses.append(loss_t)\n",
    "            \n",
    "            for idx in range(len(dec_op_t)):\n",
    "                real = batchY.T[idx]\n",
    "                predict = np.argmax(dec_op_t, axis=2)[idx]\n",
    "                predict_loss.append(all(real==predict))\n",
    "        return np.mean(losses), np.mean(predict_loss)        \n",
    "    \n",
    "    def predict(self, ckpt_file, X):\n",
    "        with tf.Session(graph=self.__graph, config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "            sess.run(self.__init)\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, self.model_dir + 'weights/' + ckpt_file)\n",
    "            feed_dict = {self.enc_inp[t]: X[t] for t in range(self.xseq_len)}\n",
    "            feed_dict[self.__keep_prob] = 1.\n",
    "            dec_op_v = sess.run(self.dec_outputs_test, feed_dict)\n",
    "            # dec_op_v is a list; also need to transpose 0,1 indices \n",
    "            #  (interchange batch_size and timesteps dimensions\n",
    "            dec_op_v = np.array(dec_op_v).transpose([1,0,2])\n",
    "            # return the index of item with highest probability\n",
    "            return np.argmax(dec_op_v, axis=2) \n",
    "        \n",
    "    def test_try(self, ckpt_file, test_data_iter):\n",
    "        with tf.Session(graph=self.__graph, config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "            sess.run(self.__init)\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, self.model_dir + 'weights/' + ckpt_file)\n",
    "            accuracy = []\n",
    "            loss_t, out_t, X, Y = self.test_step(test_data_iter, sess)\n",
    "            \n",
    "            for idx in range(len(out_t)):\n",
    "                real = Y.T[idx]\n",
    "                prediction = np.argmax(out_t, axis=2)[idx]\n",
    "                accuracy.append(all(real==prediction))\n",
    "            return np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataIterator:\n",
    "    def __init__(self, data, batch_size):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.data_iterator = self.make_random_iter()\n",
    "        \n",
    "    def next_batch(self):\n",
    "        try:\n",
    "            idxs = next(self.data_iterator)\n",
    "        except StopIteration:\n",
    "            self.data_iterator = self.make_random_iter()\n",
    "            idxs = next(self.data_iterator)\n",
    "            \n",
    "        batch = [self.data[i] for i in idxs]\n",
    "        batch_idxs = [idx for idx in idxs]\n",
    "        return batch, batch_idxs\n",
    "\n",
    "    def make_random_iter(self):\n",
    "        splits = np.arange(self.batch_size, len(self.data), self.batch_size)\n",
    "        it = np.split(np.random.permutation(range(len(self.data))), splits)[:-1]\n",
    "        return iter(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- context: string (nullable = true)\n",
      " |-- question: string (nullable = true)\n",
      " |-- answer: string (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- context: string (nullable = true)\n",
      " |-- question: string (nullable = true)\n",
      " |-- answer: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_context, train_question, train_answer = preproccessData('train-v1.1.json')\n",
    "test_context, test_question, test_answer = preproccessData('dev-v1.1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context length stats: min = 22, max = 766, avg = 138\n",
      "Context pad length set to: 766\n",
      "Question length stats: min = 1, max = 60, avg = 11\n",
      "Question pad length set to: 60\n",
      "Answer length stats: min = 1, max = 46, avg = 3\n",
      "Answer pad length set to: 3\n"
     ]
    }
   ],
   "source": [
    "data = QA_Dataset(train_context, train_question, train_answer,\n",
    "                 test_context, test_question, test_answer,\n",
    "                 autopad_context='max', autopad_answer='avg', autopad_ques='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = list(zip(data.train_context_ids, data.train_question_ids, data.train_answer_ids))\n",
    "test_data = list(zip(data.test_context_ids, data.test_question_ids, data.test_answer_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_iter = DataIterator(train_data, 256)\n",
    "test_data_iter = DataIterator(test_data, 358)\n",
    "deploy_data_iter = DataIterator(test_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(test_data)/97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(data.vocab) \n",
    "\n",
    "xseq_len = data.context_pad_len\n",
    "yseq_len = data.answer_pad_len\n",
    "test_batches = 97 \n",
    "num_layers = 3\n",
    "lr_rate = 0.001\n",
    "momentum = 0.9\n",
    "n_hidden = 256 \n",
    "word_dim = 100\n",
    "dropout_rate = 0.5\n",
    "gpu_device = 0\n",
    "model_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Seq2Seq(xseq_len=xseq_len,\n",
    "               yseq_len=yseq_len, \n",
    "               vocab_size=vocab_size,\n",
    "               word_dim = word_dim,\n",
    "               test_batches = test_batches\n",
    "               num_layers = num_layers,\n",
    "               dropout_rate=0.5,\n",
    "               gpu_device = gpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = model.train(train_data_iter, test_data_iter, deploy_data_iter,\n",
    "                  train_iters=1000, display_step=200,\n",
    "                  save_weights_interval = 200, id2word_dict=data.id2word_dict, \n",
    "                  weights_prefix='SQuAD_QA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105183"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105183"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.word2id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105181"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.word_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_env]",
   "language": "python",
   "name": "conda-env-tensorflow_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
